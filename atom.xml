<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Justlovesmile&#39;s BLOG</title>
  <icon>https://www.gravatar.com/avatar/7637880d6fba1338ba97dd0c0ccfc1c8</icon>
  <subtitle>Writer(记录) &amp; Maker(创作) &amp; Developer(启发)</subtitle>
  <link href="https://blog.justlovesmile.top/atom.xml" rel="self"/>
  
  <link href="https://blog.justlovesmile.top/"/>
  <updated>2022-01-27T02:24:38.000Z</updated>
  <id>https://blog.justlovesmile.top/</id>
  
  <author>
    <name>Justlovesmile</name>
    <email>865717150@qq.com</email>
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>深度学习 | 小样本学习基础概念</title>
    <link href="https://blog.justlovesmile.top/posts/d150f284.html"/>
    <id>https://blog.justlovesmile.top/posts/d150f284.html</id>
    <published>2022-01-27T02:24:38.000Z</published>
    <updated>2022-01-27T02:24:38.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="小样本学习"><a href="#小样本学习" class="headerlink" title="小样本学习"></a>小样本学习</h1><p>人类非常擅长通过极少量的样本识别一个新物体，比如小孩子只需要书中的一些图片就可以认识什么是“斑马”，什么是“犀牛”。在人类的快速学习能力的启发下，研究人员希望机器学习模型在学习了一定类别的大量数据后，对于新的类别，只需要少量的样本就能快速学习，这就是 Few-shot Learning 要解决的问题。Few-shot learning (FSL) 在机器学习领域具有重大意义和挑战性，是否拥有从少量样本中学习和概括的能力，是将人工智能和人类智能进行区分的明显分界点，因为人类可以仅通过一个或几个示例就可以轻松地建立对新事物的认知，而机器学习算法通常需要成千上万个有监督样本来保证其泛化能力。</p><h2 id="1-基础概念"><a href="#1-基础概念" class="headerlink" title="1.基础概念"></a>1.基础概念</h2><p>机器学习定义：A computer program is said to learn from experience <code>E</code> with respect to some classes of task <code>T</code> and performance measure <code>P</code> if its performance can improve with <code>E</code> on <code>T</code> measured by <code>P</code>.</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/202109201329876.png" alt="image-20210920132843364"></p><p>小样本学习定义：Few-Shot Learning (FSL) is a type of machine learning problems (specified by <code>E</code>, <code>T</code> and <code>P</code>), where <code>E</code> contains only a limited number of examples with supervised information for the target <code>T</code>.</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/202109201329400.png" alt="image-20210920132907931"></p><p>小样本学习（Few-shot learning），或者称为少样本学习（Low-shot learning），包含了n-shot learning，其中<code>n</code>代表样本数量，<code>n=1</code>的情况下，也被称One-shot learning，而<code>n=0</code>的情况下，被称为Zero-shot learning。</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/202109201348334.png" alt="image-20210920134843156"></p><p>小样本学习的主要思想是利用先验知识使其快速适用于只包含少量带有监督信息的样本的任务中。</p><h2 id="2-方法分类"><a href="#2-方法分类" class="headerlink" title="2. 方法分类"></a>2. 方法分类</h2><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/202109201336741.png" alt="image-20210920133613257"></p><p>小样本学习问题的解决方法可以根据先验知识的利用方式分为三类：</p><ul><li>数据：此类方法利用先验知识来增强训练数据集或者增加样本数量（从样本量的角度）<ul><li><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/202110051544192.png" alt="image-20211005154338367"></li><li>1.使用旋转，翻转，裁剪等方法对训练集图像增强</li><li>2.从其他数据集获取图像用于扩充训练集</li><li>3.使用GAN来生成具有相似分布的数据用于扩充训练集</li></ul></li><li>模型：此类方法利用先验知识来限制假设空间的复杂性（从模型训练的角度）<ul><li><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/202110051544913.png" alt="image-20211005154419771"></li><li>1.多任务学习（同时进行多个相关任务训练，共享表示，以获得更好的泛化能力）与迁移学习不同（将源任务中学到的知识运用到目标任务中）<ul><li>parameter sharing：多任务间共享参数（例如最开始几层网络结构共享，最后输出层单独训练）</li><li>parameter typing：对不同任务的参数正则化处理，使其参数相似（encourages parameters of different tasks to be similar using regularization）</li></ul></li><li>2.嵌入学习（将样本映射（嵌入）到低纬度空间后，相似样本距离更近，不相似样本距离远）<ul><li><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/202110052028366.png" alt="image-20211005202850882"></li><li>Task-Specific Embedding Model:只使用来自任务的信息学习一个定制的嵌入函数</li><li>Task-Invariant Embedding Model：将从其他充足样本中学到的信息直接利用到小样本学习任务中<ul><li><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/202110062246351.png" alt="image-20211006224637164"></li><li>Matching Nets</li><li>Prototypical Networks(ProtoNet)</li></ul></li><li>Hybrid Embedding Model: 前两种方法的结合，使用小样本任务中的task specific信息运用到从先验知识学到的task invariant嵌入模型<ul><li><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/202110062249691.png" alt="image-20211006224925493"></li></ul></li></ul></li><li>3.带有存储的模型，构建键值存储，并优化内存，每个新样本都可以由内存中提取出的内容的加权平均值表示（通过查询相似性），进一步限制假设空间。<ul><li><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/202110052104077.png" alt="image-20211005202918521"></li><li><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/202110052033347.png" alt="image-20211005203314132"></li><li>优化表征（representation）</li><li>优化参数（parameter）</li></ul></li><li>4.生成模型，对于样本x在先验知识的帮助下可以估计其分布p(x)：假设x的分布可以表示为受$\theta$约束的$p(x;\theta)$，并且通常还存在潜在变量$z \sim p(z;y)$，因此$x \sim \int p(x|z;\theta)p(z;y)dz$，即在先验分布$p(z;y)$的帮助下，可以进一步缩小假设空间的大小.<ul><li><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/202110052048660.png" alt="image-20211005204829440"></li><li>Decomposable Components：训练可分解组件模型，在不同任务间共享分解组件的信息，最后再找到分解组件的组合方式（模型层面？）</li><li>Groupwise Shared Prior：使用无监督学习将数据集分组，对于新类别，首先查询其所属组，再根据其所属组的先验概率建模（相似的任务拥有相似的先验概率）</li><li>Parameters of Inference Networks：找到最佳的$\theta$，使得最大化$p(z|x;\theta,\gamma)=\frac{p(x,z;\theta,\gamma)}{p(x;\gamma)}=\frac{p(x|z;\theta)p(z;\gamma)}{\int p(x|z;\theta)p(z;\gamma)dz}$，通常使用从数据中学到的变分分布$q(z;\delta)$来估计$p(z|x;\theta,\gamma)$。（？）</li></ul></li></ul></li><li>算法：此类方法利用先验知识在假设空间中搜索最优的假设<ul><li><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/202110052106876.png" alt="image-20211005210608746"></li><li>1.精炼现存参数<ul><li>使用预训练模型，通过正则化进行微调<ul><li><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/202110052115770.png" alt="image-20211005211532604"></li><li>Early-stopping</li><li>Selectively updating parameters</li><li>Updating related parts of parameters together</li><li>Using a model regression network</li></ul></li><li>聚集子任务的参数（参数层面？）<ul><li><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/202110052109091.png" alt="image-20211005210954883"></li></ul></li><li>使用新参数微调现有参数：给模型参数扩充一个$\delta$，使其参数为$\theta={\theta_0,\delta}$，然后通过学习$\delta$来微调初始参数$\theta_0$。</li></ul></li><li>2.精炼Meta-Learned参数<ul><li><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/202110071533336.png" alt="image-20211007153301984"></li></ul></li><li>3.学习优化器：不使用梯度下降来更新参数，而是通过学习一个优化器来输出参数的更新，即$\Delta{\theta^{i-1}}$<ul><li><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/202110071540903.png" alt="image-20211007154007652"></li></ul></li></ul></li></ul><h2 id="3-小样本学习常用数据集"><a href="#3-小样本学习常用数据集" class="headerlink" title="3. 小样本学习常用数据集"></a>3. 小样本学习常用数据集</h2><p>小样本常用Benchmark图像数据集：</p><ul><li>Omniglot</li><li>Mini-Imagenet</li><li>CU-Birds</li></ul>]]></content>
    
    
    <summary type="html">在很多场景下，收集大量的有标签的数据是非常昂贵、困难、甚至不可能的，比如医疗数据、手机上用户手动标注的数据等。是否能仅利用少量带标签的数据来训练就得到一个好的模型？这已经成为机器学习的发展中一个十分重要的课题，不论是学术界还是工业界都高度关注。</summary>
    
    
    
    <category term="人工智能" scheme="https://blog.justlovesmile.top/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
    <category term="深度学习" scheme="https://blog.justlovesmile.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="小样本学习" scheme="https://blog.justlovesmile.top/tags/%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Jupyter Lab | 安装、配置、插件推荐、多用户使用教程</title>
    <link href="https://blog.justlovesmile.top/posts/e05a9ab6.html"/>
    <id>https://blog.justlovesmile.top/posts/e05a9ab6.html</id>
    <published>2021-11-25T09:38:43.000Z</published>
    <updated>2021-11-25T09:38:43.000Z</updated>
    
    <content type="html"><![CDATA[<p>首先相信很多使用过python的人都或多或少地了解过<code>Jupyter Notebook</code>这个应用。<code>Jupyter Notebook</code>是一个开源Web应用程序，可让用户创建和共享包含实时代码、公式、可视化和叙述文本的文档。 用途包括：数据清理和转换、数值模拟、统计建模、数据可视化、机器学习等等。</p><p>而<code>Jupyter Lab</code>则是Jupyter的下一代笔记本界面。<code>Jupyter Lab</code> 是一个基于Web的交互式开发环境，用于Jupyter notebook、代码和数据。 <code>Jupyter Lab</code> 非常灵活，可支持数据科学、科学计算和机器学习领域的广泛工作。 <code>Jupyter Lab</code> 是可扩展和模块化的,其可编写插件来添加新组件并与现有组件相集成。</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/image-20211125180351537.png" alt="image-20211125180351537"></p><h1 id="Jupyter-Lab安装和配置"><a href="#Jupyter-Lab安装和配置" class="headerlink" title="Jupyter Lab安装和配置"></a>Jupyter Lab安装和配置</h1><h2 id="1-Jupyter-Lab安装"><a href="#1-Jupyter-Lab安装" class="headerlink" title="1.Jupyter Lab安装"></a>1.Jupyter Lab安装</h2><p>首先进入自己的<code>Python</code>环境或者其他<code>Conda</code>虚拟环境：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source activate XXXXXXX</span><br></pre></td></tr></table></figure><p>然后在<code>terminal</code>或者<code>cmd</code>输入安装命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install jupyterlab</span><br><span class="line">//或者</span><br><span class="line">conda install -c conda-forge jupyterlab</span><br></pre></td></tr></table></figure><p>等待安装完成！</p><h2 id="2-Jupyter-Lab配置"><a href="#2-Jupyter-Lab配置" class="headerlink" title="2.Jupyter Lab配置"></a>2.Jupyter Lab配置</h2><p>使用命令创建配置文件，其会生成<code>C:\Users\用户名\.jupyter\jupyter_notebook_config.py</code>或者<code>/home/用户名/.jupyter/jupyter_notebook_config.py</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter lab --generate-config</span><br></pre></td></tr></table></figure><p>使用编辑器打开配置文件，在文件上方添加：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">c.ServerApp.ip = <span class="string">&#x27;*&#x27;</span></span><br><span class="line">c.ServerApp.port = <span class="number">8000</span></span><br><span class="line">c.ServerApp.open_browser = <span class="literal">False</span></span><br><span class="line">c.ServerApp.root_dir = <span class="string">&#x27;/xxxx/xxxx/xxx&#x27;</span> </span><br><span class="line">c.ServerApp.password_required = <span class="literal">True</span></span><br><span class="line">c.ServerApp.password = <span class="string">&#x27;xxxxxxx&#x27;</span></span><br></pre></td></tr></table></figure><p>其中<code>ip</code>代表允许访问的ip，<code>*</code>代表全部，<code>port</code>用于设置端口，<code>open_browser</code>用于设置启动lab时是否打开浏览器，<code>root_dir</code>用于设置lab启动文件夹根路径，<code>password_required</code>用于设置是否需要密码，<code>password</code>用于设置（加密）密码，这个加密密码的获取方式如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#打开python或者ipython环境</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> notebook.auth <span class="keyword">import</span> passwd</span><br><span class="line">passwd()</span><br><span class="line"><span class="comment">#Enter password: </span></span><br><span class="line"><span class="comment">#Verify password: </span></span><br><span class="line"><span class="comment">#Out[2]: &#x27;argon2:f704bjkasjdfkjasdjfkasjdkjfklmasjdfkalflakdkf&#x27;</span></span><br></pre></td></tr></table></figure><p>复制上方输出的加密密码即可。</p><p>当然也可以在<code>Terminal</code>强制设置/修改密码：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter lab password</span><br></pre></td></tr></table></figure><p>更多配置可以查看默认配置文件下方的注释！</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/image-20211120214858878.png" alt="image-20211120214858878"></p><h2 id="3-Jupyter-Lab启动"><a href="#3-Jupyter-Lab启动" class="headerlink" title="3. Jupyter Lab启动"></a>3. Jupyter Lab启动</h2><p>在<code>Terminal</code>输入:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter lab -p 9090 --no-browser</span><br></pre></td></tr></table></figure><p>更多启动命名可通过<code>jupyter lab --help</code>查看，启动之后即可在浏览器输入：ip+端口 ，进行访问，如：<code>127.0.0.0:9090</code>。</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/image-20211120215159090.png" alt="image-20211120215159090"></p><h2 id="4-Jupyter-Lab插件推荐"><a href="#4-Jupyter-Lab插件推荐" class="headerlink" title="4. Jupyter Lab插件推荐"></a>4. Jupyter Lab插件推荐</h2><p>首先启动Jupyter Lab，在Lab中打开菜单栏的<code>Setting</code>里的<code>Advanced Setting Editor</code>，接着找到<code>Extension Manager</code>，并在右边填入<code>&#123;&#39;enabled&#39;:true&#125;</code></p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/image-20211120215625334.png" alt="image-20211120215625334"></p><p>然后即可在左边菜单栏找到插件安装符号，在里面就可以搜索插件，推荐如下：</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/image-20211120215754890.png" alt="image-20211120215754890"></p><ul><li>theme-darcula：一个好看的主题配色</li><li>jupyterlab_go_to_definition：跳转到定义</li><li>jupyterlab_lsp：代码跳转+代码补全</li><li>还有很多如：latex，git，html，plotly，bokeh，matplotlib，drawio等等</li></ul><h2 id="5-Jupyter-Lab多用户使用"><a href="#5-Jupyter-Lab多用户使用" class="headerlink" title="5. Jupyter Lab多用户使用"></a>5. Jupyter Lab多用户使用</h2><p>复制配置文件到指定位置，例如:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp /home/admin555/.jupyter/jupyter_notebook_config.py /指定位置/jupyter_notebook_config.py</span><br></pre></td></tr></table></figure><p>之后启动时，使用命令：</p> <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter lab --config /指定位置/jupyter_notebook_config.py</span><br></pre></td></tr></table></figure><p> 我的博客即将同步至腾讯云+社区，邀请大家一同入驻：<a href="https://cloud.tencent.com/developer/support-plan?invite_code=1izx0kxkb2lzz">https://cloud.tencent.com/developer/support-plan?invite_code=1izx0kxkb2lzz</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;首先相信很多使用过python的人都或多或少地了解过&lt;code&gt;Jupyter Notebook&lt;/code&gt;这个应用。&lt;code&gt;Jupyter Notebook&lt;/code&gt;是一个开源Web应用程序，可让用户创建和共享包含实时代码、公式、可视化和叙述文本的文档。 用途包</summary>
      
    
    
    
    <category term="随笔记录" scheme="https://blog.justlovesmile.top/categories/%E9%9A%8F%E7%AC%94%E8%AE%B0%E5%BD%95/"/>
    
    
    <category term="Jupyter Lab" scheme="https://blog.justlovesmile.top/tags/Jupyter-Lab/"/>
    
    <category term="教程" scheme="https://blog.justlovesmile.top/tags/%E6%95%99%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>目标检测 | 常用数据集标注格式及生成脚本</title>
    <link href="https://blog.justlovesmile.top/posts/865c56ba.html"/>
    <id>https://blog.justlovesmile.top/posts/865c56ba.html</id>
    <published>2021-09-11T07:11:26.000Z</published>
    <updated>2021-09-11T07:11:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>目标检测是计算机视觉任务中的一个重要研究方向，其用于解决对数码图像中特定种类的可视目标实例的检测问题。目标检测作为计算机视觉的根本性问题之一，是其他诸多计算机视觉任务，例如图像描述生成，实例分割和目标跟踪的基础以及前提。而在解决此类问题时，我们常常需要使用自己的脚本或者利用标注工具生成数据集，数据集格式往往会多种多样，因此对于目标检测任务而言，为了更好地兼容训练，大多数目标检测模型框架会默认支持几种常用的数据集标注格式，常见的分别是COCO，Pascal VOC，YOLO等等。本文主要介绍上述几种数据集格式以及我写的Python脚本（一般需要根据实际情况再改改）。</p><h1 id="1-COCO"><a href="#1-COCO" class="headerlink" title="1. COCO"></a>1. COCO</h1><h2 id="1-1-COCO数据集格式"><a href="#1-1-COCO数据集格式" class="headerlink" title="1.1 COCO数据集格式"></a>1.1 COCO数据集格式</h2><p>COCO（Common Objects in COtext）数据集，是一个大规模的，适用于目标检测，图像分割，Image Captioning任务的数据集，其标注格式是最常用的几种格式之一。目前使用较多的是COCO2017数据集。其官网为<a href="https://cocodataset.org/">COCO - Common Objects in Context (cocodataset.org)</a>。</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/202109111535004.png" alt="image-20210911153516753"></p><p>COCO数据集主要包含图像（jpg或者png等等）和标注文件（json），其数据集格式如下(<code>/</code>代表文件夹)：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">-coco/</span><br><span class="line">    |-train2017/</span><br><span class="line">    |-1.jpg</span><br><span class="line">    |-2.jpg</span><br><span class="line">    |-val2017/</span><br><span class="line">    |-3.jpg</span><br><span class="line">    |-4.jpg</span><br><span class="line">    |-test2017/</span><br><span class="line">    |-5.jpg</span><br><span class="line">    |-6.jpg</span><br><span class="line">    |-annotations/</span><br><span class="line">    |-instances_train2017.json</span><br><span class="line">    |-instances_val2017.json</span><br><span class="line">    |-*.json</span><br></pre></td></tr></table></figure><p><code>train2017</code>以及<code>val2017</code>这两个文件夹中存储的是训练集和验证集的图像，而<code>test2017</code>文件夹中存储的是测试集的信息，可以只是图像，也可以包含标注，一般是单独使用的。</p><p><code>annotations</code>文件夹中的文件就是标注文件，如果你有<code>xml</code>文件，通常需要转换成<code>json</code>格式，其格式如下（更详细的可以参考<a href="https://cocodataset.org/#format-data">官网</a>）：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"><span class="attr">&quot;info&quot;</span>: info, </span><br><span class="line"><span class="attr">&quot;images&quot;</span>: [image], <span class="comment">//列表</span></span><br><span class="line"><span class="attr">&quot;annotations&quot;</span>: [annotation], <span class="comment">//列表</span></span><br><span class="line"><span class="attr">&quot;categories&quot;</span>: [category], <span class="comment">//列表</span></span><br><span class="line"><span class="attr">&quot;licenses&quot;</span>: [license], <span class="comment">//列表</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中<code>info</code>为整个数据集的信息，包括年份，版本，描述等等信息，如果只是完成训练任务，其实不太重要，如下所示：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//对于训练，不是那么的重要</span></span><br><span class="line">info&#123;</span><br><span class="line"><span class="attr">&quot;year&quot;</span>: int, </span><br><span class="line"><span class="attr">&quot;version&quot;</span>: str, </span><br><span class="line"><span class="attr">&quot;description&quot;</span>: str, </span><br><span class="line"><span class="attr">&quot;contributor&quot;</span>: str, </span><br><span class="line"><span class="attr">&quot;url&quot;</span>: str, </span><br><span class="line"><span class="attr">&quot;date_created&quot;</span>: datetime,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中的<code>image</code>为图像的基本信息，包括序号，宽高，文件名等等信息，其中的序号（<code>id</code>）需要和后面的<code>annotations</code>中的标注所属图片序号对应如下所示：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">image&#123;</span><br><span class="line"><span class="attr">&quot;id&quot;</span>: int, <span class="comment">//必要</span></span><br><span class="line"><span class="attr">&quot;width&quot;</span>: int, <span class="comment">//必要</span></span><br><span class="line"><span class="attr">&quot;height&quot;</span>: int, <span class="comment">//必要</span></span><br><span class="line"><span class="attr">&quot;file_name&quot;</span>: str, <span class="comment">//必要</span></span><br><span class="line"><span class="attr">&quot;license&quot;</span>: int,</span><br><span class="line"><span class="attr">&quot;flickr_url&quot;</span>: str,</span><br><span class="line"><span class="attr">&quot;coco_url&quot;</span>: str,</span><br><span class="line"><span class="attr">&quot;date_captured&quot;</span>: datetime, </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中的<code>annotation</code>是最重要的标注信息，包括序号，所属图像序号，类别序号等等信息，如下所示：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">annotation&#123;</span><br><span class="line"><span class="attr">&quot;id&quot;</span>: int, <span class="comment">//标注id</span></span><br><span class="line"><span class="attr">&quot;image_id&quot;</span>: int, <span class="comment">//所属图像id</span></span><br><span class="line"><span class="attr">&quot;category_id&quot;</span>: int, <span class="comment">//类别id</span></span><br><span class="line"><span class="attr">&quot;segmentation&quot;</span>: RLE or [polygon], <span class="comment">//图像分割标注</span></span><br><span class="line"><span class="attr">&quot;area&quot;</span>: float, <span class="comment">//区域面积</span></span><br><span class="line"><span class="attr">&quot;bbox&quot;</span>: [x,y,width,height], <span class="comment">//目标框左上角坐标以及宽高</span></span><br><span class="line"><span class="attr">&quot;iscrowd&quot;</span>: <span class="number">0</span> or <span class="number">1</span>, <span class="comment">//是否密集</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中的<code>category</code>代表类别信息，包括父类别，类别序号以及类别名称，如下所示：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">category&#123;</span><br><span class="line"><span class="attr">&quot;id&quot;</span>: int, <span class="comment">//类别序号</span></span><br><span class="line"><span class="attr">&quot;name&quot;</span>: str, <span class="comment">//类别名称</span></span><br><span class="line"><span class="attr">&quot;supercategory&quot;</span>: str, <span class="comment">//父类别</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中的<code>license</code>代表数据集的协议许可信息，包括序号，协议名称以及链接信息，如下所示：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//对于训练，不重要</span></span><br><span class="line">license&#123;</span><br><span class="line"><span class="attr">&quot;id&quot;</span>: int, </span><br><span class="line"><span class="attr">&quot;name&quot;</span>: str, </span><br><span class="line"><span class="attr">&quot;url&quot;</span>: str,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接下来，我们来看一个简单的示例：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"><span class="attr">&quot;info&quot;</span>: &#123;略&#125;, <span class="attr">&quot;images&quot;</span>: [&#123;<span class="attr">&quot;id&quot;</span>: <span class="number">1</span>, <span class="attr">&quot;file_name&quot;</span>: <span class="string">&quot;1.jpg&quot;</span>, <span class="attr">&quot;height&quot;</span>: <span class="number">334</span>, <span class="attr">&quot;width&quot;</span>: <span class="number">500</span>&#125;, &#123;<span class="attr">&quot;id&quot;</span>: <span class="number">2</span>, <span class="attr">&quot;file_name&quot;</span>: <span class="string">&quot;2.jpg&quot;</span>, <span class="attr">&quot;height&quot;</span>: <span class="number">445</span>, <span class="attr">&quot;width&quot;</span>: <span class="number">556</span>&#125;], <span class="attr">&quot;annotations&quot;</span>: [&#123;<span class="attr">&quot;id&quot;</span>: <span class="number">1</span>, <span class="attr">&quot;area&quot;</span>: <span class="number">40448</span>, <span class="attr">&quot;iscrowd&quot;</span>: <span class="number">0</span>, <span class="attr">&quot;image_id&quot;</span>: <span class="number">1</span>, <span class="attr">&quot;bbox&quot;</span>: [<span class="number">246</span>, <span class="number">61</span>, <span class="number">128</span>, <span class="number">316</span>], <span class="attr">&quot;category_id&quot;</span>: <span class="number">3</span>, <span class="attr">&quot;segmentation&quot;</span>: []&#125;, &#123;<span class="attr">&quot;id&quot;</span>: <span class="number">2</span>, <span class="attr">&quot;area&quot;</span>: <span class="number">40448</span>, <span class="attr">&quot;iscrowd&quot;</span>: <span class="number">0</span>, <span class="attr">&quot;image_id&quot;</span>: <span class="number">1</span>, <span class="attr">&quot;bbox&quot;</span>: [<span class="number">246</span>, <span class="number">61</span>, <span class="number">128</span>, <span class="number">316</span>], <span class="attr">&quot;category_id&quot;</span>: <span class="number">2</span>, <span class="attr">&quot;segmentation&quot;</span>: []&#125;, &#123;<span class="attr">&quot;id&quot;</span>: <span class="number">3</span>, <span class="attr">&quot;area&quot;</span>: <span class="number">40448</span>, <span class="attr">&quot;iscrowd&quot;</span>: <span class="number">0</span>, <span class="attr">&quot;image_id&quot;</span>: <span class="number">2</span>, <span class="attr">&quot;bbox&quot;</span>: [<span class="number">246</span>, <span class="number">61</span>, <span class="number">128</span>, <span class="number">316</span>], <span class="attr">&quot;category_id&quot;</span>: <span class="number">1</span>, <span class="attr">&quot;segmentation&quot;</span>: []&#125;], <span class="attr">&quot;categories&quot;</span>: [&#123;<span class="attr">&quot;supercategory&quot;</span>: <span class="string">&quot;none&quot;</span>, <span class="attr">&quot;id&quot;</span>: <span class="number">1</span>, <span class="attr">&quot;name&quot;</span>: <span class="string">&quot;liner&quot;</span>&#125;,&#123;<span class="attr">&quot;supercategory&quot;</span>: <span class="string">&quot;none&quot;</span>, <span class="attr">&quot;id&quot;</span>: <span class="number">2</span>, <span class="attr">&quot;name&quot;</span>: <span class="string">&quot;containership&quot;</span>&#125;,&#123;<span class="attr">&quot;supercategory&quot;</span>: <span class="string">&quot;none&quot;</span>, <span class="attr">&quot;id&quot;</span>: <span class="number">3</span>, <span class="attr">&quot;name&quot;</span>: <span class="string">&quot;bulkcarrier&quot;</span>&#125;], <span class="attr">&quot;licenses&quot;</span>: [&#123;略&#125;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="1-2-COCO转换脚本"><a href="#1-2-COCO转换脚本" class="headerlink" title="1.2 COCO转换脚本"></a>1.2 COCO转换脚本</h2><p><code>Python转换脚本</code>如下所示，需要准备<code>图像</code>和<code>xml</code>标注文件：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Author    : justlovesmile</span></span><br><span class="line"><span class="comment"># @Date      : 2021/9/8 15:36</span></span><br><span class="line"><span class="keyword">import</span> os, random, json</span><br><span class="line"><span class="keyword">import</span> shutil <span class="keyword">as</span> sh</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> xml.etree.ElementTree <span class="keyword">as</span> xmlET</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mkdir</span>(<span class="params">path</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(path):</span><br><span class="line">        os.makedirs(path)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;The path (<span class="subst">&#123;path&#125;</span>) already exists.&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readxml</span>(<span class="params">file</span>):</span></span><br><span class="line">    tree = xmlET.parse(file)</span><br><span class="line">    <span class="comment">#图片尺寸字段</span></span><br><span class="line">    size = tree.find(<span class="string">&#x27;size&#x27;</span>)</span><br><span class="line">    width = <span class="built_in">int</span>(size.find(<span class="string">&#x27;width&#x27;</span>).text)</span><br><span class="line">    height = <span class="built_in">int</span>(size.find(<span class="string">&#x27;height&#x27;</span>).text)</span><br><span class="line">    <span class="comment">#目标字段</span></span><br><span class="line">    objs = tree.findall(<span class="string">&#x27;object&#x27;</span>)</span><br><span class="line">    bndbox = []</span><br><span class="line">    <span class="keyword">for</span> obj <span class="keyword">in</span> objs:</span><br><span class="line">        label = obj.find(<span class="string">&quot;name&quot;</span>).text</span><br><span class="line">        bnd = obj.find(<span class="string">&quot;bndbox&quot;</span>)</span><br><span class="line">        xmin = <span class="built_in">int</span>(bnd.find(<span class="string">&quot;xmin&quot;</span>).text)</span><br><span class="line">        ymin = <span class="built_in">int</span>(bnd.find(<span class="string">&quot;ymin&quot;</span>).text)</span><br><span class="line">        xmax = <span class="built_in">int</span>(bnd.find(<span class="string">&quot;xmax&quot;</span>).text)</span><br><span class="line">        ymax = <span class="built_in">int</span>(bnd.find(<span class="string">&quot;ymax&quot;</span>).text)</span><br><span class="line">        bbox = [xmin, ymin, xmax, ymax, label]</span><br><span class="line">        bndbox.append(bbox)</span><br><span class="line">    <span class="keyword">return</span> [[width, height], bndbox]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tococo</span>(<span class="params">xml_root, image_root, output_root,classes=&#123;&#125;,errorId=[],train_percent=<span class="number">0.9</span></span>):</span></span><br><span class="line">    <span class="comment"># assert</span></span><br><span class="line">    <span class="keyword">assert</span> train_percent&lt;=<span class="number">1</span> <span class="keyword">and</span> <span class="built_in">len</span>(classes)&gt;<span class="number">0</span></span><br><span class="line">    <span class="comment"># define the root path</span></span><br><span class="line">    train_root = os.path.join(output_root, <span class="string">&quot;train2017&quot;</span>)</span><br><span class="line">    val_root = os.path.join(output_root, <span class="string">&quot;val2017&quot;</span>)</span><br><span class="line">    ann_root = os.path.join(output_root, <span class="string">&quot;annotations&quot;</span>)</span><br><span class="line">    <span class="comment"># initialize train and val dict</span></span><br><span class="line">    train_content = &#123;</span><br><span class="line">        <span class="string">&quot;images&quot;</span>: [],  <span class="comment"># &#123;&quot;file_name&quot;: &quot;09780.jpg&quot;, &quot;height&quot;: 334, &quot;width&quot;: 500, &quot;id&quot;: 9780&#125;</span></span><br><span class="line">        <span class="string">&quot;annotations&quot;</span>: [],<span class="comment"># &#123;&quot;area&quot;: 40448, &quot;iscrowd&quot;: 0, &quot;image_id&quot;: 1, &quot;bbox&quot;: [246, 61, 128, 316], &quot;category_id&quot;: 5, &quot;id&quot;: 1, &quot;segmentation&quot;: []&#125;</span></span><br><span class="line">        <span class="string">&quot;categories&quot;</span>: []  <span class="comment"># &#123;&quot;supercategory&quot;: &quot;none&quot;, &quot;id&quot;: 1, &quot;name&quot;: &quot;liner&quot;&#125;</span></span><br><span class="line">    &#125;</span><br><span class="line">    val_content = &#123;</span><br><span class="line">        <span class="string">&quot;images&quot;</span>: [],  <span class="comment"># &#123;&quot;file_name&quot;: &quot;09780.jpg&quot;, &quot;height&quot;: 334, &quot;width&quot;: 500, &quot;id&quot;: 9780&#125;</span></span><br><span class="line">        <span class="string">&quot;annotations&quot;</span>: [],<span class="comment"># &#123;&quot;area&quot;: 40448, &quot;iscrowd&quot;: 0, &quot;image_id&quot;: 1, &quot;bbox&quot;: [246, 61, 128, 316], &quot;category_id&quot;: 5, &quot;id&quot;: 1, &quot;segmentation&quot;: []&#125;</span></span><br><span class="line">        <span class="string">&quot;categories&quot;</span>: []  <span class="comment"># &#123;&quot;supercategory&quot;: &quot;none&quot;, &quot;id&quot;: 1, &quot;name&quot;: &quot;liner&quot;&#125;</span></span><br><span class="line">    &#125;</span><br><span class="line">    train_json = <span class="string">&#x27;instances_train2017.json&#x27;</span></span><br><span class="line">    val_json = <span class="string">&#x27;instances_val2017.json&#x27;</span></span><br><span class="line">    <span class="comment"># divide the trainset and valset</span></span><br><span class="line">    images = os.listdir(image_root)</span><br><span class="line">    total_num = <span class="built_in">len</span>(images)</span><br><span class="line">    train_percent = train_percent</span><br><span class="line">    train_num = <span class="built_in">int</span>(total_num * train_percent)</span><br><span class="line">    train_file = <span class="built_in">sorted</span>(random.sample(images, train_num))</span><br><span class="line">    <span class="keyword">if</span> mkdir(output_root):</span><br><span class="line">        <span class="keyword">if</span> mkdir(train_root) <span class="keyword">and</span> mkdir(val_root) <span class="keyword">and</span> mkdir(ann_root):</span><br><span class="line">            idx1, idx2, dx1, dx2 = <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> file <span class="keyword">in</span> tqdm(images):</span><br><span class="line">                name=os.path.splitext(os.path.basename(file))[<span class="number">0</span>]</span><br><span class="line">                <span class="keyword">if</span> name <span class="keyword">not</span> <span class="keyword">in</span> errorId:</span><br><span class="line">                    res = readxml(os.path.join(xml_root, name + <span class="string">&#x27;.xml&#x27;</span>))</span><br><span class="line">                    <span class="keyword">if</span> file <span class="keyword">in</span> train_file:</span><br><span class="line">                        idx1 += <span class="number">1</span></span><br><span class="line">                        sh.copy(os.path.join(image_root, file), train_root)</span><br><span class="line">                        train_content[<span class="string">&#x27;images&#x27;</span>].append(</span><br><span class="line">                            &#123;<span class="string">&quot;file_name&quot;</span>: file, <span class="string">&quot;width&quot;</span>: res[<span class="number">0</span>][<span class="number">0</span>], <span class="string">&quot;height&quot;</span>: res[<span class="number">0</span>][<span class="number">1</span>], <span class="string">&quot;id&quot;</span>: idx1&#125;)</span><br><span class="line">                        <span class="keyword">for</span> b <span class="keyword">in</span> res[<span class="number">1</span>]:</span><br><span class="line">                            dx1 += <span class="number">1</span></span><br><span class="line">                            x = b[<span class="number">0</span>]</span><br><span class="line">                            y = b[<span class="number">1</span>]</span><br><span class="line">                            w = b[<span class="number">2</span>] - b[<span class="number">0</span>]</span><br><span class="line">                            h = b[<span class="number">3</span>] - b[<span class="number">1</span>]</span><br><span class="line">                            train_content[<span class="string">&#x27;annotations&#x27;</span>].append(</span><br><span class="line">                                &#123;<span class="string">&quot;area&quot;</span>: w * h, <span class="string">&quot;iscrowd&quot;</span>: <span class="number">0</span>, <span class="string">&quot;image_id&quot;</span>: idx1, <span class="string">&quot;bbox&quot;</span>: [x, y, w, h],</span><br><span class="line">                                 <span class="string">&quot;category_id&quot;</span>: classes[b[<span class="number">4</span>]], <span class="string">&quot;id&quot;</span>: dx1, <span class="string">&quot;segmentation&quot;</span>: []&#125;)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        idx2 += <span class="number">1</span></span><br><span class="line">                        sh.copy(os.path.join(image_root, file), val_root)</span><br><span class="line">                        val_content[<span class="string">&#x27;images&#x27;</span>].append(</span><br><span class="line">                            &#123;<span class="string">&quot;file_name&quot;</span>: file, <span class="string">&quot;width&quot;</span>: res[<span class="number">0</span>][<span class="number">0</span>], <span class="string">&quot;height&quot;</span>: res[<span class="number">0</span>][<span class="number">1</span>], <span class="string">&quot;id&quot;</span>: idx2&#125;)</span><br><span class="line">                        <span class="keyword">for</span> b <span class="keyword">in</span> res[<span class="number">1</span>]:</span><br><span class="line">                            dx2 += <span class="number">1</span></span><br><span class="line">                            x = b[<span class="number">0</span>]</span><br><span class="line">                            y = b[<span class="number">1</span>]</span><br><span class="line">                            w = b[<span class="number">2</span>] - b[<span class="number">0</span>]</span><br><span class="line">                            h = b[<span class="number">3</span>] - b[<span class="number">1</span>]</span><br><span class="line">                            val_content[<span class="string">&#x27;annotations&#x27;</span>].append(</span><br><span class="line">                                &#123;<span class="string">&quot;area&quot;</span>: w * h, <span class="string">&quot;iscrowd&quot;</span>: <span class="number">0</span>, <span class="string">&quot;image_id&quot;</span>: idx2, <span class="string">&quot;bbox&quot;</span>: [x, y, w, h],</span><br><span class="line">                                 <span class="string">&quot;category_id&quot;</span>: classes[b[<span class="number">4</span>]], <span class="string">&quot;id&quot;</span>: dx2, <span class="string">&quot;segmentation&quot;</span>: []&#125;)</span><br><span class="line">            <span class="keyword">for</span> i, j <span class="keyword">in</span> classes.items():</span><br><span class="line">                train_content[<span class="string">&#x27;categories&#x27;</span>].append(&#123;<span class="string">&quot;supercategory&quot;</span>: <span class="string">&quot;none&quot;</span>, <span class="string">&quot;id&quot;</span>: j, <span class="string">&quot;name&quot;</span>: i&#125;)</span><br><span class="line">                val_content[<span class="string">&#x27;categories&#x27;</span>].append(&#123;<span class="string">&quot;supercategory&quot;</span>: <span class="string">&quot;none&quot;</span>, <span class="string">&quot;id&quot;</span>: j, <span class="string">&quot;name&quot;</span>: i&#125;)</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(ann_root, train_json), <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                json.dump(train_content, f)</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(ann_root, val_json), <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                json.dump(val_content, f)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Number of Train Images:&quot;</span>, <span class="built_in">len</span>(os.listdir(train_root)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Number of Val Images:&quot;</span>, <span class="built_in">len</span>(os.listdir(val_root)))</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>():</span></span><br><span class="line">    box_root = <span class="string">&quot;E:/MyProject/Dataset/hwtest/annotations&quot;</span> <span class="comment">#xml文件夹</span></span><br><span class="line">    image_root = <span class="string">&quot;E:/MyProject/Dataset/hwtest/images&quot;</span> <span class="comment">#image文件夹</span></span><br><span class="line">    output_root = <span class="string">&quot;E:/MyProject/Dataset/coco&quot;</span> <span class="comment">#输出文件夹</span></span><br><span class="line">    classes = &#123;<span class="string">&quot;liner&quot;</span>: <span class="number">0</span>,<span class="string">&quot;bulk carrier&quot;</span>: <span class="number">1</span>,<span class="string">&quot;warship&quot;</span>: <span class="number">2</span>,<span class="string">&quot;sailboat&quot;</span>: <span class="number">3</span>,<span class="string">&quot;canoe&quot;</span>: <span class="number">4</span>,<span class="string">&quot;container ship&quot;</span>: <span class="number">5</span>,<span class="string">&quot;fishing boat&quot;</span>: <span class="number">6</span>&#125; <span class="comment">#类别字典</span></span><br><span class="line">    errorId = [] <span class="comment">#脏数据id</span></span><br><span class="line">    train_percent = <span class="number">0.9</span> <span class="comment">#训练集和验证集比例</span></span><br><span class="line">    tococo(box_root, image_root, output_root,classes=classes,errorId=errorId,train_percent=train_percent)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    test()</span><br></pre></td></tr></table></figure><h1 id="2-VOC"><a href="#2-VOC" class="headerlink" title="2. VOC"></a>2. VOC</h1><h2 id="2-1-VOC数据集格式"><a href="#2-1-VOC数据集格式" class="headerlink" title="2.1 VOC数据集格式"></a>2.1 VOC数据集格式</h2><p>VOC（Visual Object Classes）数据集来源于PASCAL VOC挑战赛，其主要任务有<code>Object Classification</code> 、<code>Object Detection</code>、<code>Object Segmentation</code>、<code>Human Layout</code>、<code>Action Classification</code>。其官网为<a href="http://host.robots.ox.ac.uk/pascal/VOC/">The PASCAL Visual Object Classes Homepage (ox.ac.uk)</a>。其主要数据集有VOC2007以及VOC2012。</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/202109111939729.png" alt="image-20210911193933398"></p><p>VOC数据集主要包含图像（jpg或者png等等）和标注文件（xml），其数据集格式如下(<code>/</code>代表文件夹)：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">-VOC/</span><br><span class="line">|-JPEGImages/</span><br><span class="line">|-1.jpg</span><br><span class="line">|-2.jpg</span><br><span class="line">|-Annotations/</span><br><span class="line">|-1.xml</span><br><span class="line">|-2.xml</span><br><span class="line">|-ImageSets/</span><br><span class="line">|-Layout/</span><br><span class="line">|-*.txt</span><br><span class="line">|-Main/</span><br><span class="line">|-train.txt</span><br><span class="line">|-val.txt</span><br><span class="line">|-trainval.txt</span><br><span class="line">|-test.txt</span><br><span class="line">|-Segmentation/</span><br><span class="line">|-*.txt</span><br><span class="line">|-Action/</span><br><span class="line">|-*.txt</span><br><span class="line">|-SegmentationClass/</span><br><span class="line">|-SegmentationObject/</span><br></pre></td></tr></table></figure><p>其中对于目标检测任务而言，最常用的以及必须的文件夹包括：<code>JPEGImages</code>，<code>Annotations</code>，<code>ImageSets/Main</code>。</p><p><code>JPEGImages</code>里存放的是图像，而<code>Annotations</code>里存放的是<code>xml</code>标注文件，文件内容如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">&lt;annotation&gt;</span><br><span class="line">&lt;folder&gt;VOC&lt;/folder&gt;            # 图像所在文件夹</span><br><span class="line">&lt;filename&gt;000032.jpg&lt;/filename&gt; # 图像文件名</span><br><span class="line">&lt;source&gt;                        # 图像源</span><br><span class="line">&lt;database&gt;The VOC Database&lt;/database&gt;</span><br><span class="line">&lt;annotation&gt;PASCAL VOC&lt;/annotation&gt;</span><br><span class="line">&lt;image&gt;flickr&lt;/image&gt;</span><br><span class="line">&lt;/source&gt;</span><br><span class="line">&lt;size&gt;                          # 图像尺寸信息</span><br><span class="line">&lt;width&gt;500&lt;/width&gt;    # 图像宽度</span><br><span class="line">&lt;height&gt;281&lt;/height&gt;  # 图像高度</span><br><span class="line">&lt;depth&gt;3&lt;/depth&gt;      # 图像通道数</span><br><span class="line">&lt;/size&gt;</span><br><span class="line">&lt;segmented&gt;0&lt;/segmented&gt;  # 图像是否用于分割，0代表不适用，对目标检测而言没关系</span><br><span class="line">&lt;object&gt;                  # 一个目标对象的信息</span><br><span class="line">&lt;name&gt;aeroplane&lt;/name&gt;    # 目标的类别名</span><br><span class="line">&lt;pose&gt;Frontal&lt;/pose&gt;      # 拍摄角度，若无一般为Unspecified</span><br><span class="line">&lt;truncated&gt;0&lt;/truncated&gt;  # 是否被截断，0表示完整未截断</span><br><span class="line">&lt;difficult&gt;0&lt;/difficult&gt;  # 是否难以识别，0表示不难识别</span><br><span class="line">&lt;bndbox&gt;            # 边界框信息</span><br><span class="line">&lt;xmin&gt;104&lt;/xmin&gt;  # 左上角x</span><br><span class="line">&lt;ymin&gt;78&lt;/ymin&gt;   # 左上角y</span><br><span class="line">&lt;xmax&gt;375&lt;/xmax&gt;  # 右下角x</span><br><span class="line">&lt;ymax&gt;183&lt;/ymax&gt;  # 右下角y</span><br><span class="line">&lt;/bndbox&gt;</span><br><span class="line">&lt;/object&gt;</span><br><span class="line">    # 下面是其他目标的信息，这里略掉</span><br><span class="line">&lt;object&gt;</span><br><span class="line">        其他object信息，这里省略</span><br><span class="line">&lt;/object&gt;</span><br><span class="line">&lt;/annotation&gt;</span><br></pre></td></tr></table></figure><h2 id="2-2-VOC转换脚本"><a href="#2-2-VOC转换脚本" class="headerlink" title="2.2 VOC转换脚本"></a>2.2 VOC转换脚本</h2><p>下面这个脚本，只适用于有图像和xml文件的情况下，coco转voc格式以后有需要再写：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Author    : justlovesmile</span></span><br><span class="line"><span class="comment"># @Date      : 2021/9/8 21:01</span></span><br><span class="line"><span class="keyword">import</span> os,random</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> shutil <span class="keyword">as</span> sh</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mkdir</span>(<span class="params">path</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(path):</span><br><span class="line">        os.mkdir(path)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;The path (<span class="subst">&#123;path&#125;</span>) already exists.&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tovoc</span>(<span class="params">xmlroot,imgroot,saveroot,errorId=[],classes=&#123;&#125;,tvp=<span class="number">1.0</span>,trp=<span class="number">0.9</span></span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">        root：数据集存放根目录</span></span><br><span class="line"><span class="string">    功能：</span></span><br><span class="line"><span class="string">        加载数据，并保存为VOC格式</span></span><br><span class="line"><span class="string">    加载后的格式：</span></span><br><span class="line"><span class="string">    VOC/</span></span><br><span class="line"><span class="string">      Annotations/</span></span><br><span class="line"><span class="string">        - **.xml</span></span><br><span class="line"><span class="string">      JPEGImages/</span></span><br><span class="line"><span class="string">        - **.jpg</span></span><br><span class="line"><span class="string">      ImageSets/</span></span><br><span class="line"><span class="string">        Main/</span></span><br><span class="line"><span class="string">          - train.txt</span></span><br><span class="line"><span class="string">          - test.txt</span></span><br><span class="line"><span class="string">          - val.txt</span></span><br><span class="line"><span class="string">          - trainval.txt</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># assert</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(classes)&gt;<span class="number">0</span></span><br><span class="line">    <span class="comment"># init path</span></span><br><span class="line">    VOC = saveroot</span><br><span class="line">    ann_path = os.path.join(VOC, <span class="string">&#x27;Annotations&#x27;</span>)</span><br><span class="line">    img_path = os.path.join(VOC,<span class="string">&#x27;JPEGImages&#x27;</span>)</span><br><span class="line">    set_path = os.path.join(VOC,<span class="string">&#x27;ImageSets&#x27;</span>)</span><br><span class="line">    txt_path = os.path.join(set_path,<span class="string">&#x27;Main&#x27;</span>)</span><br><span class="line">    <span class="comment"># mkdirs </span></span><br><span class="line">    <span class="keyword">if</span> mkdir(VOC):</span><br><span class="line">        <span class="keyword">if</span> mkdir(ann_path) <span class="keyword">and</span> mkdir(img_path) <span class="keyword">and</span> mkdir(set_path):</span><br><span class="line">            mkdir(txt_path)</span><br><span class="line"></span><br><span class="line">    images = os.listdir(imgroot)</span><br><span class="line">    list_index = <span class="built_in">range</span>(<span class="built_in">len</span>(images))</span><br><span class="line">    <span class="comment">#test and trainval set</span></span><br><span class="line">    trainval_percent = tvp</span><br><span class="line">    train_percent = trp</span><br><span class="line">    val_percent = <span class="number">1</span> - train_percent <span class="keyword">if</span> train_percent&lt;<span class="number">1</span> <span class="keyword">else</span> <span class="number">0.1</span></span><br><span class="line">    total_num = <span class="built_in">len</span>(images)</span><br><span class="line">    trainval_num = <span class="built_in">int</span>(total_num*trainval_percent)</span><br><span class="line">    train_num = <span class="built_in">int</span>(trainval_num*train_percent)</span><br><span class="line">    val_num = <span class="built_in">int</span>(trainval_num*val_percent) <span class="keyword">if</span> train_percent&lt;<span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    trainval = random.sample(list_index,trainval_num)</span><br><span class="line">    train = random.sample(list_index,train_num)</span><br><span class="line">    val = random.sample(list_index,val_num)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(list_index):</span><br><span class="line">        imgfile = images[i]</span><br><span class="line">        img_id = os.path.splitext(os.path.basename(imgfile))[<span class="number">0</span>]</span><br><span class="line">        xmlfile = img_id+<span class="string">&quot;.xml&quot;</span></span><br><span class="line">        sh.copy(os.path.join(imgroot,imgfile),os.path.join(img_path,imgfile))</span><br><span class="line">        sh.copy(os.path.join(xmlroot,xmlfile),os.path.join(ann_path,xmlfile))</span><br><span class="line">        <span class="keyword">if</span> img_id <span class="keyword">not</span> <span class="keyword">in</span> errorId:</span><br><span class="line">            <span class="keyword">if</span> i <span class="keyword">in</span> trainval:</span><br><span class="line">                <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(txt_path,<span class="string">&#x27;trainval.txt&#x27;</span>),<span class="string">&#x27;a&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                    f.write(img_id+<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">                <span class="keyword">if</span> i <span class="keyword">in</span> train:</span><br><span class="line">                    <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(txt_path,<span class="string">&#x27;train.txt&#x27;</span>),<span class="string">&#x27;a&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                        f.write(img_id+<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(txt_path,<span class="string">&#x27;val.txt&#x27;</span>),<span class="string">&#x27;a&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                        f.write(img_id+<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">                <span class="keyword">if</span> train_percent==<span class="number">1</span> <span class="keyword">and</span> i <span class="keyword">in</span> val:</span><br><span class="line">                    <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(txt_path,<span class="string">&#x27;val.txt&#x27;</span>),<span class="string">&#x27;a&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                        f.write(img_id+<span class="string">&#x27;\n&#x27;</span>)          </span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(txt_path,<span class="string">&#x27;test.txt&#x27;</span>),<span class="string">&#x27;a&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                    f.write(img_id+<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># end</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Dataset to VOC format finished!&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>():</span></span><br><span class="line">    box_root = <span class="string">&quot;E:/MyProject/Dataset/hwtest/annotations&quot;</span></span><br><span class="line">    image_root = <span class="string">&quot;E:/MyProject/Dataset/hwtest/images&quot;</span></span><br><span class="line">    output_root = <span class="string">&quot;E:/MyProject/Dataset/voc&quot;</span></span><br><span class="line">    classes = &#123;<span class="string">&quot;liner&quot;</span>: <span class="number">0</span>,<span class="string">&quot;bulk carrier&quot;</span>: <span class="number">1</span>,<span class="string">&quot;warship&quot;</span>: <span class="number">2</span>,<span class="string">&quot;sailboat&quot;</span>: <span class="number">3</span>,<span class="string">&quot;canoe&quot;</span>: <span class="number">4</span>,<span class="string">&quot;container ship&quot;</span>: <span class="number">5</span>,<span class="string">&quot;fishing boat&quot;</span>: <span class="number">6</span>&#125;</span><br><span class="line">    errorId = []</span><br><span class="line">    train_percent = <span class="number">0.9</span></span><br><span class="line">    tovoc(box_root,image_root,output_root,errorId,classes,trp=train_percent)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    test()</span><br></pre></td></tr></table></figure><h1 id="3-YOLO"><a href="#3-YOLO" class="headerlink" title="3. YOLO"></a>3. YOLO</h1><h2 id="3-1-YOLO数据集格式"><a href="#3-1-YOLO数据集格式" class="headerlink" title="3.1 YOLO数据集格式"></a>3.1 YOLO数据集格式</h2><p><code>YOLO</code>数据集格式的出现主要是为了训练<code>YOLO</code>模型，其文件格式没有固定的要求，因为可以通过修改模型的配置文件进行数据加载，唯一需要注意的是<code>YOLO</code>数据集的标注格式是将目标框的位置信息进行归一化处理（此处归一化指的是除以图片宽和高），如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;目标类别&#125; &#123;归一化后的目标中心点x坐标&#125; &#123;归一化后的目标中心点y坐标&#125; &#123;归一化后的目标框宽度w&#125; &#123;归一化后的目标框高度h&#125;</span><br></pre></td></tr></table></figure><h2 id="3-2-YOLO转换脚本"><a href="#3-2-YOLO转换脚本" class="headerlink" title="3.2 YOLO转换脚本"></a>3.2 YOLO转换脚本</h2><p><code>Python</code>转换脚本如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Author    : justlovesmile</span></span><br><span class="line"><span class="comment"># @Date      : 2021/9/8 20:28</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> shutil <span class="keyword">as</span> sh</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">import</span> xml.etree.cElementTree <span class="keyword">as</span> et</span><br><span class="line"><span class="keyword">except</span> ImportError:</span><br><span class="line">    <span class="keyword">import</span> xml.etree.ElementTree <span class="keyword">as</span> et</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mkdir</span>(<span class="params">path</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(path):</span><br><span class="line">        os.makedirs(path)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;The path (<span class="subst">&#123;path&#125;</span>) already exists.&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span>  </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">xml2yolo</span>(<span class="params">xmlpath,savepath,classes=&#123;&#125;</span>):</span></span><br><span class="line">    namemap = classes</span><br><span class="line">    <span class="comment">#try:</span></span><br><span class="line">    <span class="comment">#    with open(&#x27;classes_yolo.json&#x27;,&#x27;r&#x27;) as f:</span></span><br><span class="line">    <span class="comment">#        namemap=json.load(f)</span></span><br><span class="line">    <span class="comment">#except:</span></span><br><span class="line">    <span class="comment">#    pass</span></span><br><span class="line">    rt = et.parse(xmlpath).getroot()</span><br><span class="line">    w = <span class="built_in">int</span>(rt.find(<span class="string">&quot;size&quot;</span>).find(<span class="string">&quot;width&quot;</span>).text)</span><br><span class="line">    h = <span class="built_in">int</span>(rt.find(<span class="string">&quot;size&quot;</span>).find(<span class="string">&quot;height&quot;</span>).text)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(savepath, <span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> obj <span class="keyword">in</span> rt.findall(<span class="string">&quot;object&quot;</span>):</span><br><span class="line">            name = obj.find(<span class="string">&quot;name&quot;</span>).text</span><br><span class="line">            xmin = <span class="built_in">int</span>(obj.find(<span class="string">&quot;bndbox&quot;</span>).find(<span class="string">&quot;xmin&quot;</span>).text)</span><br><span class="line">            ymin = <span class="built_in">int</span>(obj.find(<span class="string">&quot;bndbox&quot;</span>).find(<span class="string">&quot;ymin&quot;</span>).text)</span><br><span class="line">            xmax = <span class="built_in">int</span>(obj.find(<span class="string">&quot;bndbox&quot;</span>).find(<span class="string">&quot;xmax&quot;</span>).text)</span><br><span class="line">            ymax = <span class="built_in">int</span>(obj.find(<span class="string">&quot;bndbox&quot;</span>).find(<span class="string">&quot;ymax&quot;</span>).text)</span><br><span class="line">            f.write(</span><br><span class="line">                <span class="string">f&quot;<span class="subst">&#123;namemap[name]&#125;</span> <span class="subst">&#123;(xmin+xmax)/w/<span class="number">2.</span>&#125;</span> <span class="subst">&#123;(ymin+ymax)/h/<span class="number">2.</span>&#125;</span> <span class="subst">&#123;(xmax-xmin)/w&#125;</span> <span class="subst">&#123;(ymax-ymin)/h&#125;</span>&quot;</span></span><br><span class="line">                + <span class="string">&quot;\n&quot;</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainval</span>(<span class="params">xmlroot,imgroot,saveroot,errorId=[],classes=&#123;&#125;,tvp=<span class="number">1.0</span>,trp=<span class="number">0.9</span></span>):</span></span><br><span class="line">    <span class="comment"># assert</span></span><br><span class="line">    <span class="keyword">assert</span> tvp&lt;=<span class="number">1.0</span> <span class="keyword">and</span> trp &lt;=<span class="number">1.0</span> <span class="keyword">and</span> <span class="built_in">len</span>(classes)&gt;<span class="number">0</span></span><br><span class="line">    <span class="comment"># create dirs</span></span><br><span class="line">    imglabel = [<span class="string">&#x27;images&#x27;</span>,<span class="string">&#x27;labels&#x27;</span>]</span><br><span class="line">    trainvaltest = [<span class="string">&#x27;train&#x27;</span>,<span class="string">&#x27;val&#x27;</span>,<span class="string">&#x27;test&#x27;</span>]</span><br><span class="line">    mkdir(saveroot)</span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> imglabel:</span><br><span class="line">        mkdir(os.path.join(saveroot,r))</span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> trainvaltest:</span><br><span class="line">            mkdir(os.path.join(saveroot,r,s))</span><br><span class="line">    <span class="comment">#train / val</span></span><br><span class="line">    trainval_percent = tvp</span><br><span class="line">    train_percent = trp</span><br><span class="line">    val_percent = <span class="number">1</span> - train_percent <span class="keyword">if</span> train_percent&lt;<span class="number">1.0</span> <span class="keyword">else</span> <span class="number">0.15</span></span><br><span class="line">    </span><br><span class="line">    total_img = os.listdir(imgroot)</span><br><span class="line">    num = <span class="built_in">len</span>(total_img)</span><br><span class="line">    list_index = <span class="built_in">range</span>(num)</span><br><span class="line">    tv = <span class="built_in">int</span>(num * trainval_percent)</span><br><span class="line">    tr = <span class="built_in">int</span>(tv * train_percent)</span><br><span class="line">    va = <span class="built_in">int</span>(tv * val_percent)</span><br><span class="line">    trainval = random.sample(list_index, tv) <span class="comment"># trainset and valset</span></span><br><span class="line">    train = random.sample(trainval, tr) <span class="comment"># trainset</span></span><br><span class="line">    val = random.sample(trainval, va) <span class="comment">#valset, use it only when train_percent = 1 </span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;trainval_percent:<span class="subst">&#123;trainval_percent&#125;</span>,train_percent:<span class="subst">&#123;train_percent&#125;</span>,val_percent:<span class="subst">&#123;val_percent&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(list_index):</span><br><span class="line">        name = total_img[i]</span><br><span class="line">        op = os.path.join(imgroot,name)</span><br><span class="line">        file_id = os.path.splitext(os.path.basename(name))[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> file_id <span class="keyword">not</span> <span class="keyword">in</span> errorId:</span><br><span class="line">            xmlp = os.path.join(xmlroot,file_id+<span class="string">&#x27;.xml&#x27;</span>)</span><br><span class="line">            <span class="keyword">if</span> i <span class="keyword">in</span> trainval:</span><br><span class="line">                <span class="comment"># trainset and valset</span></span><br><span class="line">                <span class="keyword">if</span> i <span class="keyword">in</span> train:</span><br><span class="line">                    sp = os.path.join(saveroot,<span class="string">&quot;images&quot;</span>,<span class="string">&quot;train&quot;</span>,name)</span><br><span class="line">                    xml2yolo(xmlp,os.path.join(saveroot,<span class="string">&quot;labels&quot;</span>,<span class="string">&quot;train&quot;</span>,file_id+<span class="string">&#x27;.txt&#x27;</span>),classes)</span><br><span class="line">                    sh.copy(op,sp)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    sp = os.path.join(saveroot,<span class="string">&quot;images&quot;</span>,<span class="string">&quot;val&quot;</span>,name)</span><br><span class="line">                    xml2yolo(xmlp,os.path.join(saveroot,<span class="string">&quot;labels&quot;</span>,<span class="string">&quot;val&quot;</span>,file_id+<span class="string">&#x27;.txt&#x27;</span>),classes)</span><br><span class="line">                    sh.copy(op,sp)</span><br><span class="line">                <span class="keyword">if</span> (train_percent==<span class="number">1.0</span> <span class="keyword">and</span> i <span class="keyword">in</span> val):</span><br><span class="line">                    sp = os.path.join(saveroot,<span class="string">&quot;images&quot;</span>,<span class="string">&quot;val&quot;</span>,name)</span><br><span class="line">                    xml2yolo(xmlp,os.path.join(saveroot,<span class="string">&quot;labels&quot;</span>,<span class="string">&quot;val&quot;</span>,file_id+<span class="string">&#x27;.txt&#x27;</span>),classes)</span><br><span class="line">                    sh.copy(op,sp)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># testset</span></span><br><span class="line">                sp = os.path.join(saveroot,<span class="string">&quot;images&quot;</span>,<span class="string">&quot;test&quot;</span>,name)</span><br><span class="line">                xml2yolo(xmlp,os.path.join(saveroot,<span class="string">&quot;labels&quot;</span>,<span class="string">&quot;test&quot;</span>,file_id+<span class="string">&#x27;.txt&#x27;</span>),classes)</span><br><span class="line">                sh.copy(op,sp)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maketxt</span>(<span class="params"><span class="built_in">dir</span>,saveroot,filename</span>):</span></span><br><span class="line">    savetxt = os.path.join(saveroot,filename)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(savetxt,<span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> tqdm(os.listdir(<span class="built_in">dir</span>)):</span><br><span class="line">            f.write(os.path.join(<span class="built_in">dir</span>,i)+<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">                           </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">toyolo</span>(<span class="params">xmlroot,imgroot,saveroot,errorId=[],classes=&#123;&#125;,tvp=<span class="number">1</span>,train_percent=<span class="number">0.9</span></span>):</span></span><br><span class="line">    <span class="comment"># toyolo main function</span></span><br><span class="line">    trainval(xmlroot,imgroot,saveroot,errorId,classes,tvp,train_percent)</span><br><span class="line">    maketxt(os.path.join(saveroot,<span class="string">&quot;images&quot;</span>,<span class="string">&quot;train&quot;</span>),saveroot,<span class="string">&quot;train.txt&quot;</span>)</span><br><span class="line">    maketxt(os.path.join(saveroot,<span class="string">&quot;images&quot;</span>,<span class="string">&quot;val&quot;</span>),saveroot,<span class="string">&quot;val.txt&quot;</span>)</span><br><span class="line">    maketxt(os.path.join(saveroot,<span class="string">&quot;images&quot;</span>,<span class="string">&quot;test&quot;</span>),saveroot,<span class="string">&quot;test.txt&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Dataset to yolo format success.&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>():</span></span><br><span class="line">    box_root = <span class="string">&quot;E:/MyProject/Dataset/hwtest/annotations&quot;</span></span><br><span class="line">    image_root = <span class="string">&quot;E:/MyProject/Dataset/hwtest/images&quot;</span></span><br><span class="line">    output_root = <span class="string">&quot;E:/MyProject/Dataset/yolo&quot;</span></span><br><span class="line">    classes = &#123;<span class="string">&quot;liner&quot;</span>: <span class="number">0</span>,<span class="string">&quot;bulk carrier&quot;</span>: <span class="number">1</span>,<span class="string">&quot;warship&quot;</span>: <span class="number">2</span>,<span class="string">&quot;sailboat&quot;</span>: <span class="number">3</span>,<span class="string">&quot;canoe&quot;</span>: <span class="number">4</span>,<span class="string">&quot;container ship&quot;</span>: <span class="number">5</span>,<span class="string">&quot;fishing boat&quot;</span>: <span class="number">6</span>&#125;</span><br><span class="line">    errorId = []</span><br><span class="line">    train_percent = <span class="number">0.9</span></span><br><span class="line">    toyolo(box_root,image_root,output_root,errorId,classes,train_percent=train_percent)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    test()</span><br></pre></td></tr></table></figure><p>按照此脚本，将会在输出文件夹中生成以下内容：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">-yolo/</span><br><span class="line">|-images/</span><br><span class="line">|-train/</span><br><span class="line">|-1.jpg</span><br><span class="line">|-2.jpg</span><br><span class="line">|-test/</span><br><span class="line">|-3.jpg</span><br><span class="line">|-4.jpg</span><br><span class="line">|-val/</span><br><span class="line">|-5.jpg</span><br><span class="line">|-6.jpg</span><br><span class="line">|-labels/</span><br><span class="line">|-train/</span><br><span class="line">|-1.txt</span><br><span class="line">|-2.txt</span><br><span class="line">|-test/</span><br><span class="line">|-3.txt</span><br><span class="line">|-4.txt</span><br><span class="line">|-val/</span><br><span class="line">|-5.txt</span><br><span class="line">|-6.txt</span><br><span class="line">|-train.txt</span><br><span class="line">|-test.txt</span><br><span class="line">|-val.txt</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">目标检测是计算机视觉任务中的一个重要研究方向，是计算机视觉的根本性问题之一，是其他诸多计算机视觉任务的基础以及前提。本文主要介绍了目标检测数据集的几种标注格式和转换代码。</summary>
    
    
    
    <category term="人工智能" scheme="https://blog.justlovesmile.top/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
    <category term="python" scheme="https://blog.justlovesmile.top/tags/python/"/>
    
    <category term="深度学习" scheme="https://blog.justlovesmile.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="目标检测" scheme="https://blog.justlovesmile.top/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>Hexo博客 | 动态分类标签条，自动获取全站分类与标签进行展示</title>
    <link href="https://blog.justlovesmile.top/posts/2bfb1caa.html"/>
    <id>https://blog.justlovesmile.top/posts/2bfb1caa.html</id>
    <published>2021-08-14T13:10:23.000Z</published>
    <updated>2021-08-14T13:10:23.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h1><p>本文是对Heo博主写的<a href="https://blog.zhheo.com/p/bc61964d.html">Butterfly魔改：动态分类条，可以根据页面变化而改变的分类列表展示方式</a>文章的补充，增加了动态标签条，并且可以自动获取全站分类和标签名称。</p><h1 id="2-预览"><a href="#2-预览" class="headerlink" title="2. 预览"></a>2. 预览</h1><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/202108142116064.png" alt="image-20210814211626863"></p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/202108142121159.png" alt="image-20210814212127747"></p><h1 id="3-配置"><a href="#3-配置" class="headerlink" title="3. 配置"></a>3. 配置</h1><h2 id="3-1-新建PUG文件"><a href="#3-1-新建PUG文件" class="headerlink" title="3.1 新建PUG文件"></a>3.1 新建PUG文件</h2><p>首先是分类条，在<code>themes/butterfly/layout/includes/</code>处新建文件<code>categoryBar.pug</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#category-bar</span><br><span class="line">  .category-bar-items#category-bar-items</span><br><span class="line">    !=getarray_bar(&quot;category&quot;)</span><br><span class="line">  a.category-bar-more(href=&quot;/categories/&quot;) 更多</span><br></pre></td></tr></table></figure><p>其次是标签条，在<code>themes/butterfly/layout/includes/</code>处新建文件<code>tagBar.pug</code>，因为样式一样，所以没有更改id和class名称。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#category-bar</span><br><span class="line">  .category-bar-items#category-bar-items</span><br><span class="line">    !=getarray_bar(&quot;tag&quot;)</span><br><span class="line">  a.category-bar-more(href=&quot;/tags/&quot;) 更多</span><br></pre></td></tr></table></figure><h2 id="3-2-新建Hexo辅助函数"><a href="#3-2-新建Hexo辅助函数" class="headerlink" title="3.2 新建Hexo辅助函数"></a>3.2 新建Hexo辅助函数</h2><p>在<code>theme/butterfly/scripts/helpers/</code>中创建<code>get_arrays.js</code></p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">hexo.extend.helper.register(<span class="string">&#x27;getarray_bar&#x27;</span>, <span class="function"><span class="keyword">function</span> (<span class="params">types</span>) </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (!types) &#123;</span><br><span class="line">    types = <span class="string">&quot;category&quot;</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">const</span> categoriesBar = <span class="function"><span class="keyword">function</span> (<span class="params">categories</span>) </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (!categories || !categories.length) <span class="keyword">return</span> <span class="string">``</span></span><br><span class="line">    <span class="keyword">const</span> categoryArr = []</span><br><span class="line">    hexo.locals.get(<span class="string">&#x27;categories&#x27;</span>).map(<span class="function"><span class="keyword">function</span> (<span class="params">category</span>) </span>&#123;</span><br><span class="line">      categoryArr.push(&#123; <span class="attr">name</span>: category.name, <span class="attr">value</span>: category.length &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line">    categoryArr.sort(<span class="function">(<span class="params">a, b</span>) =&gt;</span> &#123; <span class="keyword">return</span> b.value - a.value &#125;)</span><br><span class="line">    <span class="keyword">let</span> strCategoriesBar = <span class="string">``</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">let</span> i = <span class="number">0</span>; i &lt; categories.length; i++) &#123;</span><br><span class="line">      strTemp=<span class="string">`</span></span><br><span class="line"><span class="string">      &lt;div class=&quot;category-bar-item&quot; id=&quot;<span class="subst">$&#123;categoryArr[i].name&#125;</span>&quot;&gt;</span></span><br><span class="line"><span class="string">      &lt;a href=&quot;/categories/<span class="subst">$&#123;categoryArr[i].name&#125;</span>/&quot;&gt;<span class="subst">$&#123;categoryArr[i].name&#125;</span>&lt;/a&gt;</span></span><br><span class="line"><span class="string">      &lt;/div&gt;`</span></span><br><span class="line">      strCategoriesBar+=strTemp</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> strCategoriesBar</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">const</span> tagsBar = <span class="function"><span class="keyword">function</span>(<span class="params">tags</span>) </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (!tags || !tags.length) <span class="keyword">return</span> <span class="string">``</span></span><br><span class="line">    <span class="keyword">const</span> tagArr = []</span><br><span class="line">    hexo.locals.get(<span class="string">&#x27;tags&#x27;</span>).map(<span class="function"><span class="keyword">function</span> (<span class="params">tag</span>) </span>&#123;</span><br><span class="line">      tagArr.push(&#123; <span class="attr">name</span>: tag.name, <span class="attr">value</span>: tag.length &#125;)</span><br><span class="line">    &#125;)</span><br><span class="line">    tagArr.sort(<span class="function">(<span class="params">a, b</span>) =&gt;</span> &#123; <span class="keyword">return</span> b.value - a.value &#125;)</span><br><span class="line">    <span class="keyword">let</span> strTagsBar = <span class="string">``</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">let</span> i = <span class="number">0</span>; i &lt; tags.length; i++) &#123;</span><br><span class="line">      strTemp=<span class="string">`</span></span><br><span class="line"><span class="string">      &lt;div class=&quot;category-bar-item&quot; id=&quot;<span class="subst">$&#123;tagArr[i].name&#125;</span>&quot;&gt;</span></span><br><span class="line"><span class="string">      &lt;a href=&quot;/tags/<span class="subst">$&#123;tagArr[i].name&#125;</span>/&quot;&gt;<span class="subst">$&#123;tagArr[i].name&#125;</span>&lt;/a&gt;</span></span><br><span class="line"><span class="string">      &lt;/div&gt;`</span></span><br><span class="line">      strTagsBar+=strTemp</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> strTagsBar</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (types == <span class="string">&quot;category&quot;</span>)&#123;</span><br><span class="line">    <span class="keyword">return</span> categoriesBar(<span class="built_in">this</span>.site.categories)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (types == <span class="string">&quot;tag&quot;</span>)&#123;</span><br><span class="line">    <span class="keyword">return</span> tagsBar(<span class="built_in">this</span>.site.tags)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><h2 id="3-3-引用模块"><a href="#3-3-引用模块" class="headerlink" title="3.3 引用模块"></a>3.3 引用模块</h2><p>在需要的位置引用该模块，例如：</p><p>在分类页面引用：找到<code>theme/butterfly/layout/category.pug</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">extends includes/layout.pug</span><br><span class="line"></span><br><span class="line">block content</span><br><span class="line">  if theme.category_ui == &#x27;index&#x27;</span><br><span class="line">    include ./includes/mixins/post-ui.pug</span><br><span class="line">    #recent-posts.recent-posts.category_ui   </span><br><span class="line">      +postUI</span><br><span class="line">      include includes/pagination.pug    </span><br><span class="line">  else</span><br><span class="line">    include ./includes/mixins/article-sort.pug</span><br><span class="line">    #category</span><br><span class="line">+      .category-in-bar</span><br><span class="line">+        .category-in-bar-tips</span><br><span class="line">+          i.fa-fw.fas.fa-folder-open</span><br><span class="line">+        include includes/categoryBar.pug</span><br><span class="line">      .article-sort-title= _p(&#x27;page.category&#x27;) + &#x27; - &#x27; + page.category</span><br><span class="line">      +articleSort(page.posts)</span><br><span class="line">      include includes/pagination.pug</span><br></pre></td></tr></table></figure><p>在标签页引用：找到<code>theme/butterfly/layout/tag.pug</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">extends includes/layout.pug</span><br><span class="line"></span><br><span class="line">block content</span><br><span class="line">  if theme.tag_ui == &#x27;index&#x27;</span><br><span class="line">    include ./includes/mixins/post-ui.pug</span><br><span class="line">    #recent-posts.recent-posts</span><br><span class="line">      +postUI</span><br><span class="line">      include includes/pagination.pug</span><br><span class="line">  else</span><br><span class="line">    include ./includes/mixins/article-sort.pug</span><br><span class="line">    #tag</span><br><span class="line">+      .category-in-bar</span><br><span class="line">+        .category-in-bar-tips</span><br><span class="line">+          i.fa-fw.fas.fa-tags</span><br><span class="line">+        include includes/tagBar.pug</span><br><span class="line">      .article-sort-title= _p(&#x27;page.tag&#x27;) + &#x27; - &#x27; + page.tag</span><br><span class="line">      +articleSort(page.posts)</span><br><span class="line">      include includes/pagination.pug</span><br></pre></td></tr></table></figure><h1 id="4-引入js和css文件"><a href="#4-引入js和css文件" class="headerlink" title="4. 引入js和css文件"></a>4. 引入js和css文件</h1><p>这一部分和Heo博主的教程<a href="https://blog.zhheo.com/p/bc61964d.html">Butterfly魔改：动态分类条，可以根据页面变化而改变的分类列表展示方式 | 张洪Heo (zhheo.com)</a>一致。</p><p>不过如果添加了标签条，js文件需要增加一个函数</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//标签条</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">tagsBarActive</span>(<span class="params"></span>)</span>&#123;</span><br><span class="line">  <span class="keyword">var</span> urlinfo = <span class="built_in">window</span>.location.pathname;</span><br><span class="line">  urlinfo = <span class="built_in">decodeURIComponent</span>(urlinfo)</span><br><span class="line">  <span class="comment">//console.log(urlinfo);</span></span><br><span class="line">  <span class="comment">//判断是否是首页</span></span><br><span class="line">  <span class="keyword">if</span> (urlinfo == <span class="string">&#x27;/&#x27;</span>)&#123;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">document</span>.querySelector(<span class="string">&#x27;#tags-bar&#x27;</span>))&#123;</span><br><span class="line">      <span class="built_in">document</span>.getElementById(<span class="string">&#x27;首页&#x27;</span>).classList.add(<span class="string">&quot;select&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// 验证是否是分类链接</span></span><br><span class="line">    <span class="keyword">var</span> pattern = <span class="regexp">/\/tags\/.*?\//</span>;</span><br><span class="line">    <span class="keyword">var</span> patbool = pattern.test(urlinfo);</span><br><span class="line">    <span class="comment">//console.log(patbool);</span></span><br><span class="line">    <span class="comment">// 获取当前的标签</span></span><br><span class="line">    <span class="keyword">if</span> (patbool) &#123;</span><br><span class="line">      <span class="keyword">var</span> valuegroup = urlinfo.split(<span class="string">&quot;/&quot;</span>);</span><br><span class="line">      <span class="comment">//console.log(valuegroup[2]);</span></span><br><span class="line">      <span class="comment">// 获取当前分类</span></span><br><span class="line">      <span class="keyword">var</span> nowTag = valuegroup[<span class="number">2</span>];</span><br><span class="line">      <span class="keyword">if</span> (<span class="built_in">document</span>.querySelector(<span class="string">&#x27;#category-bar&#x27;</span>))&#123;</span><br><span class="line">        <span class="built_in">document</span>.getElementById(nowTag).classList.add(<span class="string">&quot;select&quot;</span>);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br><span class="line">tagsBarActive()</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;1-前言&quot;&gt;&lt;a href=&quot;#1-前言&quot; class=&quot;headerlink&quot; title=&quot;1. 前言&quot;&gt;&lt;/a&gt;1. 前言&lt;/h1&gt;&lt;p&gt;本文是对Heo博主写的&lt;a href=&quot;https://blog.zhheo.com/p/bc61964d.html&quot;&gt;</summary>
      
    
    
    
    <category term="博客相关" scheme="https://blog.justlovesmile.top/categories/%E5%8D%9A%E5%AE%A2%E7%9B%B8%E5%85%B3/"/>
    
    
    <category term="前端" scheme="https://blog.justlovesmile.top/tags/%E5%89%8D%E7%AB%AF/"/>
    
    <category term="Hexo" scheme="https://blog.justlovesmile.top/tags/Hexo/"/>
    
    <category term="博客" scheme="https://blog.justlovesmile.top/tags/%E5%8D%9A%E5%AE%A2/"/>
    
  </entry>
  
  <entry>
    <title>Hexo博客 | 如何让你的博客拥有星空背景和流星特效</title>
    <link href="https://blog.justlovesmile.top/posts/6a260bf6.html"/>
    <id>https://blog.justlovesmile.top/posts/6a260bf6.html</id>
    <published>2021-08-12T09:41:33.000Z</published>
    <updated>2021-08-12T09:41:33.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近很多小伙伴留言想要<code>星空和流星特效</code>，于是写了这篇文章准备介绍如何部署。</p><blockquote><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/202108121834269.gif"></p></blockquote><h2 id="1-插入Canvas标签"><a href="#1-插入Canvas标签" class="headerlink" title="1. 插入Canvas标签"></a>1. 插入Canvas标签</h2><p>首先打开Butterfly主题的<code>_config.yml</code>文件或者使用HTML直接插入，找到配置文件对应的<code>inject</code>部分，插入<code>&lt;canvas id=&quot;universe&quot;&gt;&lt;/canvas&gt;</code></p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/202108121759902.png" alt="image-20210812175952760"></p><h2 id="2-创建JS文件"><a href="#2-创建JS文件" class="headerlink" title="2. 创建JS文件"></a>2. 创建JS文件</h2><p>在<code>butterfly/source/js/</code>创建一个<code>universe.js</code>文件，或者添加到自己的<code>js</code>文件中</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">dark</span>(<span class="params"></span>) </span>&#123;<span class="built_in">window</span>.requestAnimationFrame=<span class="built_in">window</span>.requestAnimationFrame||<span class="built_in">window</span>.mozRequestAnimationFrame||<span class="built_in">window</span>.webkitRequestAnimationFrame||<span class="built_in">window</span>.msRequestAnimationFrame;<span class="keyword">var</span> n,e,i,h,t=<span class="number">.05</span>,s=<span class="built_in">document</span>.getElementById(<span class="string">&quot;universe&quot;</span>),o=!<span class="number">0</span>,a=<span class="string">&quot;180,184,240&quot;</span>,r=<span class="string">&quot;226,225,142&quot;</span>,d=<span class="string">&quot;226,225,224&quot;</span>,c=[];<span class="function"><span class="keyword">function</span> <span class="title">f</span>(<span class="params"></span>)</span>&#123;n=<span class="built_in">window</span>.innerWidth,e=<span class="built_in">window</span>.innerHeight,i=<span class="number">.216</span>*n,s.setAttribute(<span class="string">&quot;width&quot;</span>,n),s.setAttribute(<span class="string">&quot;height&quot;</span>,e)&#125;<span class="function"><span class="keyword">function</span> <span class="title">u</span>(<span class="params"></span>)</span>&#123;h.clearRect(<span class="number">0</span>,<span class="number">0</span>,n,e);<span class="keyword">for</span>(<span class="keyword">var</span> t=c.length,i=<span class="number">0</span>;i&lt;t;i++)&#123;<span class="keyword">var</span> s=c[i];s.move(),s.fadeIn(),s.fadeOut(),s.draw()&#125;&#125;<span class="function"><span class="keyword">function</span> <span class="title">y</span>(<span class="params"></span>)</span>&#123;<span class="built_in">this</span>.reset=<span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;<span class="built_in">this</span>.giant=m(<span class="number">3</span>),<span class="built_in">this</span>.comet=!<span class="built_in">this</span>.giant&amp;&amp;!o&amp;&amp;m(<span class="number">10</span>),<span class="built_in">this</span>.x=l(<span class="number">0</span>,n-<span class="number">10</span>),<span class="built_in">this</span>.y=l(<span class="number">0</span>,e),<span class="built_in">this</span>.r=l(<span class="number">1.1</span>,<span class="number">2.6</span>),<span class="built_in">this</span>.dx=l(t,<span class="number">6</span>*t)+(<span class="built_in">this</span>.comet+<span class="number">1</span>-<span class="number">1</span>)*t*l(<span class="number">50</span>,<span class="number">120</span>)+<span class="number">2</span>*t,<span class="built_in">this</span>.dy=-l(t,<span class="number">6</span>*t)-(<span class="built_in">this</span>.comet+<span class="number">1</span>-<span class="number">1</span>)*t*l(<span class="number">50</span>,<span class="number">120</span>),<span class="built_in">this</span>.fadingOut=<span class="literal">null</span>,<span class="built_in">this</span>.fadingIn=!<span class="number">0</span>,<span class="built_in">this</span>.opacity=<span class="number">0</span>,<span class="built_in">this</span>.opacityTresh=l(<span class="number">.2</span>,<span class="number">1</span>-<span class="number">.4</span>*(<span class="built_in">this</span>.comet+<span class="number">1</span>-<span class="number">1</span>)),<span class="built_in">this</span>.do=l(<span class="number">5e-4</span>,<span class="number">.002</span>)+<span class="number">.001</span>*(<span class="built_in">this</span>.comet+<span class="number">1</span>-<span class="number">1</span>)&#125;,<span class="built_in">this</span>.fadeIn=<span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;<span class="built_in">this</span>.fadingIn&amp;&amp;(<span class="built_in">this</span>.fadingIn=!(<span class="built_in">this</span>.opacity&gt;<span class="built_in">this</span>.opacityTresh),<span class="built_in">this</span>.opacity+=<span class="built_in">this</span>.do)&#125;,<span class="built_in">this</span>.fadeOut=<span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;<span class="built_in">this</span>.fadingOut&amp;&amp;(<span class="built_in">this</span>.fadingOut=!(<span class="built_in">this</span>.opacity&lt;<span class="number">0</span>),<span class="built_in">this</span>.opacity-=<span class="built_in">this</span>.do/<span class="number">2</span>,(<span class="built_in">this</span>.x&gt;n||<span class="built_in">this</span>.y&lt;<span class="number">0</span>)&amp;&amp;(<span class="built_in">this</span>.fadingOut=!<span class="number">1</span>,<span class="built_in">this</span>.reset()))&#125;,<span class="built_in">this</span>.draw=<span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;<span class="keyword">if</span>(h.beginPath(),<span class="built_in">this</span>.giant)h.fillStyle=<span class="string">&quot;rgba(&quot;</span>+a+<span class="string">&quot;,&quot;</span>+<span class="built_in">this</span>.opacity+<span class="string">&quot;)&quot;</span>,h.arc(<span class="built_in">this</span>.x,<span class="built_in">this</span>.y,<span class="number">2</span>,<span class="number">0</span>,<span class="number">2</span>*<span class="built_in">Math</span>.PI,!<span class="number">1</span>);<span class="keyword">else</span> <span class="keyword">if</span>(<span class="built_in">this</span>.comet)&#123;h.fillStyle=<span class="string">&quot;rgba(&quot;</span>+d+<span class="string">&quot;,&quot;</span>+<span class="built_in">this</span>.opacity+<span class="string">&quot;)&quot;</span>,h.arc(<span class="built_in">this</span>.x,<span class="built_in">this</span>.y,<span class="number">1.5</span>,<span class="number">0</span>,<span class="number">2</span>*<span class="built_in">Math</span>.PI,!<span class="number">1</span>);<span class="keyword">for</span>(<span class="keyword">var</span> t=<span class="number">0</span>;t&lt;<span class="number">30</span>;t++)h.fillStyle=<span class="string">&quot;rgba(&quot;</span>+d+<span class="string">&quot;,&quot;</span>+(<span class="built_in">this</span>.opacity-<span class="built_in">this</span>.opacity/<span class="number">20</span>*t)+<span class="string">&quot;)&quot;</span>,h.rect(<span class="built_in">this</span>.x-<span class="built_in">this</span>.dx/<span class="number">4</span>*t,<span class="built_in">this</span>.y-<span class="built_in">this</span>.dy/<span class="number">4</span>*t-<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>),h.fill()&#125;<span class="keyword">else</span> h.fillStyle=<span class="string">&quot;rgba(&quot;</span>+r+<span class="string">&quot;,&quot;</span>+<span class="built_in">this</span>.opacity+<span class="string">&quot;)&quot;</span>,h.rect(<span class="built_in">this</span>.x,<span class="built_in">this</span>.y,<span class="built_in">this</span>.r,<span class="built_in">this</span>.r);h.closePath(),h.fill()&#125;,<span class="built_in">this</span>.move=<span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;<span class="built_in">this</span>.x+=<span class="built_in">this</span>.dx,<span class="built_in">this</span>.y+=<span class="built_in">this</span>.dy,!<span class="number">1</span>===<span class="built_in">this</span>.fadingOut&amp;&amp;<span class="built_in">this</span>.reset(),(<span class="built_in">this</span>.x&gt;n-n/<span class="number">4</span>||<span class="built_in">this</span>.y&lt;<span class="number">0</span>)&amp;&amp;(<span class="built_in">this</span>.fadingOut=!<span class="number">0</span>)&#125;,<span class="built_in">setTimeout</span>(<span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;o=!<span class="number">1</span>&#125;,<span class="number">50</span>)&#125;<span class="function"><span class="keyword">function</span> <span class="title">m</span>(<span class="params">t</span>)</span>&#123;<span class="keyword">return</span> <span class="built_in">Math</span>.floor(<span class="number">1e3</span>*<span class="built_in">Math</span>.random())+<span class="number">1</span>&lt;<span class="number">10</span>*t&#125;<span class="function"><span class="keyword">function</span> <span class="title">l</span>(<span class="params">t,i</span>)</span>&#123;<span class="keyword">return</span> <span class="built_in">Math</span>.random()*(i-t)+t&#125;f(),<span class="built_in">window</span>.addEventListener(<span class="string">&quot;resize&quot;</span>,f,!<span class="number">1</span>),<span class="function"><span class="keyword">function</span>(<span class="params"></span>)</span>&#123;h=s.getContext(<span class="string">&quot;2d&quot;</span>);<span class="keyword">for</span>(<span class="keyword">var</span> t=<span class="number">0</span>;t&lt;i;t++)c[t]=<span class="keyword">new</span> y,c[t].reset();u()&#125;(),<span class="function"><span class="keyword">function</span> <span class="title">t</span>(<span class="params"></span>)</span>&#123;<span class="built_in">document</span>.getElementsByTagName(<span class="string">&#x27;html&#x27;</span>)[<span class="number">0</span>].getAttribute(<span class="string">&#x27;data-theme&#x27;</span>)==<span class="string">&#x27;dark&#x27;</span>&amp;&amp;u(),<span class="built_in">window</span>.requestAnimationFrame(t)&#125;()&#125;;</span><br><span class="line">dark()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/202108121822274.png" alt="image-20210812182203080"></p><p>代码的这一部分要求<code>data-theme</code>也就是主题为<code>dark</code>暗色主题，因此仅在暗色主题生效，随后将<code>js</code>文件添加到配置文件的<code>inject</code>处或者其他需要的位置。</p><h2 id="3-CSS样式"><a href="#3-CSS样式" class="headerlink" title="3. CSS样式"></a>3. CSS样式</h2><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* 背景宇宙星光  */</span></span><br><span class="line"><span class="selector-id">#universe</span>&#123;</span><br><span class="line">  <span class="attribute">display</span>: block;</span><br><span class="line">  <span class="attribute">position</span>: fixed;</span><br><span class="line">  <span class="attribute">margin</span>: <span class="number">0</span>;</span><br><span class="line">  <span class="attribute">padding</span>: <span class="number">0</span>;</span><br><span class="line">  <span class="attribute">border</span>: <span class="number">0</span>;</span><br><span class="line">  <span class="attribute">outline</span>: <span class="number">0</span>;</span><br><span class="line">  <span class="attribute">left</span>: <span class="number">0</span>;</span><br><span class="line">  <span class="attribute">top</span>: <span class="number">0</span>;</span><br><span class="line">  <span class="attribute">width</span>: <span class="number">100%</span>;</span><br><span class="line">  <span class="attribute">height</span>: <span class="number">100%</span>;</span><br><span class="line">  <span class="attribute">pointer-events</span>: none;</span><br><span class="line">  <span class="attribute">z-index</span>: -<span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">最近很多小伙伴留言想要`星空和流星特效`，于是写了这篇文章准备介绍如何部署。本文主要以Butterfly主题为例进行介绍。</summary>
    
    
    
    <category term="博客相关" scheme="https://blog.justlovesmile.top/categories/%E5%8D%9A%E5%AE%A2%E7%9B%B8%E5%85%B3/"/>
    
    
    <category term="前端" scheme="https://blog.justlovesmile.top/tags/%E5%89%8D%E7%AB%AF/"/>
    
    <category term="Hexo" scheme="https://blog.justlovesmile.top/tags/Hexo/"/>
    
    <category term="博客" scheme="https://blog.justlovesmile.top/tags/%E5%8D%9A%E5%AE%A2/"/>
    
  </entry>
  
  <entry>
    <title>iOS快捷指令 | iPicGo，随时随地用手机上传图片到图床</title>
    <link href="https://blog.justlovesmile.top/posts/223c1a0c.html"/>
    <id>https://blog.justlovesmile.top/posts/223c1a0c.html</id>
    <published>2021-03-26T05:48:49.000Z</published>
    <updated>2021-03-26T05:48:49.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="iOS快捷指令版PicGo"><a href="#iOS快捷指令版PicGo" class="headerlink" title="iOS快捷指令版PicGo"></a>iOS快捷指令版PicGo</h1><blockquote><p>功能：<strong>上传手机图片到Github，并将其在jsdelivr的CDN图片链接复制到剪切板。</strong></p><p>当然我主要是为了方便发图片链接到<a href="/essay/">哔哔</a>😀。哔哔来自<a href="https://immmmm.com/bb-by-wechat-pro/">木木木木木</a>，iOS<a href="https://www.icloud.com/shortcuts/8e9e01fd2fc14124b1a7cf43a5ea64bd">哔哔发射</a>捷径来自<a href="https://blog.zhheo.com/p/27be0e44.html">Heo</a></p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210326105717.png" alt="image-20210326105717200"></p><h2 id="1-申请github的personal-access-token"><a href="#1-申请github的personal-access-token" class="headerlink" title="1. 申请github的personal access token"></a>1. 申请github的personal access token</h2><p>点击Settings-Developer settings-Personal access tokens-Generate new token-取个名字勾选repo-复制token（!!!）即可获得token</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210326132347.png" alt="image-20210326132347250"></p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210326132516.png" alt="image-20210326132516102"></p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210326132545.png" alt="image-20210326132545789"></p><h2 id="2-使用iOS-的iPicGo捷径"><a href="#2-使用iOS-的iPicGo捷径" class="headerlink" title="2. 使用iOS 的iPicGo捷径"></a>2. 使用iOS 的iPicGo捷径</h2><p>使用手机Safari浏览器打开<a href="https://www.icloud.com/shortcuts/7c950e63f0ff4533b125253705e18f7c">快捷指令链接</a>，修改第一块<code>词典</code>里面的参数。</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210326130845.png" alt="06B2762F01E8C962472455EBA7B39F19"></p><p>其中参数如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &#x27;name&#x27;: github的用户名,</span><br><span class="line">    &#x27;repo&#x27;: github图床的仓库名,</span><br><span class="line">    &#x27;path&#x27;: 你想要上传的子路径名，例如：image或者image/pic，首尾无斜杠,</span><br><span class="line">    &#x27;token&#x27;: 申请到的github personal access token</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中path部分默认是有子路径的，因为我有😀，所以没有增加判断…可以自行修改url链接，如下图所示URL部分：</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210326131817.png" alt="image-20210326131817492"></p><p>修改完成后即可实现手机图片上传到github的功能。</p><h2 id="3-使用iPicGo和哔哔发射"><a href="#3-使用iPicGo和哔哔发射" class="headerlink" title="3. 使用iPicGo和哔哔发射"></a>3. 使用iPicGo和哔哔发射</h2><p>所以现在，我就可以先使用iPicGo捷径上传到github图床，然后捷径会自动返回图片链接到剪切板，再打开哔哔发射，粘贴图片链接，就可以方便哔哔了。当然，木木大佬的哔哔点啥公众号也可以发图片😂。</p><p>Gif演示：</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210326134511.gif" alt="20201105174303"></p><style>    img{max-height: 450px;}</style>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;iOS快捷指令版PicGo&quot;&gt;&lt;a href=&quot;#iOS快捷指令版PicGo&quot; class=&quot;headerlink&quot; title=&quot;iOS快捷指令版PicGo&quot;&gt;&lt;/a&gt;iOS快捷指令版PicGo&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;功能：&lt;strong&gt;上传</summary>
      
    
    
    
    <category term="随笔记录" scheme="https://blog.justlovesmile.top/categories/%E9%9A%8F%E7%AC%94%E8%AE%B0%E5%BD%95/"/>
    
    
    <category term="iOS" scheme="https://blog.justlovesmile.top/tags/iOS/"/>
    
    <category term="捷径" scheme="https://blog.justlovesmile.top/tags/%E6%8D%B7%E5%BE%84/"/>
    
  </entry>
  
  <entry>
    <title>深度学习 | GAN，什么是生成对抗网络</title>
    <link href="https://blog.justlovesmile.top/posts/6a054795.html"/>
    <id>https://blog.justlovesmile.top/posts/6a054795.html</id>
    <published>2021-03-03T02:43:37.000Z</published>
    <updated>2021-03-03T02:43:37.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="GAN学习笔记"><a href="#GAN学习笔记" class="headerlink" title="GAN学习笔记"></a>GAN学习笔记</h1><h2 id="1-GAN原理"><a href="#1-GAN原理" class="headerlink" title="1. GAN原理"></a>1. GAN原理</h2><p>论文链接：<a href="https://arxiv.org/abs/1406.2661">Generative Adversarial Networks</a></p><blockquote><p>生成式对抗网络(GAN, Generative Adversarial Networks)是一种深度学习模型，是近年来复杂分布上无监督学习最具前景的方法之一。模型通过框架中（至少）两个模块：生成模型（Generative Model）和判别模型（Discriminative Model）的互相博弈学习产生相当好的输出。原始 GAN 理论中，并不要求 G 和 D 都是神经网络，只需要是能拟合相应生成和判别的函数即可。但实用中一般均使用深度神经网络作为 G 和 D 。一个优秀的GAN应用需要有良好的训练方法，否则可能由于神经网络模型的自由性而导致输出不理想。<br>Ian J. Goodfellow等人于2014年10月在<a href="https://arxiv.org/abs/1406.2661">Generative Adversarial Networks</a>中提出了一个通过对抗过程估计生成模型的新框架。框架中同时训练两个模型：捕获数据分布的生成模型G，和估计样本来自训练数据的概率的判别模型D。G的训练程序是将D错误的概率最大化。这个框架对应一个最大值集下限的双方对抗游戏。可以证明在任意函数G和D的空间中，存在唯一的解决方案，使得G重现训练数据分布，而D=0.5。在G和D由多层感知器定义的情况下，整个系统可以用反向传播进行训练。在训练或生成样本期间，不需要任何马尔科夫链或展开的近似推理网络。实验通过对生成的样品的定性和定量评估证明了本框架的潜力。<br>  —- 摘自<a href="https://baike.baidu.com/item/Gan/22181905?fr=aladdin">百度百科</a></p></blockquote><p>GAN是由两部分组成的，第一部分是生成，第二部分是对抗。简单来说，就是有一个生成网络G和一个判别网络D，通过训练让两个网络相互竞争，生成网络G接受一个随机噪声z来生成假的数据G(z)，对抗网络D通过判别器去判别真伪概率，最后希望生成器G生成的数据能够以假乱真。在最理想的状态下，D(G(z)) = 0.5。</p><p>以上原理的数学公式为：</p><p>$$ min_{G}max_{D}V(D,G) = \mathbb{E} _ {x \sim p_{data}(x)} [\log D(x)] + \mathbb{E} _ {z \sim p_{z}(z) [\log (1-D(G(z)))]} $$</p><p>式子中，x表示真实数据，z表示噪声，G(z)表示G网络根据z生成的数据，D(x)表示D网络判断真实数据是否为真的概率，因此D(x)接近1越好。而D(G(z))代表D网络判断G网络生成的虚假数据是真实的概率。<br>因此，对于D网络(辨别器)：</p><ul><li>如果x来自$P_{data}$，那么D(x)要越大越好，可以用$\log(D(x)) \uparrow$表示。</li><li>如果x来自于$P_{generator}$，那么D(G(z))越小越好，进而表示为$\log[1−D(G(z))] \uparrow$。</li><li>因此需要最大化$max_D$<br>对于G网络(生成器)：</li><li>$D(G(z))$越大越好，进而表示为log[1−D(G(z))]↓</li><li>因此需要最小化$min_{G}$。</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210223180633.png"></p><p>第一步我们训练D，D是希望V(D,G)越大越好，所以是加上梯度(ascending)。第二步训练G时，V(D,G)越小越好，所以是减去梯度(descending)。整个训练过程交替进行。</p><h2 id="2-GAN实例"><a href="#2-GAN实例" class="headerlink" title="2. GAN实例"></a>2. GAN实例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn,optim</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> tfs</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line">transforms = tfs.Compose([</span><br><span class="line">    tfs.Resize((<span class="number">32</span>,<span class="number">32</span>)),</span><br><span class="line">    tfs.ToTensor(),</span><br><span class="line">    <span class="comment">#tfs.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">flat_img = <span class="number">32</span>*<span class="number">32</span>*<span class="number">3</span></span><br><span class="line">noise_dim = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&#x27;1.jpg&#x27;</span>)</span><br><span class="line">real_img = transforms(img)</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">2</span>)</span><br><span class="line">fake_img = torch.rand(<span class="number">1</span>,noise_dim)</span><br><span class="line"></span><br><span class="line">plt.imshow(np.transpose(real_img.numpy(),(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)))</span><br><span class="line"><span class="comment">#print(real_img)</span></span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210224172221.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.linear = nn.Sequential(</span><br><span class="line">            nn.Linear(flat_img, <span class="number">1024</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, <span class="number">2048</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">2048</span>, <span class="number">1</span>),</span><br><span class="line">            nn.Sigmoid() <span class="comment">#sigmoid常用于二分类问题</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, img</span>):</span></span><br><span class="line">        img = img.view(<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">        out = self.linear(img)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.linear = nn.Sequential(</span><br><span class="line">            nn.Linear(noise_dim, <span class="number">1024</span>),</span><br><span class="line">            nn.LeakyReLU(),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, <span class="number">2048</span>),</span><br><span class="line">            nn.LeakyReLU(),</span><br><span class="line">            nn.Linear(<span class="number">2048</span>, flat_img)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, latent_space</span>):</span></span><br><span class="line">        latent_space = latent_space.view(<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">        out = self.linear(latent_space)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">device = <span class="string">&#x27;cuda:0&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line"></span><br><span class="line">discr = Discriminator().to(device)</span><br><span class="line">gen = Generator().to(device)</span><br><span class="line"></span><br><span class="line">opt_d = optim.SGD(discr.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">opt_g = optim.SGD(gen.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">criterion = nn.BCELoss()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">epochs = <span class="number">200</span></span><br><span class="line">discr_e = <span class="number">4</span></span><br><span class="line">gen_e = <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#whole model training starts here</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line"></span><br><span class="line">    <span class="comment">#discriminator training</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(discr_e):</span><br><span class="line">        out_d1 = discr(real_img.to(device))</span><br><span class="line">        <span class="comment">#loss for real image</span></span><br><span class="line">        loss_d1 = criterion(out_d1, torch.ones((<span class="number">1</span>, <span class="number">1</span>)).to(device))</span><br><span class="line"></span><br><span class="line">        out_d2 = gen(fake_img.to(device)).detach()</span><br><span class="line">        <span class="comment">#loss for fake image</span></span><br><span class="line">        loss_d2 = criterion(discr(out_d2.to(device)), torch.zeros((<span class="number">1</span>, <span class="number">1</span>)).to(device))</span><br><span class="line"></span><br><span class="line">        opt_d.zero_grad()</span><br><span class="line">        loss_d = loss_d1+loss_d2</span><br><span class="line">        loss_d.backward()</span><br><span class="line">        opt_d.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment">#generator training</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(gen_e):</span><br><span class="line">        out_g = gen(fake_img.to(device))</span><br><span class="line">        <span class="comment">#Binary cross entropy loss</span></span><br><span class="line">        loss_g = criterion(discr(out_g.to(device)), torch.ones(<span class="number">1</span>, <span class="number">1</span>).to(device))</span><br><span class="line">        <span class="comment">#Loss function in the GAN paper</span></span><br><span class="line">        <span class="comment">#[log(1 - D(G(z)))]</span></span><br><span class="line">        <span class="comment">#loss_g = torch.log(torch.ones(1, 1).to(device) - (discr(out_g.to(device))))</span></span><br><span class="line">        </span><br><span class="line">        opt_g.zero_grad()</span><br><span class="line">        loss_g.backward()</span><br><span class="line">        opt_g.step()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (epoch+<span class="number">1</span>)%<span class="number">10</span>==<span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Epoch[&#123;&#125;/&#123;&#125;],d_loss:&#123;:.6f&#125;,g_loss:&#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch+<span class="number">1</span>,epochs,loss_d.data.item(),loss_g.data.item()))</span><br><span class="line"></span><br><span class="line">out=gen(fake_img.to(device)).detach()</span><br><span class="line">out_score=discr(out_g.to(device))</span><br><span class="line">loss = criterion(out_score, torch.ones(<span class="number">1</span>, <span class="number">1</span>).to(device))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;score:&quot;</span>,out_score.item(),<span class="string">&quot;loss:&quot;</span>,loss.item())</span><br><span class="line"></span><br><span class="line">out=out.reshape((<span class="number">3</span>,<span class="number">32</span>,<span class="number">32</span>)).cpu()</span><br><span class="line"></span><br><span class="line"><span class="comment">#print(out)</span></span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;fake&#x27;</span>)</span><br><span class="line">plt.imshow(np.transpose(out.numpy(),(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)))</span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;real&#x27;</span>)</span><br><span class="line">plt.imshow(np.transpose(real_img.numpy(),(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)))</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210224183510.png"></p><h2 id="3-DCGAN原理"><a href="#3-DCGAN原理" class="headerlink" title="3. DCGAN原理"></a>3. DCGAN原理</h2><p><a href="https://arxiv.org/pdf/1511.06434.pdf">https://arxiv.org/pdf/1511.06434.pdf</a></p><p>DCGAN的原理和GAN是一样的。只不过DCGANs体系结构有所改变：</p><ul><li>使用指定步长的卷积层代替池化层</li><li>在生成器和鉴别器中使用batch norm。</li><li>移除全连接层，以实现更深层次的体系结构，减少参数。</li><li>在生成器中使用ReLU激活，但输出使用Tanh。</li><li>在鉴别器中使用LeakyReLU激活</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210224183753.png"></p><p>DCGAN中提到了网络的训练细节：</p><ul><li>使用Adam算法更新参数，betas=(0.5, 0.999)；</li><li>batch size选为128；</li><li>权重使用正太分布，均值为0，标准差为0.02；</li><li>学习率0.0002。</li></ul><h2 id="4-DCGAN实例"><a href="#4-DCGAN实例" class="headerlink" title="4. DCGAN实例"></a>4. DCGAN实例</h2><p>生成动漫头像，数据集来自<a href="https://www.kaggle.com/soumikrakshit/anime-faces">https://www.kaggle.com/soumikrakshit/anime-faces</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> imageio</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> torch,torchvision</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms, utils</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">avatar_img_path = <span class="string">&quot;E:/python/dataset/anime face/data&quot;</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">trans = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">noise_dim = <span class="number">100</span></span><br><span class="line">batch_size = <span class="number">16</span></span><br><span class="line">beta1=<span class="number">0.5</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">#自定义数据集</span></span><br><span class="line"><span class="string">file_train=[]</span></span><br><span class="line"><span class="string">for image_name in tqdm(os.listdir(avatar_img_path)):</span></span><br><span class="line"><span class="string">    file_train.append(os.path.join(avatar_img_path,image_name))</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">def default_loader(path):</span></span><br><span class="line"><span class="string">    img = imageio.imread(path)</span></span><br><span class="line"><span class="string">    img = img/255</span></span><br><span class="line"><span class="string">    img = trans(img)</span></span><br><span class="line"><span class="string">    return img</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">class trainset(Dataset):</span></span><br><span class="line"><span class="string">    def __init__(self, loader=default_loader):</span></span><br><span class="line"><span class="string">    #定义好 image 的路径</span></span><br><span class="line"><span class="string">        self.images = file_train</span></span><br><span class="line"><span class="string">        self.target = 0</span></span><br><span class="line"><span class="string">        self.loader = loader</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    def __getitem__(self, index):</span></span><br><span class="line"><span class="string">        fn = self.images[index]</span></span><br><span class="line"><span class="string">        img = self.loader(fn)</span></span><br><span class="line"><span class="string">        target = self.target</span></span><br><span class="line"><span class="string">        return img,target</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    def __len__(self):</span></span><br><span class="line"><span class="string">        return len(self.images)</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">img_dataset=torchvision.datasets.ImageFolder(<span class="string">&quot;E:/python/dataset/anime face&quot;</span>, transform=trans)</span><br><span class="line"><span class="comment">#img_dataset=trainset()</span></span><br><span class="line">img_dataloader=DataLoader(img_dataset,batch_size=batch_size,shuffle=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#print(img_dataset)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, z_dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Generator,self).__init__()</span><br><span class="line">        self.z_dim = z_dim</span><br><span class="line">        self.generator = nn.Sequential(</span><br><span class="line">            nn.ConvTranspose2d(self.z_dim,<span class="number">512</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="number">0</span>,bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(num_features=<span class="number">512</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.ConvTranspose2d(<span class="number">512</span>,<span class="number">256</span>,<span class="number">4</span>,<span class="number">2</span>,<span class="number">1</span>,bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(num_features=<span class="number">256</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.ConvTranspose2d(<span class="number">256</span>,<span class="number">128</span>,<span class="number">4</span>,<span class="number">2</span>,<span class="number">1</span>,bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(num_features=<span class="number">128</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.ConvTranspose2d(<span class="number">128</span>,<span class="number">64</span>,<span class="number">4</span>,<span class="number">2</span>,<span class="number">1</span>,bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(num_features=<span class="number">64</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.ConvTranspose2d(<span class="number">64</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">2</span>,<span class="number">1</span>,bias=<span class="literal">False</span>),</span><br><span class="line">            nn.Tanh()</span><br><span class="line">        )</span><br><span class="line">        self.weight_init()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">weight_init</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.generator.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.ConvTranspose2d):</span><br><span class="line">                nn.init.normal_(m.weight.data, <span class="number">0</span>, <span class="number">0.02</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.BatchNorm2d):</span><br><span class="line">                nn.init.normal_(m.weight.data, <span class="number">0</span>, <span class="number">0.02</span>)</span><br><span class="line">                nn.init.constant_(m.bias.data, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        out = self.generator(x)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Discriminator</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        initialize</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        :param image_size: tuple (3, h, w)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(Discriminator,self).__init__()</span><br><span class="line">        self.discriminator = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>,<span class="number">64</span>,<span class="number">4</span>,<span class="number">2</span>,<span class="number">1</span>,bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(num_features=<span class="number">64</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">64</span>,<span class="number">128</span>,<span class="number">4</span>,<span class="number">2</span>,<span class="number">1</span>,bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(num_features=<span class="number">128</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">128</span>,<span class="number">256</span>,<span class="number">4</span>,<span class="number">2</span>,<span class="number">1</span>,bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(num_features=<span class="number">256</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">256</span>,<span class="number">512</span>,<span class="number">4</span>,<span class="number">2</span>,<span class="number">1</span>,bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(num_features=<span class="number">512</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">512</span>,<span class="number">1</span>,<span class="number">4</span>,<span class="number">2</span>,<span class="number">0</span>,bias=<span class="literal">False</span>),</span><br><span class="line">            nn.Sigmoid()</span><br><span class="line">        )</span><br><span class="line">        self.weight_init()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">weight_init</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.discriminator.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.ConvTranspose2d):</span><br><span class="line">                nn.init.normal_(m.weight.data, <span class="number">0</span>, <span class="number">0.02</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.BatchNorm2d):</span><br><span class="line">                nn.init.normal_(m.weight.data, <span class="number">0</span>, <span class="number">0.02</span>)</span><br><span class="line">                nn.init.constant_(m.bias.data, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        out = self.discriminator(x)</span><br><span class="line">        out = out.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line">generator = Generator(noise_dim).to(device)</span><br><span class="line">discriminator = Discriminator().to(device)</span><br><span class="line"></span><br><span class="line">bce_loss = nn.BCELoss()</span><br><span class="line"><span class="comment">#optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(beta1, 0.999))</span></span><br><span class="line">optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=<span class="number">0.00005</span>, betas=(beta1, <span class="number">0.999</span>))</span><br><span class="line">optimizer_G = torch.optim.Adam(generator.parameters(), lr=<span class="number">0.0002</span>, betas=(beta1, <span class="number">0.999</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">epochs=<span class="number">20</span></span><br><span class="line"></span><br><span class="line">fixed_z=torch.randn(batch_size,noise_dim,<span class="number">1</span>,<span class="number">1</span>,device=device)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="keyword">for</span> step,(image,_) <span class="keyword">in</span> <span class="built_in">enumerate</span>(img_dataloader):</span><br><span class="line">        batch_size=image.size(<span class="number">0</span>)</span><br><span class="line">        <span class="comment">#=====训练辨别器====</span></span><br><span class="line">        optimizer_D.zero_grad()</span><br><span class="line">        <span class="comment"># 计算判别器对真实样本给出为真的概率</span></span><br><span class="line">        d_out_real = discriminator(image.<span class="built_in">type</span>(torch.FloatTensor).to(device))</span><br><span class="line">        real_loss = bce_loss(d_out_real, torch.ones(size=(batch_size, <span class="number">1</span>)).to(device))</span><br><span class="line">        real_scores = d_out_real</span><br><span class="line">        <span class="comment">#real_loss.backward()</span></span><br><span class="line">        <span class="comment"># 计算判别器对假样本给出为真的概率</span></span><br><span class="line">        noise = torch.randn(batch_size,noise_dim,<span class="number">1</span>,<span class="number">1</span>,device=device)</span><br><span class="line">        fake_img = generator(noise)</span><br><span class="line">        d_out_fake = discriminator(fake_img.detach())</span><br><span class="line">        fake_loss = bce_loss(d_out_fake, torch.zeros(size=(batch_size, <span class="number">1</span>)).to(device))</span><br><span class="line">        fake_scores = d_out_fake</span><br><span class="line">        <span class="comment">#fake_loss.backward()</span></span><br><span class="line">        <span class="comment"># 更新判别器参数</span></span><br><span class="line">        d_loss = (real_loss + fake_loss)/<span class="number">2</span></span><br><span class="line">        d_loss.backward()</span><br><span class="line">        optimizer_D.step()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#=====训练生成器====</span></span><br><span class="line">        optimizer_G.zero_grad()</span><br><span class="line">        <span class="comment"># 计算判别器对伪造样本的输出的为真样本的概率值</span></span><br><span class="line">        d_out_fake = discriminator(fake_img)</span><br><span class="line">        <span class="comment"># 计算生成器伪造样本不被认为是真的损失</span></span><br><span class="line">        g_loss = bce_loss(d_out_fake, torch.ones(size=(batch_size, <span class="number">1</span>)).to(device))</span><br><span class="line">        <span class="comment"># 更新生成器</span></span><br><span class="line">        g_loss.backward()</span><br><span class="line">        optimizer_G.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># #################################################</span></span><br><span class="line">        <span class="comment"># 4：打印损失，保存图片</span></span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">200</span> == <span class="number">0</span>:</span><br><span class="line">            generator.<span class="built_in">eval</span>()</span><br><span class="line">            fixed_image = generator(fixed_z)</span><br><span class="line">            generator.train()</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;[epoch: &#123;&#125;/&#123;&#125;], [iter: &#123;&#125;], [G loss: &#123;:.3f&#125;], [D loss: &#123;:.3f&#125;], [R Score: &#123;:.3f&#125;], [F Score: &#123;:.3f&#125;]&quot;</span>.<span class="built_in">format</span>(epoch+<span class="number">1</span>,epochs,step, g_loss.item(), d_loss.item(),real_scores.data.mean(), fake_scores.data.mean()))</span><br><span class="line">            utils.save_image(fixed_image.detach(), <span class="built_in">str</span>(epoch+<span class="number">1</span>)+<span class="string">&quot;fake.jpg&quot;</span>,normalize=<span class="literal">True</span>)</span><br><span class="line">            utils.save_image(image,<span class="built_in">str</span>(epoch+<span class="number">1</span>)+<span class="string">&quot;real.jpg&quot;</span>,normalize=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>结果如下：<br>    [epoch: 1/20], [iter: 0], [G loss: 0.699], [D loss: 0.694], [R Score: 0.499], [F Score: 0.500]<br>    [epoch: 1/20], [iter: 200], [G loss: 0.803], [D loss: 0.715], [R Score: 0.512], [F Score: 0.529]<br>    [epoch: 1/20], [iter: 400], [G loss: 0.734], [D loss: 0.692], [R Score: 0.492], [F Score: 0.491]<br>    [epoch: 1/20], [iter: 600], [G loss: 0.730], [D loss: 0.693], [R Score: 0.496], [F Score: 0.496]<br>    [epoch: 1/20], [iter: 800], [G loss: 0.748], [D loss: 0.686], [R Score: 0.500], [F Score: 0.492]<br>    [epoch: 1/20], [iter: 1000], [G loss: 0.745], [D loss: 0.680], [R Score: 0.514], [F Score: 0.499]<br>    [epoch: 1/20], [iter: 1200], [G loss: 0.715], [D loss: 0.701], [R Score: 0.527], [F Score: 0.532]<br>    [epoch: 2/20], [iter: 0], [G loss: 0.762], [D loss: 0.679], [R Score: 0.524], [F Score: 0.508]<br>    [epoch: 2/20], [iter: 200], [G loss: 0.815], [D loss: 0.686], [R Score: 0.507], [F Score: 0.498]<br>    [epoch: 2/20], [iter: 400], [G loss: 0.836], [D loss: 0.665], [R Score: 0.509], [F Score: 0.479]<br>    [epoch: 2/20], [iter: 600], [G loss: 0.759], [D loss: 0.694], [R Score: 0.523], [F Score: 0.520]<br>    [epoch: 2/20], [iter: 800], [G loss: 0.973], [D loss: 0.646], [R Score: 0.551], [F Score: 0.499]<br>    [epoch: 2/20], [iter: 1000], [G loss: 0.926], [D loss: 0.671], [R Score: 0.531], [F Score: 0.495]<br>    [epoch: 2/20], [iter: 1200], [G loss: 1.100], [D loss: 0.582], [R Score: 0.497], [F Score: 0.362]</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210225180739.png"></p><p>第7个epoch：<br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210226103604.jpg"></p><p>batch_size以及其他参数可自行调整。</p><h2 id="5-WGAN原理"><a href="#5-WGAN原理" class="headerlink" title="5. WGAN原理"></a>5. WGAN原理</h2><p>论文：<a href="https://arxiv.org/pdf/1701.07875.pdf">Wasserstein GAN</a><br><a href="https://arxiv.org/abs/1701.04862">Towards Principled Methods for Training Generative Adversarial Networks</a></p><p>总所周知，GAN的训练存在很多问题和挑战：</p><ul><li>训练困难，需要精心设计模型结构，协调G和D的训练程度</li><li>G和D的损失函数无法指示训练过程，缺乏一个有意义的指标和生成图片的质量相关联</li><li>模式崩坏（mode collapse），生成的图片虽然看起来像是真的，但是缺乏多样性</li></ul><p>WGAN相比较于传统的GAN，做了如下修改：</p><ul><li>D最后一层去掉sigmoid</li><li>G和D的loss不取log</li><li>每次更新D的参数后，将其绝对值截断到不超过一个固定常数c</li><li>不要用基于动量的优化算法（包括momentum和Adam），推荐RMSProp，SGD也行</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210303105435.png"></p><p>G的损失函数原本为$\mathbb{E} _ {z \sim p _ z}[\log(1-D(G(z)))]$ ，其导致的结果是，如果D训练得太好，G将学习不到有效的梯度。但是，如果D训练得不够好，G也学习不到有效的梯度。<br>因此以上损失函数导致GAN训练特别不稳定，需要小心协调G和D的训练程度。</p><blockquote><p>WGAN参考资料：<br><a href="https://zhuanlan.zhihu.com/p/44169714">https://zhuanlan.zhihu.com/p/44169714</a><br><a href="https://www.cnblogs.com/Allen-rg/p/10305125.html">https://www.cnblogs.com/Allen-rg/p/10305125.html</a></p></blockquote>]]></content>
    
    
    <summary type="html">2014年，arXiv上面刊载了一篇关于生成对抗网络的文章，名为《Generative Adversarial Nets》，作者是深度学习领域的大牛Ian J. Goodfellow。本文主要记录博主对于GAN及其基础变种的学习笔记，主要包括GAN，DCGAN的原理和实例，以及WGAN的基础原理。</summary>
    
    
    
    <category term="人工智能" scheme="https://blog.justlovesmile.top/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
    <category term="深度学习" scheme="https://blog.justlovesmile.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="交叉熵" scheme="https://blog.justlovesmile.top/tags/%E4%BA%A4%E5%8F%89%E7%86%B5/"/>
    
  </entry>
  
  <entry>
    <title>深度学习 | Wasserstein距离</title>
    <link href="https://blog.justlovesmile.top/posts/ebe3a70b.html"/>
    <id>https://blog.justlovesmile.top/posts/ebe3a70b.html</id>
    <published>2021-01-31T07:14:05.000Z</published>
    <updated>2021-01-31T07:14:05.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Wasserstein-距离"><a href="#Wasserstein-距离" class="headerlink" title="Wasserstein 距离"></a>Wasserstein 距离</h1><p>对于绝大多数的机器学习问题，尤其是预测问题和隐变量模型（<code>latent factor model</code>）中，学习到数据集背后所服从的分布往往是模型所要解决的最终问题。在变分推断（<code>variational inference</code>）等领域中，往往会先从一个简单的分布引入，比如高斯分布或者多项式分布等；希望由这个简单的分布模型能不断学习进而逼近最终想要的、符合数据背后规律的分布，注意这时候的分布往往可能在形状上与初始假设的分布有所差异。</p><h2 id="KL散度和JS散度"><a href="#KL散度和JS散度" class="headerlink" title="KL散度和JS散度"></a>KL散度和JS散度</h2><p>在学习Wasserstein距离，首先回顾在机器学习算法中，衡量两个分布相似程度的指标常常是KL散度（<code>Kullback-Leibler Divergence</code>）以及JS散度 （<code>Jensen-Shannon Divergence</code>）。</p><h3 id="KL散度"><a href="#KL散度" class="headerlink" title="KL散度"></a>KL散度</h3><p>KL散度描述的是，评价训练所得的概率分布p与目标分布q之间的距离，可以表示为：</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210131104040.png" alt="image-20210131104033253"></p><p>机器学习的算法最终的目的是缩小 $D _ {KL}$ 的值，可以看到当 $p(x)==q(x)$ 的时候，KL散度处处为0，达到最优结果。 但同时必须注意的是，由于KL散度中，对数项中$p(x)$与$q(x)$相对位置的关系，决定了KL散度其实是非对称的，即 $D_{KL}(p||q) \neq D_{KL}(q||p)$ .从物理学参考系的角度可以直观感受出，如果要想评价两个物体（分布）的相似程度，相似程度的值（比如KL散度）应该不能因为选取的参考目标（目标分布）的不同而改变。</p><h3 id="JS散度"><a href="#JS散度" class="headerlink" title="JS散度"></a>JS散度</h3><p>既然KL散度不具备对称性，那么依然从参考系的角度出发，那我们直接把所有参考系下计算的距离平均即可（在本文环境下只有目标分布和预测分布两个参考系）。这样便是JS散度的思想，具体的定义为:</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210131104625.png" alt="image-20210131104625712"></p><p>因而JS散度便有了对称性，并且形式上更为平滑，更适合作为最后最大似然的函数，这点在生成对抗网络（GAN）的损失函数取得了不错的成绩。</p><h2 id="Wasserstein距离"><a href="#Wasserstein距离" class="headerlink" title="Wasserstein距离"></a>Wasserstein距离</h2><p>Wasserstein距离也叫做推土机距离（Earth Mover’s distance），这也是由于它的推导过程可以很形象的用挖土填土来解释，这也是因为该距离定义中由一个分布转变为另一个分布所需要的代价和挖土填土的过程十分相似。考虑两个离散的分布P和Q:</p><ul><li><p>P1 = 3, P2 = 2, P3 = 1, P4 = 4</p></li><li><p>Q1 = 1, Q2 = 2, Q3 = 4, Q4 = 3</p></li></ul><p>为了让两个分布相同，我们一个个变量地观察，</p><ul><li>为了让P1和Q1相同，我们需要P1把手头上的3分2到P2去，这样P1和Q1都等于1，此时P2=4，其他数保持不变，这个过程是不是非常像挖掉P1的土填到P2上</li><li>为了让P2和Q2相同，我们也要做类似的挖土填土工作，但注意，此时P2手头上由P1填的2，因此现在P2是4，但是Q2依然是2，因而P2也要挖2分土给P3，保持和Q2一样。</li><li>P3和Q3也是一样，但此时P3为3，Q3为4，因为我们只能先挖土再填土，因此要Q3挖1分土给Q4，这样P4和Q4也能够一样。</li></ul><p>每一步的代价计算公式为$\delta_{i+1} = \delta_{i} + P_{i} -Q_i$，第0步我们规定为0，故有</p><ul><li><p>$\delta_{0} = 0$</p></li><li><p>$\delta_{1} = 0+3-1 = 2$</p></li><li><p>$\delta_{2} = 2+2-2 = 2$</p></li><li><p>$\delta_{3} = 2+1-4 = -1$</p></li><li><p>$\delta_{4} = -1+4-3 = 0$</p><p>所以最终的总代价，也即Wasserstein距离则为$W=\sum_i |\delta_i|=5$</p></li></ul><p>该挖土填土的过程可以由下图表示:(图片来源：<a href="https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html#kullbackleibler-and-jensenshannon-divergence">From GAN to WGAN</a>)</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210131105502.jpeg" alt="img"></p><p>由离散情况理解了距离计算以后，针对一般的<strong>连续分布</strong>，Wasserstein距离则变成如下形式:</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210131105656.png" alt="image-20210131105656319"></p><p>其中<code>inf</code>指代最大下界，$S(p_r,p_g)$表示的是分布$p_r$和$p_g$中所有可能的联合分布，每一个联合分布$\gamma \in S(p_r,p_g)$都是之前提到的“土”，用于刻画连续空间中分布间转换的代价，更具体而言，$\gamma(x,y)$刻画从x点转移到y点从而让x，y服从相同分布所需要的“土”的百分比。因此$\gamma$的边缘分布可以表示为$\sum_x \gamma(x,y)=p_g(y),\sum_y \gamma(x,y)=p_r(x)$</p><p>当我们将x作为我们的起始点，y作为我们要逼近的终点时，挖土填土的总量即为$\gamma(x,y)$，也即上文离散情况下计算的代价$\delta$，而点与点之间的距离则为||x-y||，因而总代价为$\sum_{x,y} \gamma(x,y)||x-y||$,总代价最后可以使用EM等方法求得最小值。</p><h3 id="为什么Wasserstein距离优于KL和JS散度"><a href="#为什么Wasserstein距离优于KL和JS散度" class="headerlink" title="为什么Wasserstein距离优于KL和JS散度"></a>为什么Wasserstein距离优于KL和JS散度</h3><p>P,Q两个分布完全重合，此时这三种距离度量方式均为0。可以看出KL散度在两个分布完全没有任何交集的时候会得出无穷的结果，而JS散度则会有突然的阶跃，并且在0点出不可微，只有Wasserstein距离能够提供更为平滑的结果用于梯度下降法的参数更新。不过值得一提的是，目前主流的分布距离度量依然是KL散度，这是由于KL散度的计算方式简单，计算成本较Wasserstein低，但近年来Wasserstein距离的近似<code>Sinkhorn distance</code>以及其他加快距离计算方法的论文也在不断涌现.</p><blockquote><p>转自：<a href="https://zhuanlan.zhihu.com/p/84617531">https://zhuanlan.zhihu.com/p/84617531</a></p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Wasserstein-距离&quot;&gt;&lt;a href=&quot;#Wasserstein-距离&quot; class=&quot;headerlink&quot; title=&quot;Wasserstein 距离&quot;&gt;&lt;/a&gt;Wasserstein 距离&lt;/h1&gt;&lt;p&gt;对于绝大多数的机器学习问题，尤其是预测问题和</summary>
      
    
    
    
    <category term="人工智能" scheme="https://blog.justlovesmile.top/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
    <category term="深度学习" scheme="https://blog.justlovesmile.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Wasserstein" scheme="https://blog.justlovesmile.top/tags/Wasserstein/"/>
    
  </entry>
  
  <entry>
    <title>深度学习 | 论文笔记（Lifelong Zero-Shot Learning）</title>
    <link href="https://blog.justlovesmile.top/posts/f6289062.html"/>
    <id>https://blog.justlovesmile.top/posts/f6289062.html</id>
    <published>2021-01-08T09:40:56.000Z</published>
    <updated>2021-01-08T09:40:56.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Lifelong-Zero-Shot-Learning-论文翻译"><a href="#Lifelong-Zero-Shot-Learning-论文翻译" class="headerlink" title="Lifelong Zero-Shot Learning(论文翻译)"></a>Lifelong Zero-Shot Learning(论文翻译)</h1><p><strong>终身零样本学习</strong></p><p>作者：<strong>Kun Wei, Cheng Deng, Xu Yang</strong></p><p><a href="https://www.ijcai.org/Proceedings/2020/0077.pdf">https://www.ijcai.org/Proceedings/2020/0077.pdf</a></p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>零样本学习(Zero-Shot Learning, ZSL)解决了一些测试类别在训练集中从未出现的问题。现有的零样本学习方法是被设计用来从一个固定的训练集中学习的，不具备对多种训练集的知识进行捕获和积累的能力，因此不适合许多现实生活中的应用。在本文中，我们提出了一种新的零样本学习方法，称为终身零样本学习(Lifelong Zero-Shot Learning，LZSL)，其目的是在多种数据集的学习过程中积累知识，并对所有训练数据集的从未出现的类别进行识别。此外，我们提出了一种革新的方法用来实现终身零样本学习，有效地缓解了连续训练过程中的灾难性遗忘。针对包含不同语义嵌入的数据集，我们利用变分自动编码器实现统一的语义表示。然后，在微调整个模型时，我们利用选择性再训练策略来保留先前任务的训练权重，并避免负迁移。最后，利用知识蒸馏，将之前的训练阶段的知识转移到当前阶段。我们还设计了终身零样本学习评估协议和高要求的基准。在这些基准上的大量实验表明，当现有的零样本学习方法失败时，我们的方法有效地解决了零样本学习问题。</p><h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h2><p>在最近几年，零样本学习在计算机视觉和机器学习社区中获得了越来越多的关注。与在训练阶段要求所有类别都有足够的样本的传统的分类任务不同，零样本学习的目标是识别在训练阶段从未出现过的新的类别的样本。在流行的零样本学习方法中，学习模型只在单个数据集的可见类上进行训练，然后在同一数据集的不可见类上进行测试，该数据集的可见类和不可见类是不相交的。然而，在许多现实世界的应用中，识别系统需要具有从获得的训练数据中不断学习的能力，并以终身的方式改进系统。</p><p>为了满足这一要求，我们提出了一种更实用的零样本学习方法，称为终身零样本学习(Lifelong Zero-Shot Learning,LZSL)，它要求模型积累不同数据集的知识，并对所有面向数据集的未出现的类别进行识别。如图1所示，该模型在多个学习阶段进行训练，每个阶段都包含来自新数据集的图像和语义嵌入。这些数据集的语义嵌入是多样而复杂的，例如，这些数据集的属性列表是不同的。在完成所有训练阶段后，模型将对所有数据集上的可见的和不可见的测试图像进行评估。</p><p>主流的零样本学习方法旨在学习图像之间的映射和相应的语义嵌入。这些方法根据分类空间可分为三种类型，即视觉空间、语义空间和常见嵌入空间。除此之外，还有一些零样本学习方法通过训练生成模型来获取不可见的类别的特征。然后，利用可见类别的视觉特征和生成的不可见类别的视觉特征训练分类器。这些方法将零样本学习任务转换为监督学习任务。然而，这些方法不能有效地处理终身零样本学习问题，因为它们缺乏在没有排查的情况下从之前训练的任务中积累知识的机制。</p><p>为了解决上述问题，实现终身零样本学习，我们提出了一种将统一语义嵌入、选择性再训练和知识蒸馏策略无缝集成的新方法。选择交叉和分布对齐变分自编码器(Cross and Distribution Aligned VAE, CACD-VAE)作为基础模型，训练VAEs [Kingma and Welling, 2013]分别对视觉嵌入和语义嵌入的特征进行编码和解码，并使用学习到的潜在特征训练一个零样本学习分类器。为了使CACD-VAE具备终身学习的能力，我们首先利用训练后的VAEs在每个训练阶段获得统一的语义嵌入。利用统一的语义嵌入，分别学习和固定不同任务的潜在空间。为了保证视觉特征能够准确地投射到固定的潜在空间中，利用选择性再训练策略提高了不同任务的分类空间之间的相似性，也避免了在获取新任务知识过程中的负迁移。此外，知识蒸馏被用来将知识从之前的任务转移到当前任务。大量的实验表明，当其他最先进的零样本学习方法无效时，我们的方法可以有效地从之前学习的任务中积累知识并缓解灾难性遗忘。我们的方法的贡献总结如下:</p><ul><li><p>据我们所知，我们是第一个提出并解决终身零样本学习问题的。我们以一种新颖的方式设计了终身零样本学习的基准和评估协议。</p></li><li><p>针对不同数据集的异构语义嵌入的挑战，我们采用了可以固定相应任务的潜在空间的VAEs算法去获得统一的语义嵌入。</p></li><li><p>利用选择性再训练提高不同数据集的分类空间之间的相似性，并通过知识蒸馏损失来监督，规范了知识从之前的任务向当前任务转移的过程。</p></li><li><p>在提出的基准上的大量的实验结果证明了我们提出的方法的有效性，它显著优于最先进的零样本学习方法。</p></li></ul><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20201230132259.png" alt="1"></p><p>图1:终身零样本学习的概述。当新任务到来时，模型按顺序学习新任务，从所有面临的任务中积累知识。将先前任务中的知识转移到当前任务中，可以有效地对不同数据集的不可见的类别进行分类。</p><h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h2><h3 id="2-1-零样本学习"><a href="#2-1-零样本学习" class="headerlink" title="2.1 零样本学习"></a>2.1 零样本学习</h3><p>零样本学习已经成为一个热门的研究课题，其目标是在没有任何标记的训练数据的情况下识别不可见的类别。此外，零样本学习是迁移学习的一个子问题，其重点是将知识从可见的类别转移到不可见的类别。在测试阶段，测试样本从视觉空间中获取，而我们只在语义空间中进行不可见的类别的语义嵌入。因此，零样本学习方法的主流方法是构建视觉空间与语义空间的连接。典型的方法是学习将视觉特征和语义特征映射到一个共同的嵌入空间的函数，在这个空间中视觉特征和语义特征的嵌入是匹配的。最近，生成对抗网络(GANs)被提出并成功引入到零样本学习问题中。生成零样本学习方法的任务是根据语义特征生成不可见的类别的视觉特征，将零样本学习转换为传统的监督分类任务。例如，f-CLSWGAN是利用conditional Wasserstein GANs提出的，它生成了差别性的不可见的视觉特征。基于f-CLSWGAN, Cycle-WGAN 重建正则化的目的是，保留转移过程中的类别的不同特征。</p><p>然而，上述所有方法都仅在单个数据集上进行训练，因为顺序学习各种数据集的能力有限。据我们所知，我们是第一个提出并解决终身零样本学习问题的。</p><h3 id="2-2-终身学习"><a href="#2-2-终身学习" class="headerlink" title="2.2 终身学习"></a>2.2 终身学习</h3><p>终身学习(Lifelong Learning)是一种学习模式，它要求模型拥有从一系列任务中进行学习，并能将从之前任务中获得的知识转移到后续任务中的能力。终身学习的关键挑战是灾难性遗忘，即当新任务到来时，被训练的模型会忘记之前任务中得到的知识。有很多终身学习的方法被提出，主要分为三部分，即，存储之前任务的训练样本，新任务到来时的正则化参数更新，以及使用额外的生成模型来重现之前任务的训练样本的记忆重现。</p><p>与传统的终身学习问题不同的是，在流行的终身学习分类问题中，传统的终身学习问题的训练和测试的类别是相同的，而在终身零样本学习中，这些是不相交的。</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20201230132412.png" alt="2"></p><p>图2:我们提出的在$t^{th}$训练阶段上方法框架，该框架由两个VAEs和一个在$(t−1)^{th}$训练阶段训练过的视觉模态编码器组成。给定一张图像,视觉特征提取器可以捕获它的视觉特征$x^t$,映射到的潜在空间作为$\mu^t_v$和$\sum^t_v$。同时，相应的语义嵌入$c^t$映射到潜在的空间作为$\mu^t_a$和$\sum ^ t _ a$。 为了实现潜在的分布对齐，在训练阶段将潜在分布之间的Wasserstein距离($L _ {DA}$)最小化。然后，利用交叉对齐损失( $L _ {CA}$ )，通过交叉模态重构，来保证潜在分布的对齐。此外，我们利用知识蒸馏( $L _ {KD}$ )将之前任务中获得的知识转移到当前任务中。</p><h2 id="3-方法"><a href="#3-方法" class="headerlink" title="3. 方法"></a>3. 方法</h2><p>针对终身零样本学习问题，我们提出终身零样本学习方法，将终身学习和零样本学习无缝结合。我们的方法框架如图2所示。首先，我们利用VAEs实现不同数据集的统一语义嵌入;然后，我们采用选择性再训练策略逼近不同数据集的分类空间，避免负迁移。最后，我们采用知识蒸馏的方法，将先前任务中的知识转移到当前任务中。</p><h3 id="3-1-问题公式"><a href="#3-1-问题公式" class="headerlink" title="3.1 问题公式"></a>3.1 问题公式</h3><p>在第$t^{th}$个训练阶段，给出一个数据集$S^t = {(x^t, y^t, c^t)|x^t \in X^t, y^t \in Y^t_s, c^t \in C^t }$， 其由一个预训练卷积神经网络(CNN)提取的图像特征$x^t$、可见的类别$Y^t_s$的标签$y^t$和对应类别的语义嵌入$c^t$组成。此外，还有一个可获得的数据集$U^t= {(u^t, c^t_u) | u^t \in Y^t_u, c^t_u \in C^t }$，该数据集包含集合$Y^t_u$中的不可见的类别的标签$u^t$和不可见的类别的语义嵌入$c^t_u$。对于最现实和最具挑战性的广义零学习(Generalized Zero-Learning, GZSL)，其目标是学习一个分类器$f^t_{GZSL}: X^t \rightarrow Y^t_s\cup Y^t_u$。然而，我们的方法主要集中在通过顺序训练不同的数据集来学习一个生成模型，然后针对不同的数据集构造几个分类器。</p><h3 id="3-2-背景-交叉分布对齐变分自编码器-CACD-VAE"><a href="#3-2-背景-交叉分布对齐变分自编码器-CACD-VAE" class="headerlink" title="3.2 背景: 交叉分布对齐变分自编码器(CACD-VAE)"></a>3.2 背景: 交叉分布对齐变分自编码器(CACD-VAE)</h3><p>本文首先介绍了一种最先进的零样本学习方法–交叉分布对齐变分自编码器 (CADA-VAE)，它是我们方法的基本模型。它的目标是搜索一个共同的分类空间，其中嵌入的语义特征和视觉特征是一致的。该模型包含两个VAEs，一个用于语义特征，另一个用于视觉特征，每个都包含一个编码器和一个解码器。每个VAE的目标函数是给定样本的边际似然的变分下界，它可以表述为:</p><p>$$ L = \mathbb{E} _ {q_{\phi} (z|x)}\left[\log p_{\phi} (x|z)\right] - \lambda D_{KL}(q_{\phi} (z|x)||p_{\theta} (z)),  (1) $$</p><p>其中，第一项为重构损失，第二项为解开的<code>Kullback-Leibler散度</code>，对推理模型$q(z|x)$和$p(z)$进行规则化。此外，$\lambda$被用来加权KL-散度。编码器预测$\mu$和$\sum$，所以有$q_{\phi}(z | x) = N(\mu, \sum)$，并且通过应用重新参数化技巧获取一个潜在的向量$z$。编码器被用于将特征投影到公共空间，并且解码器用于重建原始数据。<br>整个模型的VAE损失是两个VAE基本损失的总和:</p><p>$$ L_{VAE} = L_{VAE}^a + L_{VAE}^v,  (2) $$</p><p>其中$L_{VAE}^a$和$L_{VAE}^v$分别表示语义模态和视觉模态的VAE损失。此外，针对语义空间和视觉空间的嵌入在公共空间中的匹配问题，该模型对潜在分布进行了精确对齐，需要一个交叉重建准则来保证。因此，我们设计并应用了交叉对齐损失(CA)和分布对齐损失(DA)。</p><p>交叉对齐损失使来自另一个模态的重构特征与原始模态特征相似。交叉对齐损失为:</p><p>$$ L_{CA} = \left| c-D_a(E_v(x)) \right| + \left|x-D_v(E_a(x))\right|,  (3) $$</p><p>其中，$c$、$D_a$和$E_a$是语义模态的特征、解码器和编码器，$x$、$D_v$和$E_v$是视觉模态的特征、解码器和编码器。</p><p>利用分布对齐损失最小化语义模态的潜在高斯分布与视觉模态的之间的Wasserstein距离，使语义空间和视觉空间的隐性嵌入相匹配。<br>距离表示为:</p><p>$$ L_{DA} = (||\mu_a - \mu_v||_2^2 + ||\sum^{\frac{1}{2}}_a-\sum_a^{frac{1}{2}}||^2_Frobenius)^{frac{1}{2}} ,  (4) $$</p><p>其中$\mu_a$和$\sum_a$通过编码器$E_a$预测,而$µ_v$和$\sum_v$通过编码器$E_v$预测。<br>目标函数可以表示为:</p><p>$$ L_{CACD - VAE} = L_{VAE} + \gamma L_{CA} + \delta L_{DA}, (5)  $$</p><p>其中，$\gamma$ 和 $\delta$ 是交叉对齐和分布对齐损失的超参数，用于权衡这些损失。</p><h3 id="3-3-统一的语义嵌入"><a href="#3-3-统一的语义嵌入" class="headerlink" title="3.3 统一的语义嵌入"></a>3.3 统一的语义嵌入</h3><p>由于不同数据集的属性数量和种类不同，首先需要解决的挑战是不同数据集的语义嵌入是多种多样和复杂的。为了解决这一问题，我们尝试寻找不同数据集的统一语义嵌入。在训练$t^{th}$任务之后,语义嵌入$c^t$被预测为通过$E^t_a$映射的$\mu^t_a$和$\sum^t_a$。隐向量z是采用再参数化的技巧生成的,其过程是从点数据生成各种隐向量的过程。生成的隐向量可以作为最终分类器的训练数据，其中包含了对应类的判别信息。在此基础上,我们替换原始语义嵌入$c^t$和$\mu^t_a$,$\sum_a^t$,从一个点数据到两个点数据,数据可被视为更具代表性的语义映射。在训练完所有任务后，我们可以利用这些新的语义嵌入再现所有数据集的隐向量，并训练更强健的分类器。</p><h3 id="3-4-选择性再训练"><a href="#3-4-选择性再训练" class="headerlink" title="3.4 选择性再训练"></a>3.4 选择性再训练</h3><p>对于这项新任务，一种自然的方法是对整个模型进行微调。然而，对整个模型进行微调会改变先前任务的权重，导致神经网络的灾难性遗忘。因此，我们采用选择性再训练策略对整个模型进行微调。当获得统一的语义嵌入时，不同数据集的分类空间是固定的，这也是之前任务的潜在空间。因此,模型是从视觉空间到分类空间的投影,是视觉模态的编码器$E_v^t$。我们表示$W^t$作为$E^t_v$和$W^t_l$的参数，被表示为l层的参数，而l层的数量是L。当一个新的任务到达时，我们首先冻结参数$W^{t - 1}_l$，并对模型进行微调，以获得$L - 1$层之间输出单元$o_t$和隐藏单元的连接。然后，我们可以选择在训练过程中受影响的所有单位和权重，并保持与输出单位无关的部分不变。选择操作可以看作是对模型进行初始化，保证优化的方向是保护前一个任务的分类空间。最后，我们只对选定的权值进行微调，记为$W_S^t$。算法1描述了选择性再训练的过程。</p><table><thead><tr><th><strong>算法1</strong> 选择性再训练的过程</th></tr></thead><tbody><tr><td>输入：数据集$S^t$，之前的参数$W^{t-1}$</td></tr><tr><td>输出：选择参数$W_s^t$</td></tr><tr><td>1: 冻结参数$W^{t-1}_L$，$S^t={o_t}$</td></tr><tr><td>2: 微调网络</td></tr><tr><td>3: $\text{for l = L,…,l do}$</td></tr><tr><td>4: 添加神经元$i$到$S^t$，如果存在一些神经元$j \in S$，且$W_{l,ij}^{t-1}≠0$</td></tr><tr><td>5: $\text{end for}$</td></tr><tr><td>6: 微调选择的参数$W^t_S$</td></tr></tbody></table><h3 id="3-5-知识蒸馏"><a href="#3-5-知识蒸馏" class="headerlink" title="3.5 知识蒸馏"></a>3.5 知识蒸馏</h3><p>通过选择性再训练，选择性神经元发生变化并且其他神经元被冻结，但不能保证整个模型的优化方向，即激励模型保持之前任务的知识。为了将知识从之前的任务中转移到当前任务中，我们采用了知识蒸馏策略。当$t^{th}$任务到达时，我们希望在相同输入$x^t$的情况下，$E^t_v$的输出与$E^{t−1}_v$的输出相似，这样可以保证$t^{th}$任务和$(t-1)^{th}$任务的分类空间近似。在顺序训练所有数据集后,当$E_v^t$输入相同的图像特征$x^t$时，最后的$e_v$有能力预测相似的$\mu^t_v$和$\sum^t_v$。蒸馏损失记为:</p><p>$$ L_{KD} = ||\mu_v^t - \widehat{\mu _v^t}||_1 + ||\sum_v^t - \widehat{\sum_v^t}||_1 ,  (6) $$</p><p>其中$\mu_v^t$和$\sum_v^t$通过$E^t_v$预测，而$\widehat{\mu_v^t}$和$\widehat{\sum_v^t}$通过$E^{t-1}_v$。</p><p>当$t&gt;1$时，目标函数表示为：</p><p>$$L = L_{CACD-VAE} +\beta L_{KD},  (7) $$</p><p>其中$\beta$为加权知识蒸馏损失的超参数，设为1。</p><h3 id="3-6-训练和推理"><a href="#3-6-训练和推理" class="headerlink" title="3.6 训练和推理"></a>3.6 训练和推理</h3><p>在训练中，我们对数据集进行顺序训练，并保存所有类别的统一语义嵌入。<br>在VAEs的训练阶段结束后，我们利用保存的语义嵌入再现所有类的隐向量。隐向量的生成过程对每个可见类别重复$n_s$次，对每个不可见类别重复$n_u$次。$n_s$和$n_u$分别设置为200和400。这些隐向量包含了这些类别的判别信息。利用不同数据集的隐向量分别训练<code>softmax分类器</code>。</p><p>在测试阶段，通过视觉模态$E_v$编码器将被测试可见类和不可见类的视觉特征投影为隐向量。然后将测试特征输入到训练好的分类器，得到不同数据集的分类结果。</p><table><thead><tr><th>数据集</th><th>语义维度</th><th>图像</th><th>可见类</th><th>不可见类</th></tr></thead><tbody><tr><td>APY</td><td>64</td><td>15339</td><td>20</td><td>12</td></tr><tr><td>AWA1</td><td>85</td><td>30475</td><td>40</td><td>10</td></tr><tr><td>CUB</td><td>312</td><td>11788</td><td>150</td><td>50</td></tr><tr><td>SUN</td><td>102</td><td>14340</td><td>645</td><td>72</td></tr></tbody></table><p>表1：实验使用的数据集及其统计信息。</p><h2 id="4-实验"><a href="#4-实验" class="headerlink" title="4. 实验"></a>4. 实验</h2><p>在本节中，我们将详细介绍所涉及的数据集、评估指标和实现细节。然后，我们将呈现几个最先进的竞争对手以及我们的方法的实验结果。最后，消融研究将证明我们所提出方法的有效性。</p><h3 id="4-1-基准和评估标准"><a href="#4-1-基准和评估标准" class="headerlink" title="4.1 基准和评估标准"></a>4.1 基准和评估标准</h3><p>我们在四个数据集上评估我们的方法: <code>Attribute Pascal</code>和<code>Yahoo数据集(aPY)</code>，<code>Animals with Attributes 1 (AW A1)</code>，<code>Caltech-UCSD-Birds 200-2011数据集(CUB)</code>和<code>SUN Attribute数据集(SUN)</code>。数据集统计如表1所示。对于所有数据集，我们使用预先训练的101层的<code>ResNet</code>提取2048维视觉特征。训练数据集的顺序为<code>aPY</code>, <code>AWA1</code>, <code>CUB</code>和<code>SUN</code>，都是按字母顺序排列的。</p><p>遵循广义零样本学习方法，我们对终身零样本学习采用相同的评价指标:</p><ul><li><p>u：是对每类带有预测标签集的不可见类别的测试图像进行分类的平均准确率，用于衡量识别不可见类的能力。</p></li><li><p>s：是对每类带有预测标签集的可见类的测试图像进行分类的平均准确率，用于衡量识别增量可见类的能力。</p></li><li><p>H：u和s的调和均值，公式为：$H=\frac{2×u×s}{u+s}$。</p></li></ul><p>我们任务中最重要的指标是，H平衡u和s指标之间的性能。对所有数据集进行训练后，对三个度量的所有结果进行测量。</p><h3 id="4-2-实施细则"><a href="#4-2-实施细则" class="headerlink" title="4.2 实施细则"></a>4.2 实施细则</h3><p>所有的编码器和解码器都是多层感知机，有一个隐藏层。我们使用了1560个隐藏单元作为图像特征编码器，1660个作为解码器。编码器和解码器的属性分别有1450个和660个隐藏单元。$\delta$从第6个epoch到第22个epoch以每轮0.54的速率增加，而$\gamma$从第21个epoch到第75个epoch以每个epoch按0.044的速率增加。KL散度的权重$\lambda$以每个epoch按照0.0026的速率增加，直到第90个epoch。此外，我们使用L1距离作为重构误差，得到了比L2更好的结果。</p><p>对于每个数据集，epoch的数量设置为100，批处理大小(batch size)设置为50。VAEs学习率设置为0.00015，分类器学习率设置为0.001。另外，我们的方法是用<code>PyTorch</code>实现的，并通过<code>ADAM</code>优化器进行优化。</p><h3 id="4-3-与现存基准程序的比较"><a href="#4-3-与现存基准程序的比较" class="headerlink" title="4.3 与现存基准程序的比较"></a>4.3 与现存基准程序的比较</h3><p><strong>基线模型</strong>。由于之前没有关于终身零样本学习的研究，我们将结合了CACD-VAE与传统的终身学习方法的基线进行比较。<br>(a) 顺序微调(SFT): 当一个新任务按顺序到达时，模型被微调，该模型的参数从在前一个任务训练或微调的模型进行初始化。<br>(b) L2正则化(L2): 在每个任务t上，$W_t$初始化为$W_{t−1}$，在$W_t$和$W_{t−1}$之间持续进行L2正则化训练。<br>(C) L1正则化(L1): 在每个任务t上，$W_t$初始化为$W_{t−1}$，在$W_t$和$W_{t−1}$之间持续进行L1正则化训练。</p><p><strong>结果和分析</strong>。表2总结了在四个基准数据集上的所有的对比方法以及我们的方法在三个评价指标下的结果。对于GZSL指标上的零样本学习方法，H是评价零样本学习方法性能最重要的指标，它平衡了u和s指标的性能。</p><p>表2中的“Base”表示模型在没有任何终身策略的情况下按顺序训练，“Original”表示分别训练数据集的模型。显然，我们可以发现Base的结果获得了之前数据集的最差性能，当新任务到来时，这些数据集不具备积累之前数据集的知识的能力。此外，采用顺序微调策略的模型比不采用该策略的模型的结果更差，这表明了零样本学习中存在灾难性遗忘问题。</p><p>与其他基准相比，我们的方法在前三个数据集中获得了三个评价指标的最佳性能。在<code>aPY数据集</code>上，我们的模型的u达到了29.11%，s达到了43.29%和H达到了34.81%，其中u提升了2.69%，s提升了13.50%，并且H提升了6.80%。在<code>AWA1数据集</code>上，我们的模型的u达到了51.17%，s达到了63.66%和H达到了56.73%，其中u提升了1.53%,s提升了4.59%，并且H提升了3.14%。在<code>CUB数据集</code>上，我们的模型的u达到了38.82%,s达到了45.81%和H达到了42.03%，其中u提升了3.29%,s提升了11.07%，并且H提升了7.68%。尽管我们的方法没有在SUN数据集上获得最好的结果，但是与其他数据集的改进相比，结果只下降了很少，这是因为我们的方法更好地平衡了从之前任务积累知识的能力以及当前任务获取知识的能力。我们还计算了这些方法在四个数据集上的平均H值。Base、SFT、L1、L2以及我们方法的平均H值分别为10.2%、36.73%、38.03%、36.73%和42.48%，平均H值提高了4.45%。综上所述，我们的方法在之前任务和当前任务中获得了均衡的性能，并且明显优于基线程序。</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20201230134611.png" alt="3"></p><h3 id="4-4-消融研究"><a href="#4-4-消融研究" class="headerlink" title="4.4 消融研究"></a>4.4 消融研究</h3><p>我们进行了两组消融实验来研究我们方法的有效性。</p><p>表3显示了添加了不同模块的基本模型的结果。基本模型为使用了连续微调训练策略的CACD-VAE。在基础模型的基础上，加入知识蒸馏模块和选择性再训练模块，分别用“KD”和“SR”表示。如表3所示，知识蒸馏和选择性再训练都可以提高前三个数据集的性能。加入“KD”的改进表明知识蒸馏可以将前一个任务的知识转移到当前任务中，在一定程度上缓解了灾难性遗忘的不利影响。此外，添加“SR”的改进表明选择性再训练可以保留之前任务中受影响的权重，避免负迁移，因为没有被选择的神经元不会受到再训练过程的影响。当添加所有模块时，我们的方法表现得最好。</p><p>我们做了一个实验来讨论数字$n_s$和$n_u$重放的影响，其平均H结果如图3所示。<br>当$n_s$和$n_u$被设置为200和400时，可以获得最佳性能。显然，我们可以注意到一个现象：在平均H达到峰值性能之前，平均H随着$n_s$和$n_u$的增加而增加。</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20201230134705.png" alt="4"></p><p>图3:不同$n_s$和$n_u$超参数下的平均H结果。</p><h2 id="5-结论"><a href="#5-结论" class="headerlink" title="5. 结论"></a>5. 结论</h2><p>据我们所知，本文是第一个尝试介绍和解决终身零样本学习的。首先，我们采用VAEs方法获得统一的语义嵌入，从而弥补了不同数据集语义嵌入之间的差距。然后，利用选择性再训练策略，在很大程度上保留前一训练阶段构造的投影。最后，我们从之前的任务中提炼出知识，并转移到当前的训练阶段。实验结果表明，该方法在4个基准数据集上性能均明显优于以往的方法。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Lifelong-Zero-Shot-Learning-论文翻译&quot;&gt;&lt;a href=&quot;#Lifelong-Zero-Shot-Learning-论文翻译&quot; class=&quot;headerlink&quot; title=&quot;Lifelong Zero-Shot Learning(</summary>
      
    
    
    
    <category term="人工智能" scheme="https://blog.justlovesmile.top/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
    <category term="深度学习" scheme="https://blog.justlovesmile.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="论文" scheme="https://blog.justlovesmile.top/tags/%E8%AE%BA%E6%96%87/"/>
    
    <category term="小样本学习" scheme="https://blog.justlovesmile.top/tags/%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>深度学习 | 什么是知识图谱</title>
    <link href="https://blog.justlovesmile.top/posts/ba289ad3.html"/>
    <id>https://blog.justlovesmile.top/posts/ba289ad3.html</id>
    <published>2020-12-23T11:34:42.000Z</published>
    <updated>2020-12-23T11:34:42.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="什么是知识图谱"><a href="#什么是知识图谱" class="headerlink" title="什么是知识图谱"></a>什么是知识图谱</h1><h2 id="1-来源"><a href="#1-来源" class="headerlink" title="1. 来源"></a>1. 来源</h2><p>2012年5月17日，<strong>Google</strong>正式提出了<strong>知识图谱</strong>（Knowledge Graph）的概念，其初衷是为了优化搜索引擎返回的结果，增强用户搜索质量及体验。</p><p>实际上，知识图谱并不是一个全新的概念，早在 2006 年就有文献提出了语义网（Semantic Network）的概念，呼吁推广、完善使用本体模型来形式化表达数据中的隐含语义，RDF（resource description framework，资源描述框架）模式和 OWL（Web ontology language，万维网本体语言）就是基于上述目的产生的。用电子科技大学徐增林教授的论文原文来说：</p><p>知识图谱技术的出现正是基于以上相关研究，是对语义网标准与技术的一次扬弃与升华。</p><p>目前，随着智能信息服务应用的不断发展，知识图谱已广泛应用于智能搜索，智能问答，个性化推荐等领域。</p><h2 id="2-定义"><a href="#2-定义" class="headerlink" title="2. 定义"></a>2. 定义</h2><p><strong>知识图谱</strong>，本质上，是一种揭示<strong>实体</strong>之间<strong>关系</strong>的语义网络。</p><p>看一张简单的知识图谱：</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20201223193032.jpg" alt="img"></p><p>如图所示，你可以看到，如果两个节点之间存在关系，他们就会被一条无向边连接在一起，那么这个节点，我们就称为<strong>实体</strong>（Entity），它们之间的这条边，我们就称为<strong>关系</strong>（Relationship）。</p><p>如果你看过网络综艺《奇葩说》第五季第17期：你是否支持全人类一秒知识共享，你也许会被辩手陈铭的辩论印象深刻。他在节目中区分了信息和知识两个概念：</p><p>信息是指外部的客观事实。举例：这里有一瓶水，它现在是7°。</p><p>知识是对外部客观规律的归纳和总结。举例：水在零度的时候会结冰。</p><p>“客观规律的归纳和总结” 似乎有些难以实现。Quora 上有另一种经典的解读，区分 “信息” 和 “知识” 。</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20201223193045.jpg" alt="img"></p><p>这样我们就很容易理解，在信息的基础上，建立实体之间的联系，就能行成 “知识”，或者称为叫事实（Fact）更为合适。换句话说，知识图谱是由一条条知识组成，每条知识表示为一个SPO<strong>三元组</strong>(Subject-Predicate-Object)。</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20201223193127.png" alt="img"></p><p>知识图谱实际上就是如此工作的。曾经知识图谱非常流行<strong>自顶向下</strong>(top-down)的构建方式。自顶向下指的是先为知识图谱定义好本体与数据模式，再将实体加入到知识库。该构建方式需要利用一些现有的结构化知识库作为其基础知识库，例如 Freebase 项目就是采用这种方式，它的绝大部分数据是从维基百科中得到的。</p><p>然而目前，大多数知识图谱都采用<strong>自底向上</strong>(bottom-up)的构建方式。自底向上指的是从一些开放链接数据（也就是 “信息”）中提取出实体，选择其中置信度较高的加入到知识库，再构建实体与实体之间的联系。</p><h2 id="3-数据类型和存储方式"><a href="#3-数据类型和存储方式" class="headerlink" title="3. 数据类型和存储方式"></a>3. 数据类型和存储方式</h2><p>知识图谱的原始数据类型一般来说有三类（也是互联网上的三类原始数据）：</p><ul><li><p>结构化数据（Structed Data）：如关系数据库</p></li><li><p>半结构化数据（Semi-Structed Data）：如XML、JSON、百科</p></li><li><p>非结构化数据（UnStructed Data）：如图片、音频、视频、文本</p></li></ul><p>如何存储上面这三类数据类型呢？一般有两种选择，一个是通过RDF（资源描述框架）这样的规范存储格式来进行存储，还有一种方法，就是使用图数据库来进行存储，常用的有Neo4j等。</p><h2 id="4-体系架构"><a href="#4-体系架构" class="headerlink" title="4. 体系架构"></a>4. 体系架构</h2><p>知识图谱的架构主要包括自身的<strong>逻辑结构</strong>以及<strong>体系架构</strong>。</p><p>知识图谱在<strong>逻辑结构</strong>上可分为<strong>模式层</strong>与<strong>数据层</strong>两个层次，<strong>数据层</strong>主要是由一系列的事实组成，而知识将以事实为单位进行存储。如果用（实体1，关系，实体2）、（实体、属性，属性值）这样的三元组来表达事实，可选择图数据库作为存储介质，例如开源的 Neo4j、Twitter 的 FlockDB、JanusGraph 等。<strong>模式层</strong>构建在数据层之上，主要是通过本体库来规范数据层的一系列事实表达。本体是结构化知识库的概念模板，通过本体库而形成的知识库不仅层次结构较强，并且冗余程度较小。</p><p>知识图谱的<strong>体系架构</strong>是指其构建模式的结构，如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20201223193136.jpg" alt="img"></p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20201223193143.jpg" alt="img"></p><p>大规模知识库的构建与应用需要多种智能信息处理技术的支持。通过知识抽取技术，可以从一些公开的半结构化、非结构化的数据中提取出实体、关系、属性等知识要素。通过知识融合，可消除实体、关系、属性等指称项与事实对象之间的歧义，形成高质量的知识库。知识推理则是在已有的知识库基础上进一步挖掘隐含的知识，从而丰富、扩展知识库。分布式的知识表示形成的综合向量对知识库的构建、推理、融合以及应用均具有重要的意义。</p><h2 id="5-知识抽取"><a href="#5-知识抽取" class="headerlink" title="5. 知识抽取"></a>5. 知识抽取</h2><p>知识抽取主要是面向开放的链接数据，通过自动化的技术抽取出可用的知识单元，知识单元主要包括实体(概念的外延)、关系以及属性3个知识要素，并以此为基础，形成一系列高质量的事实表达，为上层模式层的构建奠定基础。知识抽取有三个主要工作：</p><ul><li><p><strong>实体抽取</strong>：在技术上，更多称为 NER（named entity recognition，命名实体识别），指的是从原始语料中自动识别出命名实体。由于实体是知识图谱中的最基本元素，其抽取的完整性、准确、召回率等将直接影响到知识库的质量。因此，实体抽取是知识抽取中最为基础与关键的一步；</p></li><li><p><strong>关系抽取</strong>：目标是解决实体间语义链接的问题，早期的关系抽取主要是通过人工构造语义规则以及模板的方法识别实体关系。随后，实体间的关系模型逐渐替代了人工预定义的语法与规则。</p></li><li><p><strong>属性抽取</strong>：属性抽取主要是针对实体而言的，通过属性可形成对实体的完整勾画。由于实体的属性可以看成是实体与属性值之间的一种名称性关系，因此可以将实体属性的抽取问题转换为关系抽取问题。</p></li></ul><h2 id="6-知识表示"><a href="#6-知识表示" class="headerlink" title="6. 知识表示"></a>6. 知识表示</h2><p>近年来，以深度学习为代表的表示学习技术取得了重要的进展，可以将实体的语义信息表示为稠密低维实值向量，进而在低维空间中高效计算实体、关系及其之间的复杂语义关联，对知识库的构建、推理、融合以及应用均具有重要的意义。</p><h2 id="7-知识融合"><a href="#7-知识融合" class="headerlink" title="7. 知识融合"></a>7. 知识融合</h2><p>由于知识图谱中的知识来源广泛，存在知识质量良莠不齐、来自不同数据源的知识重复、知识间的关联不够明确等问题，所以必须要进行知识的融合。知识融合是高层次的知识组织，使来自不同知识源的知识在同一框架规范下进行异构数据整合、消歧、加工、推理验证、更新等步骤，达到数据、信息、方法、经验以及人的思想的融合，形成高质量的知识库。知识融合包括两部分内容：<strong>实体链接</strong>，<strong>知识合并</strong>。</p><ul><li><p><strong>实体链接</strong>：是指对于从文本中抽取得到的实体对象，将其链接到知识库中对应的正确实体对象的操作。</p></li><li><p><strong>知识合并</strong>：常见的知识合并需求有两个，一个是合并外部知识库，另一个是合并关系数据库。</p></li></ul><h2 id="8-知识加工"><a href="#8-知识加工" class="headerlink" title="8. 知识加工"></a>8. 知识加工</h2><p>事实本身并不等于知识。要想最终获得结构化，网络化的知识体系，还需要经历知识加工的过程。<strong>知识加工</strong>主要包括三方面内容：<strong>本体构建</strong>、<strong>知识推理</strong>和<strong>质量评估</strong>。</p><h2 id="9-知识更新"><a href="#9-知识更新" class="headerlink" title="9. 知识更新"></a>9. 知识更新</h2><p>从逻辑上看，知识库的更新包括<strong>概念层的更新</strong>和<strong>数据层的更新</strong>。</p><ul><li><p><strong>概念层的更新</strong>是指新增数据后获得了新的概念，需要自动将新的概念添加到知识库的概念层中。</p></li><li><p><strong>数据层的更新</strong>主要是新增或更新实体、关系、属性值，对数据层进行更新需要考虑数据源的可靠性、数据的一致性（是否存在矛盾或冗杂等问题）等可靠数据源，并选择在各数据源中出现频率高的事实和属性加入知识库。</p></li></ul><p>知识图谱的内容更新有两种方式：</p><ul><li><p> <strong>全面更新</strong>：指以更新后的全部数据为输入，从零开始构建知识图谱。这种方法比较简单，但资源消耗大，而且需要耗费大量人力资源进行系统维护；</p></li><li><p> <strong>增量更新</strong>：以当前新增数据为输入，向现有知识图谱中添加新增知识。这种方式资源消耗小，但目前仍需要大量人工干预（定义规则等），因此实施起来十分困难。</p></li></ul><h2 id="10-知识图谱应用"><a href="#10-知识图谱应用" class="headerlink" title="10.   知识图谱应用"></a>10.   知识图谱应用</h2><p>知识图谱为互联网上海量、异构、动态的大数据表达、组织、管理以及利用提供了一种更为有效的方式，使得网络的智能化水平更高，更加接近于人类的认知思维。</p><p><strong>智能搜索，智能问答，社交网络，个性化推荐，情报分析，反欺诈</strong>等等</p><h2 id="11-总结"><a href="#11-总结" class="headerlink" title="11.   总结"></a>11.   总结</h2><p>从技术来说，知识图谱的难点在于 NLP，因为我们需要机器能够理解海量的文字信息。但在工程上，我们面临更多的问题，来源于知识的获取，知识的融合。搜索领域能做的越来越好，是因为有成千上万（成百万上亿）的用户，用户在查询的过程中，实际也在优化搜索结果，这也是为什么百度的英文搜索不可能超过 Google，因为没有那么多英文用户。知识图谱也是同样的道理，如果将用户的行为应用在知识图谱的更新上，才能走的更远。</p><p>知识图谱肯定不是人工智能的最终答案，但知识图谱这种综合各项计算机技术的应用方向，一定是人工智能未来的形式之一。</p><h2 id="12-参考资料"><a href="#12-参考资料" class="headerlink" title="12.   参考资料"></a>12.   参考资料</h2><p><a href="https://www.cnblogs.com/huangyc/p/10043749.html">https://www.cnblogs.com/huangyc/p/10043749.html</a></p><p><a href="https://zhuanlan.zhihu.com/p/71128505">https://zhuanlan.zhihu.com/p/71128505</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;什么是知识图谱&quot;&gt;&lt;a href=&quot;#什么是知识图谱&quot; class=&quot;headerlink&quot; title=&quot;什么是知识图谱&quot;&gt;&lt;/a&gt;什么是知识图谱&lt;/h1&gt;&lt;h2 id=&quot;1-来源&quot;&gt;&lt;a href=&quot;#1-来源&quot; class=&quot;headerlink&quot; titl</summary>
      
    
    
    
    <category term="人工智能" scheme="https://blog.justlovesmile.top/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
    <category term="深度学习" scheme="https://blog.justlovesmile.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="知识图谱" scheme="https://blog.justlovesmile.top/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    
    <category term="NLP" scheme="https://blog.justlovesmile.top/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>深度学习 | 交叉熵损失函数</title>
    <link href="https://blog.justlovesmile.top/posts/fdbf585c.html"/>
    <id>https://blog.justlovesmile.top/posts/fdbf585c.html</id>
    <published>2020-12-11T14:48:21.000Z</published>
    <updated>2020-12-11T14:48:21.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Cross-Entropy-Error-Function"><a href="#Cross-Entropy-Error-Function" class="headerlink" title="Cross Entropy Error Function"></a>Cross Entropy Error Function</h1><p>交叉熵损失函数</p><h2 id="一，信息量"><a href="#一，信息量" class="headerlink" title="一，信息量"></a>一，信息量</h2><p><strong>信息量：</strong></p><p>任何事件都会承载着一定的信息量，包括已经发生的事件和未发生的事件，只是它们承载的信息量会有所不同。如昨天下雨这个已知事件，因为已经发生，是既定事实，那么它的信息量就为0。如明天会下雨这个事件，因为未有发生，那么这个事件的信息量就大。</p><p>从上面例子可以看出信息量是一个与事件发生概率相关的概念，而且可以得出，事件发生的概率越小，其信息量越大。</p><p>假设$x$是一个离散型随机变量，其取值集合为$X$，概率分布函数为$p(x)$，则定义事件$x=x_0$的信息量为：$I(x_0)=-\log(p(x_0))$</p><h2 id="二，熵"><a href="#二，熵" class="headerlink" title="二，熵"></a>二，熵</h2><p><strong>熵是表示随机变量不确定的度量，是对所有可能发生的事件产生的信息量的期望。</strong>熵值越大，表明这个系统的不确定性就越大。公式如下：</p><p>$$H(X)=-\sum_{i=1}^n p(x_i)\log(p(x_i))$$</p><p>对于0-1分布问题，熵的计算方法可以简化为：</p><p>$$H(x)=-\sum_{i=1}^np(x_i)log(p(x_i))\ =-p(x)\log(p(x))-(1-p(x))\log(1-p(x))$$</p><h2 id="三，相对熵（KL散度）"><a href="#三，相对熵（KL散度）" class="headerlink" title="三，相对熵（KL散度）"></a>三，相对熵（KL散度）</h2><p>相对熵又称KL散度，用于衡量对于同一个随机变量x的两个分布p(x)和q(x)之间的差异。在机器学习中，p(x)常用于描述样本的真实分布，例如[1,0,0,0]表示样本属于第一类，而q(x)则常常用于表示预测的分布，例如[0.7,0.1,0.1,0.1]。显然使用q(x)来描述样本不如p(x)准确，q(x)需要不断地学习来拟合准确的分布p(x)。</p><p>KL散度的公式如下：</p><p>$$D_{KL}(p||q)=\sum_{i=1}^np(x_i)\log(\frac{p(x_i)}{q(x_i)})$$</p><p>KL散度的值越小，表示两个分布越接近。在机器学习中，p往往用来表示样本的真实分布，q用来表示模型所预测的分布，那么KL散度就可以计算两个分布的差异，也就是Loss损失值。</p><h2 id="四，交叉熵"><a href="#四，交叉熵" class="headerlink" title="四，交叉熵"></a>四，交叉熵</h2><p>将KL散度的公式进行变形，得到：</p><p>$$D_{KL}(p||q)=\sum_{i=1}^np(x_i)\log(\frac{p(x_i)}{q(x_i)})\ =\sum_{i=1}^np(x_i)\log(p(x_i))-\sum_{i=1}^np(x_i)\log(q(x_i))$$</p><p>根据熵的定义，前半部分是$p(x)$的熵$H(x)=-\sum_{i=1}^np(x_i)\log(p(x_i))$，而后半部分则是交叉熵，定义为：</p><p>$$H(p,q)=-\sum_{i=1}^np(x_i)\log(q(x_i))$$</p><p>因此$D_{KL}(p||q)=H(p,q)-H(p)$ ，在机器学习中，我们需要评估label和predicts之间的差距，使用KL散度刚刚好，即 $D_{KL}(p|| \widetilde {q})$ ，由于KL散度中的前一部分$−H(p)$不变，故在优化过程中，只需要关注交叉熵就可以了。</p><h2 id="五，交叉熵损失函数"><a href="#五，交叉熵损失函数" class="headerlink" title="五，交叉熵损失函数"></a>五，交叉熵损失函数</h2><p>在线性回归问题中，常常使用MSE(Mean Squared Error)作为loss函数，而在分类问题中常常使用交叉熵作为loss函数，特别是在神经网络作分类问题时，并且由于交叉熵涉及到计算每个类别的概率，所以交叉熵几乎每次都和sigmoid或者softmax函数一起出现。</p><p><strong>(1)二分类</strong></p><p>在二分的情况下，对于每个类别我们的预测的到的概率为p和1-p。此时表达式为：</p><p>$$L=\frac{1}{N}\sum_iL_i=\frac{1}{N}\sum_i(-[y_i\log(p_i)+(1-y_i)\log(1-p_i)])$$</p><p>其中：</p><ul><li>$y_i$表示样本i的label，正类为1，负类为0</li><li>$p_i$表示样本i预测为正的概率</li></ul><p><strong>(2)多分类</strong></p><p>多分类问题实际上就是对二分类问题的扩展：</p><p>$$L=\frac{1}{N}\sum_iL_i=\frac{1}{N}\sum_i(-\sum_{j=1}^My_{ij}\log(p_{ij}))$$</p><p>其中：</p><ul><li>M 表示类别的数量</li><li>$y_{ij}$表示该类别和样本i类别是否相同，相同为1，不同为0</li><li>$p_{ij}$表示对于观测样本i属于类别j的预测概率</li></ul><p>例如：</p><table><thead><tr><th>id</th><th>predict</th><th>label</th><th>isCorrect</th></tr></thead><tbody><tr><td>1</td><td>0.3 0.3 0.4</td><td>0 0 1</td><td>1</td></tr><tr><td>2</td><td>0.3 0.4 0.3</td><td>0 1 0</td><td>1</td></tr><tr><td>3</td><td>0.1 0.2 0.7</td><td>1 0 0</td><td>0</td></tr></tbody></table><p>那么求其Loss：<br>$$L_1=-(0\times \log 0.3+0\times \log 0.3+1\times \log 0.4)$$<br>$$L_2=-(0\times \log 0.3+1\times \log 0.4+0\times \log 0.3)$$<br>$$L_3=-(1\times \log 0.1+0\times \log 0.2+0\times \log 0.7)$$<br>对所有样本的Loss求平均<br>$$Loss=\frac{L_1+L_2+L_3}{3}$$</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/74075915">https://zhuanlan.zhihu.com/p/74075915</a></p><p><a href="https://zhuanlan.zhihu.com/p/61944055">https://zhuanlan.zhihu.com/p/61944055</a></p><p><a href="https://zhuanlan.zhihu.com/p/35709485">https://zhuanlan.zhihu.com/p/35709485</a></p><p><a href="https://blog.csdn.net/b1055077005/article/details/100152102">https://blog.csdn.net/b1055077005/article/details/100152102</a></p>]]></content>
    
    
    <summary type="html">交叉熵损失函数经常用于分类问题中，特别是在神经网络做分类问题时，也经常使用交叉熵作为损失函数，此外，由于交叉熵涉及到计算每个类别的概率，所以交叉熵几乎每次都和sigmoid(或softmax)函数一起出现。这篇文章主要介绍什么是交叉熵损失函数。</summary>
    
    
    
    <category term="人工智能" scheme="https://blog.justlovesmile.top/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
    <category term="深度学习" scheme="https://blog.justlovesmile.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="交叉熵" scheme="https://blog.justlovesmile.top/tags/%E4%BA%A4%E5%8F%89%E7%86%B5/"/>
    
  </entry>
  
  <entry>
    <title>深度学习 | 论文笔记（A Review on Generative Adversarial Networks）</title>
    <link href="https://blog.justlovesmile.top/posts/c63e7cba.html"/>
    <id>https://blog.justlovesmile.top/posts/c63e7cba.html</id>
    <published>2020-12-09T15:47:49.000Z</published>
    <updated>2020-12-09T15:47:49.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="A-Review-on-Generative-Adversarial-Networks-Algorithms-Theory-and-Applications"><a href="#A-Review-on-Generative-Adversarial-Networks-Algorithms-Theory-and-Applications" class="headerlink" title="A Review on Generative Adversarial Networks: Algorithms, Theory, and Applications"></a>A Review on Generative Adversarial Networks: Algorithms, Theory, and Applications</h2><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20201209235414.jpg" alt="0fe50f6cc4ef45dbaee2e011ea2940bb_th"></p><p><a href="https://arxiv.org/pdf/2001.06937.pdf">Arxiv Link : https://arxiv.org/pdf/2001.06937.pdf</a></p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>Generative Adversarial Networks (GANs)  have been widely studied since 2014. There are a large number of different GANs variants.</p><blockquote><p>生成对抗网络，始于2014年，现在已有很多变种</p></blockquote><h3 id="Index-Terms"><a href="#Index-Terms" class="headerlink" title="Index Terms"></a>Index Terms</h3><p>Deep Learning ; GANs ; Algorithm ; Theory ; Applications</p><h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h3><p>GANs consists of two models: a generator and a discriminator. These two models are typically implemented by neural networks, but they can be implemented with any form of differentiable system that maps data from one space to the other.</p><blockquote><p>GANs包括两个模型：一个生成器和一个辨别器。一般是由神经网络实现，但是也可以由不同类型的能映射数据到另一个空间的可微系统实现。</p></blockquote><p>The generator tries to capture the distribution of true examples for new data example  generation. </p><blockquote><p>生成器试图捕获真实示例的分布，以生成新的数据示例。</p></blockquote><p>The discriminator is usually a binary classifier, discriminating generated examples from the true examples as accurately as possible.</p><blockquote><p>鉴别器通常是一个二进制分类器，尽可能准确地将生成的示例与真实的示例区分开来。</p></blockquote><p>The optimization of GANs is a minimax optimization problem. The goal is to reach Nash equilibrium.</p><blockquote><p>Nash equilibrium即纳什均衡，对于GANs，其损失是：</p><p>$$\min_G \max_D V(D,G)=\mathbb{E} _ {x \sim p_{data}(x)}[\log D(x)]+\mathbb{E} _ {z \sim p_z(z)}[\log (1-D(G(z)))] $$</p><p>生成器G和判别器D两者相互对抗，共同学习，不断优化</p></blockquote><h3 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h3><p>GANs belong to generative algorithms</p><blockquote><p>GANs 属于生成算法</p></blockquote><h4 id="2-1-Generative-algorithms"><a href="#2-1-Generative-algorithms" class="headerlink" title="2.1 Generative algorithms"></a>2.1 Generative algorithms</h4><p>Generative algorithms can be classified into two classes: explicit density model and implicit density model.</p><blockquote><p>生成算法可分为两类：显式密度模型和隐式密度模型。</p></blockquote><h5 id="2-1-1-Explicit-density-model"><a href="#2-1-1-Explicit-density-model" class="headerlink" title="2.1.1 Explicit density model"></a>2.1.1 Explicit density model</h5><p>An explicit density model assumes the distribution and utilizes true data to train the model containing the distribution or fit the distribution parameters. When finished, new examples are produced utilizing the learned model or distribution.</p><blockquote><p>显式密度模型假设分布，并利用真实数据训练包含分布的模型或拟合分布参数。完成后，利用学习的模型或分布产生新的示例。</p></blockquote><p>The explicit density models include maximum likelihood estimation (MLE), approximate inference, and Markov chain method.</p><blockquote><p>显式密度模型包括最大似然估计（MLE），近似推断和马尔可夫链方法。</p></blockquote><h5 id="2-1-2-Implicit-density-model"><a href="#2-1-2-Implicit-density-model" class="headerlink" title="2.1.2 Implicit density model"></a>2.1.2 Implicit density model</h5><p>It produces data instances from the distribution without an explicit hypothesis and utilizes the produced examples to modify the model.</p><blockquote><p>它在没有明确假设的情况下从分布中生成数据实例，并利用生成的实例来修改模型。</p></blockquote><p>GANs belong to the directed implicit density model category.</p><blockquote><p>GANs属于有向隐式密度模型类别。</p></blockquote><h5 id="2-1-3-The-comparison-between-GANs-and-other-generative-algorithms"><a href="#2-1-3-The-comparison-between-GANs-and-other-generative-algorithms" class="headerlink" title="2.1.3 The comparison between GANs and other generative algorithms"></a>2.1.3 The comparison between GANs and other generative algorithms</h5><p>The basic idea behind adversarial learning is that the generator tries to create as realistic examples as possible to deceive the discriminator. The discriminator tries to distinguish fake examples from true examples. Both the generator and discriminator improve through adversarial learning. </p><blockquote><p>对抗式学习背后的基本思想是，生成器试图创建尽可能真实的示例来欺骗鉴别器。鉴别器试图区分假例子和真例子。生成器和鉴别器都通过对抗性学习进行改进。</p></blockquote><h4 id="2-2-Adversarial-idea"><a href="#2-2-Adversarial-idea" class="headerlink" title="2.2 Adversarial idea"></a>2.2 Adversarial idea</h4><p>Adversarial machine learning is a minimax problem. The defender, who builds the classifier that we want to work correctly, is searching over the parameter space to find the parameters that reduce the cost of the classifier as much as possible. Simultaneously, the attacker is searching over the inputs of the model to maximize the cost.</p><blockquote><p>对抗性机器学习是一个极小极大问题。防守者(defender)构建了我们想要正确工作的分类器，他在参数空间中搜索尽可能降低分类器成本（cost）的参数（parameter）。同时，攻击者(attacker)搜索模型的输入以使成本（cost）最大化。</p></blockquote><h3 id="3-Algorithms"><a href="#3-Algorithms" class="headerlink" title="3. Algorithms"></a>3. Algorithms</h3><h4 id="3-1-Generative-Adversarial-Nets-（GANs）"><a href="#3-1-Generative-Adversarial-Nets-（GANs）" class="headerlink" title="3.1 Generative Adversarial Nets （GANs）"></a>3.1 Generative Adversarial Nets （GANs）</h4><p>In order to learn the generator’s distribution $p_g$ over data $x$, a prior on input noise variables is defined as $p_z(z)$ and $z$ is the noise variable. </p><blockquote><p>为了了解生成器在数据$x$上的分布$p_g$，将输入噪声变量的先验定义为$p_z(z)$，$z$是噪声变量。</p></blockquote><p>Then, GANs represent a mapping from noise space to data space as $G(z, \theta_g)$, where G is a differentiable function represented by a neural network with parameters $\theta_g$. </p><blockquote><p>GANs将噪声空间到数据空间的映射表示为$G(z, \theta_g)$，其中G是一个由参数$\theta_g$的神经网络表示的可微函数</p></blockquote><p>Other than G, the other neural network $D(x, \theta_d)$ is also defined with parameters $\theta_d$ and the output of $D(x)$ is a single scalar. $D(x)$ denotes the probability that x was from the data rather than the generator G. </p><blockquote><p>除G外，另一个神经网络$D(x, \theta_d)$ 也根据参数$\theta_d$定义，$D(x)$的输出为单标量。$D(x)$表示$x$来自数据而不是生成器G的概率。</p></blockquote><p>The discriminator D is trained to maximize the probability of giving the correct label to both training data and fake samples generated from the generator G. G is trained to minimize $\log (1 −D (G(z)))$ simultaneously .</p><blockquote><p>对鉴别器D进行训练以最大限度地提高对训练数据和从生成器G生成的伪样本给出正确标签的概率。G被训练以同时最小化$\log(1−D(G(z))$。</p></blockquote><h5 id="3-1-1-Objective-function"><a href="#3-1-1-Objective-function" class="headerlink" title="3.1.1 Objective function"></a>3.1.1 Objective function</h5><p><strong>(1) Original minimax game</strong></p><p>The objective function of GANs is :</p><p>$$\min_G \max_D V(D,G)=\mathbb{E} _ {x \sim p_{data}(x)}[\log D(x)]+\mathbb{E} _ {z \sim p_z(z)}[\log (1-D(G(z)))]$$</p><p>$\log D(x)$ is the cross-entropy between $\begin{bmatrix}1 &amp; 0 \end{bmatrix}^T$ and $\begin{bmatrix}D(x) &amp; 1-D(x) \end{bmatrix}^T$. Similarly, $\log(1-D(G(z)))$ is the cross-entropy between  $\begin{bmatrix}0 &amp; 1 \end{bmatrix}^T$ and $\begin{bmatrix}D(G(z)) &amp; 1-D(G(z)) \end{bmatrix}^T$ . </p><blockquote><p>$\log D(x)$是$\begin{bmatrix}1 &amp; 0 \end{bmatrix}^T$和$\begin{bmatrix}D(x) &amp; 1-D(x) \end{bmatrix}^T$之间的<a href="/posts/fdbf585c.html">交叉熵</a>。同样，$\log(1-D(G(z)))$是$\begin{bmatrix}0 &amp; 1 \end{bmatrix}^T$和$\begin{bmatrix}D(G(z)) &amp; 1-D(G(z)) \end{bmatrix}^T$之间的<a href="/posts/fdbf585c.html">交叉熵</a>。</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;A-Review-on-Generative-Adversarial-Networks-Algorithms-Theory-and-Applications&quot;&gt;&lt;a href=&quot;#A-Review-on-Generative-Adversarial-Network</summary>
      
    
    
    
    <category term="人工智能" scheme="https://blog.justlovesmile.top/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
    <category term="深度学习" scheme="https://blog.justlovesmile.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="GANs" scheme="https://blog.justlovesmile.top/tags/GANs/"/>
    
    <category term="论文" scheme="https://blog.justlovesmile.top/tags/%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>深度学习 | 如何理解卷积</title>
    <link href="https://blog.justlovesmile.top/posts/ef2381bd.html"/>
    <id>https://blog.justlovesmile.top/posts/ef2381bd.html</id>
    <published>2020-11-03T11:09:20.000Z</published>
    <updated>2020-11-03T11:09:20.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-什么是卷积"><a href="#1-什么是卷积" class="headerlink" title="1.什么是卷积"></a>1.什么是卷积</h2><p>对于卷积的定义，如下：</p><p><strong>连续形式</strong></p><p>$$(f×g)(n)=\int_{-\infty}^{\infty}f(\tau )g(n-\tau)d\tau$$</p><p><strong>离散形式</strong></p><p>$$(f×g)(n)=\sum_{\tau=-\infty}^{\infty}f(\tau)g(n-\tau)$$</p><blockquote><p>先对g函数进行翻转，相当于在数轴上把g函数从右边褶到左边去，也就是卷积的“卷”的由来。<br>然后再把g函数平移到n，在这个位置对两个函数的对应点相乘，然后相加，这个过程是卷积的“积”的过程。</p></blockquote><p>上述公式中有一个共同的特征：<br>$$n=\tau + (n - \tau)$$</p><p>对于这个特征，我们可以令$x=\tau$,$y=n-\tau$，那么x+y=n就是一些直线</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/v2-8be52f6bada3f7a21cebfc210d2e7ea0_hd.gif"></p><p>如果遍历这些直线，就好比，把毛巾沿着角卷起来：</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/v2-1d0c819fc7ca6f8da25435da070a2715_hd.webp"></p><h2 id="2-通俗易懂的理解卷积"><a href="#2-通俗易懂的理解卷积" class="headerlink" title="2.通俗易懂的理解卷积"></a>2.通俗易懂的理解卷积</h2><h3 id="2-1离散卷积的例子：丢骰子"><a href="#2-1离散卷积的例子：丢骰子" class="headerlink" title="2.1离散卷积的例子：丢骰子"></a>2.1离散卷积的例子：丢骰子</h3><p><strong>问题：</strong></p><p>把两枚骰子抛出去，两枚骰子点数之和为4的概率是多少</p><p><strong>表示：</strong></p><p>如果用f(x)表示第一枚骰子投出x（x∈{1,2,3,4,5,6}）的概率，g(y)表示第二枚骰子投出y（y∈{1,2,3,4,5,6}）的概率</p><p><strong>结果：</strong></p><p>两枚骰子点数加起来等于4的情况有：<br>f(1)g(3)和f(2)g(2)和f(3)g(1)</p><p>那么概率为P=f(1)g(3)+f(2)g(2)+f(3)g(1)，符合卷积的定义，把他写成标准形式就是<br>$$(f×g)(4)=\sum_{m=1}^{3}f(m)g(4-m)$$</p><h3 id="2-2连续卷积的例子：做馒头"><a href="#2-2连续卷积的例子：做馒头" class="headerlink" title="2.2连续卷积的例子：做馒头"></a>2.2连续卷积的例子：做馒头</h3><p><strong>问题：</strong></p><p>如果有一家包子铺，会生产包子，但是包子会坏掉，那么一天后包子总共坏掉了多少？</p><p><strong>表示：</strong></p><p>假设包子生产速度是f(t)，对于包子铺一天生产的包子数量是<br>$$\int_{0}^{24}f(t)dt$$<br>假设腐败速度是g(t)，那么n个包子生产出来后，24小时会腐败个数<br>$$n * g(t)$$</p><p><strong>结果：</strong></p><p>一天后，包子总共腐败了：<br>$$\int_{0}^{24}f(t)g(24-t)dt$$</p><h3 id="2-3卷积提取图像特征"><a href="#2-3卷积提取图像特征" class="headerlink" title="2.3卷积提取图像特征"></a>2.3卷积提取图像特征</h3><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/v2-05f7af4e1d59e82412832c01b1144f52_720w.jpg"></p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/v2-c9b00043ba326451979abda5417bfcdf_720w.jpg"></p><p>卷积核和图像进行点乘（dot product), 就代表卷积核里的权重单独对相应位置的Pixel进行作用</p><p><strong>这里我想强调一下点乘，虽说我们称为卷积，实际上是位置一一对应的点乘，不是真正意义的卷积</strong></p><p>比如图像位置（1,1）乘以卷积核位置（1,1），仔细观察右上角你就会发现了</p><p>例如：对于一张图片</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20201103204533.png"></p><p>我们进行手动卷积</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> torch,torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line">path=<span class="string">&quot;./1.jpg&quot;</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(path)</span><br><span class="line">transform = transforms.Compose([transforms.ToTensor()])<span class="comment">#totensor 得到（C*H*W)</span></span><br><span class="line">im = transform(img)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imshow</span>(<span class="params">img</span>):</span></span><br><span class="line">    npimg = img</span><br><span class="line">    plt.imshow(np.transpose(npimg,(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>))) <span class="comment">#chw-&gt;hwc</span></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">k = torch.ShortTensor([[<span class="number">0</span>,-<span class="number">4</span>,<span class="number">0</span>],[-<span class="number">4</span>,<span class="number">16</span>,-<span class="number">4</span>],[<span class="number">0</span>,-<span class="number">4</span>,<span class="number">0</span>]])</span><br><span class="line">stride=<span class="number">2</span> <span class="comment">#步长</span></span><br><span class="line">padding=<span class="number">0</span> <span class="comment"># 补0</span></span><br><span class="line">f = k.size(<span class="number">0</span>) <span class="comment"># 卷积核的形状</span></span><br><span class="line">channels = im.size(<span class="number">0</span>) <span class="comment">#输入的图片的通道数</span></span><br><span class="line">hin = im.size(<span class="number">1</span>) <span class="comment">#输入的图片的高</span></span><br><span class="line">win = im.size(<span class="number">2</span>) <span class="comment">#输入的图片的宽</span></span><br><span class="line">hout = math.floor((hin-f+<span class="number">2</span>*padding)/stride+<span class="number">1</span>) <span class="comment">#输出的图片的高</span></span><br><span class="line">wout = math.floor((win-f+<span class="number">2</span>*padding)/stride+<span class="number">1</span>) <span class="comment">#输出的图片的宽</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;input[&#123;&#125;,&#123;&#125;],output[&#123;&#125;,&#123;&#125;]&quot;</span>.<span class="built_in">format</span>(hin,win,hout,wout))</span><br><span class="line">output=[]</span><br><span class="line">im = im.numpy()</span><br><span class="line">k = k.numpy()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Waite for calculating...&quot;</span>)</span><br><span class="line"><span class="comment"># 自定义卷积，一一对应相乘</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(channels):</span><br><span class="line">    lines=[]</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(hout):</span><br><span class="line">        line=[]</span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(wout):</span><br><span class="line">            a=[[im[i][j*stride][n*stride],im[i][j*stride][n*stride+<span class="number">1</span>],im[i][j*stride][n*stride+<span class="number">2</span>]],[im[i][j*stride+<span class="number">1</span>][n*stride],im[i][j*stride+<span class="number">1</span>][n*stride+<span class="number">1</span>],im[i][j*stride+<span class="number">1</span>][n*stride+<span class="number">2</span>]],[im[i][j*stride+<span class="number">2</span>][n*stride],im[i][j*stride+<span class="number">2</span>][n*stride+<span class="number">1</span>],im[i][j*stride+<span class="number">2</span>][n*stride+<span class="number">2</span>]]]</span><br><span class="line">            line.append(<span class="built_in">sum</span>(<span class="built_in">sum</span>(a*k)))</span><br><span class="line">        lines.append(line)</span><br><span class="line">    output.append(lines)</span><br><span class="line"></span><br><span class="line">oo=np.array(output)</span><br><span class="line"><span class="built_in">print</span>(oo.shape)</span><br><span class="line">imshow(oo)</span><br></pre></td></tr></table></figure><p><strong>提取特征效果如下：</strong></p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20201103211407111.png"></p><p>部分内容参考<a href="https://www.zhihu.com/question/22298352/answer/637156871">知乎:如何通俗易懂的理解卷积</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;1-什么是卷积&quot;&gt;&lt;a href=&quot;#1-什么是卷积&quot; class=&quot;headerlink&quot; title=&quot;1.什么是卷积&quot;&gt;&lt;/a&gt;1.什么是卷积&lt;/h2&gt;&lt;p&gt;对于卷积的定义，如下：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;连续形式&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$</summary>
      
    
    
    
    <category term="人工智能" scheme="https://blog.justlovesmile.top/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
    <category term="深度学习" scheme="https://blog.justlovesmile.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="卷积" scheme="https://blog.justlovesmile.top/tags/%E5%8D%B7%E7%A7%AF/"/>
    
  </entry>
  
  <entry>
    <title>深度学习 | 《深度学习入门之PyTorch》阅读笔记</title>
    <link href="https://blog.justlovesmile.top/posts/bfa4054.html"/>
    <id>https://blog.justlovesmile.top/posts/bfa4054.html</id>
    <published>2020-10-23T10:43:24.000Z</published>
    <updated>2020-10-23T10:43:24.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="深度学习入门之PyTorch"><a href="#深度学习入门之PyTorch" class="headerlink" title="深度学习入门之PyTorch"></a>深度学习入门之PyTorch</h1><h2 id="第一章-深度学习介绍"><a href="#第一章-深度学习介绍" class="headerlink" title="第一章 深度学习介绍"></a>第一章 深度学习介绍</h2><h3 id="1-1-人工智能"><a href="#1-1-人工智能" class="headerlink" title="1.1 人工智能"></a>1.1 人工智能</h3><ol><li>Artificial Intelligence，人工智能，也称机器智能。</li><li>人工智能分为三大类<br>（1）弱人工智能：擅长单方面<br>（2）强人工智能：类似人类等级<br>（3）超人工智能：全方面胜过人类</li></ol><h3 id="1-2-数据挖掘，机器学习和深度学习"><a href="#1-2-数据挖掘，机器学习和深度学习" class="headerlink" title="1.2 数据挖掘，机器学习和深度学习"></a>1.2 数据挖掘，机器学习和深度学习</h3><h4 id="1-2-1-数据挖掘"><a href="#1-2-1-数据挖掘" class="headerlink" title="1.2.1 数据挖掘"></a>1.2.1 数据挖掘</h4><p>KDD（knowledge discovery in database），从数据中获取有意义的信息</p><h4 id="1-2-2-机器学习"><a href="#1-2-2-机器学习" class="headerlink" title="1.2.2 机器学习"></a>1.2.2 机器学习</h4><ol><li>机器学习是实现人工智能的一种途径，涉及多门学科</li><li>大致分为五个大类<br>（1）监督学习：从给定的训练数据集中学习出一个函数，训练集中的目标是由人标注的，常见算法包括回归和分类<br>（2）无监督学习：训练集没有人为标注，常见算法如聚类<br>（3）半监督学习：介于两者之间<br>（4）迁移学习：将已经训练好的模型参数迁移到新的模型来帮助新模型训练数据集<br>（5）增强学习：通过观察周围环境来学习</li></ol><h4 id="1-2-3-深度学习"><a href="#1-2-3-深度学习" class="headerlink" title="1.2.3 深度学习"></a>1.2.3 深度学习</h4><ol><li>机器学习的一个分支，通过模拟人脑来实现数据特征的提取</li><li>常见网络结构：DNN，CNN，RNN，GAN等等</li></ol><h2 id="第二章-深度学习框架"><a href="#第二章-深度学习框架" class="headerlink" title="第二章 深度学习框架"></a>第二章 深度学习框架</h2><h3 id="2-1-深度学习框架介绍"><a href="#2-1-深度学习框架介绍" class="headerlink" title="2.1 深度学习框架介绍"></a>2.1 深度学习框架介绍</h3><ol><li>Tensorflow<br>Google开源的基于C++开发的数学计算软件</li><li>Caffe</li><li>Theano</li><li>Torch<br>支持动态图</li><li>MXNet</li></ol><h3 id="2-2-PyTorch介绍"><a href="#2-2-PyTorch介绍" class="headerlink" title="2.2 PyTorch介绍"></a>2.2 PyTorch介绍</h3><h4 id="2-2-1-什么是PyTorch"><a href="#2-2-1-什么是PyTorch" class="headerlink" title="2.2.1 什么是PyTorch"></a>2.2.1 什么是PyTorch</h4><p>Python优先的深度学习框架，支持GPU加速和动态神经网络</p><h4 id="2-2-2-为什么使用PyTorch"><a href="#2-2-2-为什么使用PyTorch" class="headerlink" title="2.2.2 为什么使用PyTorch"></a>2.2.2 为什么使用PyTorch</h4><p>1.多学习一个框架准没错<br>2.PyTorch通过一种反向自动求导的技术，可以让你零延迟地改变神经网络<br>3.线性，直观，易于使用<br>4.代码简洁直观，底层代码友好</p><h3 id="2-3-配置PyTorch深度学习环境"><a href="#2-3-配置PyTorch深度学习环境" class="headerlink" title="2.3 配置PyTorch深度学习环境"></a>2.3 配置PyTorch深度学习环境</h3><h4 id="2-3-1-操作系统"><a href="#2-3-1-操作系统" class="headerlink" title="2.3.1 操作系统"></a>2.3.1 操作系统</h4><p>Windows，Linux，Mac</p><h4 id="2-3-2-Python开发环境的安装"><a href="#2-3-2-Python开发环境的安装" class="headerlink" title="2.3.2 Python开发环境的安装"></a>2.3.2 Python开发环境的安装</h4><p>Anaconda</p><h4 id="2-3-3-PyTorch安装"><a href="#2-3-3-PyTorch安装" class="headerlink" title="2.3.3 PyTorch安装"></a>2.3.3 PyTorch安装</h4><p>官网或者anaconda</p><p>CPU或GPU</p><p>CUDA，CuDnn</p><h2 id="第三章-多层全连接神经网络"><a href="#第三章-多层全连接神经网络" class="headerlink" title="第三章 多层全连接神经网络"></a>第三章 多层全连接神经网络</h2><h3 id="3-1-PyTorch基础"><a href="#3-1-PyTorch基础" class="headerlink" title="3.1 PyTorch基础"></a>3.1 PyTorch基础</h3><h4 id="3-1-1-Tensor张量"><a href="#3-1-1-Tensor张量" class="headerlink" title="3.1.1 Tensor张量"></a>3.1.1 Tensor张量</h4><p>Tensor相当于多维的矩阵</p><p>Tensor的数据类型有：(32位浮点型)torch.FloatTensor，（64位浮点型）torch.DoubleTensor，（16位整型）torch.ShortTensor,（32位整型）torch.IntTensor，（64位整型）torch.LongTensor</p><p><strong>导入pytorch</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure><p><strong>创建一个没有初始化的5×3矩阵</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x=torch.empty(<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure><p><strong>创建一个随机初始化矩阵</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#均匀分布[0,1],rand</span></span><br><span class="line">x=torch.rand(<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"></span><br><span class="line"><span class="comment">#正态分布，randn</span></span><br><span class="line">x=torch.randn(<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure><p><strong>构造一个0矩阵，且数据类型为long</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x=torch.zeros(<span class="number">5</span>,<span class="number">3</span>,dtype=torch.long)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure><p><strong>直接根据数据构造张量</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x=torch.tensor([<span class="number">5.5</span>,<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure><p><strong>创建一个全为1的矩阵，且数据类型为double</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x=torch.ones(<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"></span><br><span class="line">x=x.new_ones(<span class="number">5</span>,<span class="number">3</span>,dtype=torch.double)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure><p><strong>根据已有tensor建立新的tensor，且除非提供新值，将重用所给张量属性</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x=x.new_ones(<span class="number">5</span>,<span class="number">3</span>,dtype=torch.double)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"></span><br><span class="line">x=torch.randn_like(x,dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure><p><strong>获取张量的形状</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(x.size())</span><br></pre></td></tr></table></figure><blockquote><p><strong>注意</strong><br><code>torch.Size</code>本质上还是tuple，所以支持tuple的一切操作</p></blockquote><p><strong>和numpy的相互转换</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line">numpy_x = x.numpy()</span><br><span class="line"><span class="built_in">print</span>(numpy_x)</span><br><span class="line">torch_x = torch.from_numpy(numpy_x)</span><br><span class="line"><span class="built_in">print</span>(torch_x)</span><br></pre></td></tr></table></figure><p><strong>绝对值</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a=torch.randn(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"></span><br><span class="line">b=torch.<span class="built_in">abs</span>(a)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br></pre></td></tr></table></figure><p><strong>运算</strong>，例如加法</p><p><strong>形式一</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y=torch.rand(<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x+y)</span><br></pre></td></tr></table></figure><p><strong>形式二</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(torch.add(x,y))</span><br></pre></td></tr></table></figure><p><strong>形式三</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">result=torch.empty(<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line">torch.add(x,y,out=result)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure><p><strong>形式四</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y.add_(x)</span><br><span class="line"><span class="built_in">print</span>(y)</span><br></pre></td></tr></table></figure><blockquote><p><strong>注意：</strong><br>任何一个in-place改变张量的操作后面都固定一个_。例如x.copy_(y)、x.t_()将更改x</p></blockquote><p><strong>剪裁</strong>:如果在上下边界内则不变，否则大于上边界值，则改为上边界值，小于下边界值，则改为下边界值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a=torch.randn(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"></span><br><span class="line">b=torch.clamp(a,-<span class="number">0.1</span>,<span class="number">0.1</span>)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br></pre></td></tr></table></figure><p><strong>除法</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a=torch.randn(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">b=torch.randn(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">c=torch.div(a,b)</span><br><span class="line">d=torch.div(c,<span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"><span class="built_in">print</span>(d)</span><br></pre></td></tr></table></figure><blockquote><p><strong>加法</strong>add，<strong>乘积</strong>mul，<strong>除法</strong>div，<strong>求幂</strong>pow，<strong>矩阵乘法</strong>mm，<strong>矩阵向量乘法</strong>mv</p></blockquote><p><strong>改变张量形状</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x=torch.randn(<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line">y=x.view(<span class="number">16</span>)</span><br><span class="line">z=x.view(-<span class="number">1</span>,<span class="number">8</span>) <span class="comment"># -1将会自动取值</span></span><br><span class="line"><span class="built_in">print</span>(x.size(),y.size(),z.size())</span><br></pre></td></tr></table></figure><p><strong>对于只含一个元素的tensor，可以使用<code>.item()</code>来得到数值</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x=torch.randn(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(x.item())</span><br></pre></td></tr></table></figure><p><strong>使用GPU</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line">    y = torch.ones_like(x, device=device)</span><br><span class="line">    x = x.to(device)</span><br><span class="line">    z = x+y</span><br><span class="line">    <span class="built_in">print</span>(z)</span><br><span class="line">    <span class="built_in">print</span>(z.to(<span class="string">&quot;CPU&quot;</span>,torch.double))</span><br></pre></td></tr></table></figure><h4 id="3-1-2-Variable（变量）"><a href="#3-1-2-Variable（变量）" class="headerlink" title="3.1.2 Variable（变量）"></a>3.1.2 Variable（变量）</h4><p><strong>1. Autograd：自动求导</strong></p><p><strong>创建一个张量并设置requires_grad=True用来追踪其计算历史</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x=torch.ones(<span class="number">2</span>,<span class="number">2</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure><p><strong>对这个张量做一次运算</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">y=x+<span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="comment"># y是计算结果，所以他有grad_fn属性</span></span><br><span class="line"><span class="built_in">print</span>(y.grad_fn)</span><br><span class="line"><span class="comment"># 对y进行更多操作</span></span><br><span class="line">z=y*y*<span class="number">3</span></span><br><span class="line">out=z.mean()</span><br><span class="line"><span class="built_in">print</span>(z,out)</span><br></pre></td></tr></table></figure><p>.requires_grad_(…) 改变了现有张量的 requires_grad 标志。如果没有指定的话，默认输入的这个标志是 False。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">a = ((a * <span class="number">3</span>) / (a - <span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(a.requires_grad)</span><br><span class="line">a.requires_grad_(<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(a.requires_grad)</span><br><span class="line">b = (a * a).<span class="built_in">sum</span>()</span><br><span class="line"><span class="built_in">print</span>(b.grad_fn)</span><br></pre></td></tr></table></figure><p><strong>2. 梯度</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x=torch.ones(<span class="number">2</span>,<span class="number">2</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">y=x+<span class="number">2</span></span><br><span class="line">z=y*y*<span class="number">3</span></span><br><span class="line">out=z.mean()</span><br><span class="line"><span class="comment"># 现在开始反向传播，因为out是一个标量，则out.backward()和out.backward(torch.tensor(1.))等价</span></span><br><span class="line">out.backward()</span><br><span class="line"><span class="comment">#输出导数d(out)/dx</span></span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br></pre></td></tr></table></figure><p>即</p><p>$$out=\frac{1}{4}\sum_iz_i$$</p><p>$$z_i=3(x_i+2)^2$$</p><p>并且$ z _ i| _ {x_i=1}=27$，因此，有</p><p>$$\frac{\partial_{out}}{\partial_{x_i}}=\frac{3}{2}(x_i+2)$$<br>因此<br>$$\frac{\partial _ {out}}{\partial_ {x_i}}|_ {x_i=1}=\frac{9}{2}=4.5$$</p><p><strong>雅可比矩阵</strong></p><p>数学上，若有向量值函数y=f(x)，那么y相当于对x的梯度是一个雅可比矩阵（下面是一个latex数学公式）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">J=\begin&#123;bmatrix&#125;</span><br><span class="line">\frac&#123;\partial y_1&#125;&#123;\partial x_1&#125; &amp;\cdots&amp; \frac&#123;\partial y_1&#125;&#123;\partial x_n&#125; \\</span><br><span class="line">\vdots &amp; \ddots &amp; \vdots \\</span><br><span class="line">\frac&#123;\partial y_m&#125;&#123;\partial x_1&#125; &amp;\cdots&amp; \frac&#123;\partial y_m&#125;&#123;\partial x_n&#125;</span><br><span class="line">\end&#123;bmatrix&#125;</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/image-20211125183936512.png" alt="image-20211125183936512"></p><p>通常来说，torch.autograd是计算雅可比向量积的一个引擎。也就是说，给定任意向量v，计算乘积$v^TJ$.如果v恰好是一个标量函数l=g(y)的导数，即$v=(\frac{\partial l}{\partial y_1} \cdots \frac{\partial l}{\partial y_m}^T)$，那么根据链式法则，雅可比向量积应该是l对x的导数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">J^T·v=\begin&#123;bmatrix&#125;</span><br><span class="line">\frac&#123;\partial y_1&#125;&#123;\partial x_1&#125; &amp;\cdots&amp; \frac&#123;\partial y_m&#125;&#123;\partial x_1&#125; \\</span><br><span class="line">\vdots &amp; \ddots &amp; \vdots \\</span><br><span class="line">\frac&#123;\partial y_1&#125;&#123;\partial x_n&#125; &amp;\cdots&amp; \frac&#123;\partial y_m&#125;&#123;\partial x_n&#125;</span><br><span class="line">\end&#123;bmatrix&#125;</span><br><span class="line">\begin&#123;bmatrix&#125;</span><br><span class="line">\frac&#123;\partial l&#125;&#123;\partial y_1&#125;\\</span><br><span class="line">\cdots\\</span><br><span class="line">\frac&#123;\partial l&#125;&#123;\partial y_m&#125;</span><br><span class="line">\end&#123;bmatrix&#125;=</span><br><span class="line">\begin&#123;bmatrix&#125;</span><br><span class="line">\frac&#123;\partial l&#125;&#123;\partial x_1&#125;\\</span><br><span class="line">\cdots\\</span><br><span class="line">\frac&#123;\partial l&#125;&#123;\partial x_n&#125;</span><br><span class="line">\end&#123;bmatrix&#125;</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/image-20211125184143947.png" alt="image-20211125184143947"></p><p>(注意：行向量的$v^T⋅J$也可以被视作列向量的$J^T⋅v$)</p><p>雅可比向量积的这一特性使得将外部梯度输入到具有非标量输出的模型中变得非常方便。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x=torch.randn(<span class="number">3</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">y=x*<span class="number">2</span></span><br><span class="line"><span class="keyword">while</span> y.data.norm() &lt;<span class="number">1000</span>:</span><br><span class="line">    y=y*<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(y)</span><br></pre></td></tr></table></figure><p>在这种情况下，y 不再是标量。torch.autograd 不能直接计算完整的雅可比矩阵，但是如果我们只想要雅可比向量积，只需将这个向量作为参数传给 backward</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">v = torch.tensor([<span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">0.0001</span>], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">y.backward(v)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br></pre></td></tr></table></figure><p>也可以通过将代码块包装在 with torch.no_grad(): 中，来阻止autograd跟踪设置了 .requires_grad=True 的张量的历史记录。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(x.requires_grad)</span><br><span class="line"><span class="built_in">print</span>((x ** <span class="number">2</span>).requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="built_in">print</span>((x ** <span class="number">2</span>).requires_grad)</span><br></pre></td></tr></table></figure><p><strong>3. Variable</strong></p><p>Variable和Tensor的区别，Variable会被放入计算图中，然后进行前向传播，反向传播，自动求导</p><p>Variable是在torch.autograd.Variable中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line">x=Variable(torch.Tensor([<span class="number">1</span>]),requires_grad=<span class="literal">True</span>)</span><br><span class="line">w=Variable(torch.Tensor([<span class="number">2</span>]),requires_grad=<span class="literal">True</span>)</span><br><span class="line">b=Variable(torch.Tensor([<span class="number">3</span>]),requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">y=w*x+b</span><br><span class="line"></span><br><span class="line">y.backward()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"><span class="built_in">print</span>(w.grad)</span><br><span class="line"><span class="built_in">print</span>(b.grad)</span><br></pre></td></tr></table></figure><p><strong>搭建一个简单的神经网络</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">batch_n = <span class="number">100</span> <span class="comment"># 一个批次中输入数据的数量</span></span><br><span class="line">hidden_layer = <span class="number">100</span> <span class="comment"># 经过隐藏层后保留的数据特征的个数</span></span><br><span class="line">input_data = <span class="number">1000</span> <span class="comment"># 每个数据包含的数据量</span></span><br><span class="line">output_data = <span class="number">10</span> <span class="comment"># 每个输出的数据包含的数据量</span></span><br><span class="line"></span><br><span class="line">x=torch.randn(batch_n,input_data) <span class="comment">#100*1000</span></span><br><span class="line">y=torch.randn(batch_n,output_data) <span class="comment">#100*10</span></span><br><span class="line"></span><br><span class="line">w1=torch.randn(input_data,hidden_layer) <span class="comment">#1000*100</span></span><br><span class="line">w2=torch.randn(hidden_layer,output_data) <span class="comment"># 100*10</span></span><br><span class="line"></span><br><span class="line">epoch_n = <span class="number">20</span> <span class="comment">#训练的次数</span></span><br><span class="line">learning_rate = <span class="number">1e-6</span> <span class="comment">#学习率</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epoch_n):</span><br><span class="line">    h1=x.mm(w1)<span class="comment">#100*100</span></span><br><span class="line">    h1=h1.clamp(<span class="built_in">min</span>=<span class="number">0</span>) <span class="comment"># if x&lt;0 ,x=0</span></span><br><span class="line">    y_pred=h1.mm(w2) <span class="comment">#100*10，前向传播预测结果</span></span><br><span class="line">    </span><br><span class="line">    loss = (y_pred - y).<span class="built_in">pow</span>(<span class="number">2</span>).<span class="built_in">sum</span>() <span class="comment">#损失函数，即均方误差</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Epoch:&#123;&#125;, Loss:&#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(epoch,loss))</span><br><span class="line">    grad_y_pred = <span class="number">2</span>*(y_pred-y) <span class="comment">#dloss/dy</span></span><br><span class="line">    grad_w2 = h1.t().mm(grad_y_pred) <span class="comment">#dloss/dy * dy/dw2</span></span><br><span class="line">    </span><br><span class="line">    grad_h = grad_y_pred.clone() <span class="comment">#复制</span></span><br><span class="line">    grad_h = grad_h.mm(w2.t()) <span class="comment">#dloss/dy * dy/dh1</span></span><br><span class="line">    grad_h.clamp_(<span class="built_in">min</span>=<span class="number">0</span>) <span class="comment"># if x&lt;0 ,x=0</span></span><br><span class="line">    grad_w1 = x.t().mm(grad_h) </span><br><span class="line">    </span><br><span class="line">    w1 -= learning_rate*grad_w1 <span class="comment">#梯度下降</span></span><br><span class="line">    w2 -= learning_rate*grad_w2</span><br></pre></td></tr></table></figure><p><strong>使用Variable搭建一个自动计算梯度的神经网络</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line">batch_n = <span class="number">100</span> <span class="comment"># 一个批次中输入数据的数量</span></span><br><span class="line">hidden_layer = <span class="number">100</span> <span class="comment"># 经过隐藏层后保留的数据特征的个数</span></span><br><span class="line">input_data = <span class="number">1000</span> <span class="comment"># 每个数据包含的数据量</span></span><br><span class="line">output_data = <span class="number">10</span> <span class="comment"># 每个输出的数据包含的数据量</span></span><br><span class="line"></span><br><span class="line">x=Variable(torch.randn(batch_n,input_data),requires_grad = <span class="literal">False</span>) <span class="comment">#requires_grad = False不保留梯度</span></span><br><span class="line">y=Variable(torch.randn(batch_n,output_data),requires_grad = <span class="literal">False</span>)</span><br><span class="line">w1=Variable(torch.randn(input_data,hidden_layer),requires_grad = <span class="literal">True</span>) <span class="comment">#requires_grad = True自动保留梯度</span></span><br><span class="line">w2=Variable(torch.randn(hidden_layer,output_data),requires_grad = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">epoch_n = <span class="number">20</span></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epoch_n):</span><br><span class="line">    y_pred = x.mm(w1).clamp(<span class="built_in">min</span> = <span class="number">0</span>).mm(w2) <span class="comment">#y_pred=w2*(w1*x)</span></span><br><span class="line">    loss = (y_pred-y).<span class="built_in">pow</span>(<span class="number">2</span>).<span class="built_in">sum</span>() <span class="comment">#损失函数</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Epoch:&#123;&#125;,Loss:&#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(epoch,loss))</span><br><span class="line">    </span><br><span class="line">    loss.backward() <span class="comment">#后向传播计算</span></span><br><span class="line">    </span><br><span class="line">    w1.data -= learning_rate*w1.grad.data</span><br><span class="line">    w2.data -=learning_rate*w2.grad.data</span><br><span class="line">    </span><br><span class="line">    w1.grad.data.zero_() <span class="comment">#置0</span></span><br><span class="line">    w2.grad.data.zero_()</span><br></pre></td></tr></table></figure><p><strong>使用nn.Module自定义传播函数来搭建神经网络</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line">batch_n = <span class="number">100</span></span><br><span class="line">hidden_layer = <span class="number">100</span></span><br><span class="line">input_data = <span class="number">1000</span></span><br><span class="line">output_data = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Model,self).__init__()</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,input_n,w1,w2</span>):</span></span><br><span class="line">        x = torch.mm(input_n,w1)</span><br><span class="line">        x = torch.clamp(x,<span class="built_in">min</span>=<span class="number">0</span>)</span><br><span class="line">        x = torch.mm(x,w2)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    </span><br><span class="line">model = Model()</span><br><span class="line"></span><br><span class="line">x=Variable(torch.randn(batch_n,input_data),requires_grad = <span class="literal">False</span>) <span class="comment">#requires_grad = False不保留梯度</span></span><br><span class="line">y=Variable(torch.randn(batch_n,output_data),requires_grad = <span class="literal">False</span>)</span><br><span class="line">w1=Variable(torch.randn(input_data,hidden_layer),requires_grad = <span class="literal">True</span>) <span class="comment">#requires_grad = True自动保留梯度</span></span><br><span class="line">w2=Variable(torch.randn(hidden_layer,output_data),requires_grad = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">epoch_n = <span class="number">20</span></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epoch_n):</span><br><span class="line">    y_pred = model(x,w1,w2)</span><br><span class="line">    loss = (y_pred-y).<span class="built_in">pow</span>(<span class="number">2</span>).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Epoch:&#123;&#125;,Loss:&#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(epoch,loss))</span><br><span class="line">    loss.backward() <span class="comment">#后向传播计算</span></span><br><span class="line">    </span><br><span class="line">    w1.data -= learning_rate*w1.grad.data</span><br><span class="line">    w2.data -=learning_rate*w2.grad.data</span><br><span class="line">    </span><br><span class="line">    w1.grad.data.zero_() <span class="comment">#置0</span></span><br><span class="line">    w2.grad.data.zero_()</span><br></pre></td></tr></table></figure><h4 id="3-1-3-Dataset-数据集"><a href="#3-1-3-Dataset-数据集" class="headerlink" title="3.1.3 Dataset(数据集)"></a>3.1.3 Dataset(数据集)</h4><p>torch.utils.data.Dataset是代表这一数据的抽象类，可以自己定义数据类继承和重写这个抽象类，只需要定义<code>__len__</code>和<code>__getitem__</code>函数即可</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">myDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, csv_file, txt_file, root_dir, other_file</span>):</span></span><br><span class="line">        self.csv_data = pd.read_csv(csv_file)</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(txt_file, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            data_list=f.readlines()</span><br><span class="line">        self.txt_data = data_list</span><br><span class="line">        self.root_dir = root_dir</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.csv_data)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self,idx</span>):</span></span><br><span class="line">        data = (self.csv_data[idx],self.txt_data[idx])</span><br><span class="line">        <span class="keyword">return</span> data</span><br></pre></td></tr></table></figure><p>通过上面的方式，可以定义需要的数据类，可以通过迭代的方法取得每一个数据，但是这样很难实现取batch，shuffle或者多线程去读取数据，所以Pytorch中提供了torch.utils.data.DataLoader来定义一个新迭代器</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line">dataiter = DataLoader(myDataset,batch_size=<span class="number">32</span>)</span><br></pre></td></tr></table></figure><h4 id="3-1-4-nn-Module-模组"><a href="#3-1-4-nn-Module-模组" class="headerlink" title="3.1.4 nn.Module(模组)"></a>3.1.4 nn.Module(模组)</h4><p>所有的层结构和损失函数来自torch.nn</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">net_name</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,other_arguments</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(net_name, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels,out_channels, kernel_size)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>一个神经网络的典型训练过程如下：</p><ul><li>定义包含一些可学习参数(或者叫权重）的神经网络</li><li>在输入数据集上迭代</li><li>通过网络处理输入</li><li>计算loss(输出和正确答案的距离）</li><li>将梯度反向传播给网络的参数</li><li>更新网络的权重，一般使用一个简单的规则：weight = weight - learning_rate * gradient</li></ul><p><strong>使用torch.nn内的序列容器Sequential</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">batch_n = <span class="number">100</span></span><br><span class="line">hidden_layer = <span class="number">100</span></span><br><span class="line">input_data = <span class="number">1000</span></span><br><span class="line">output_data = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一种方式</span></span><br><span class="line">models_1 = torch.nn.Sequential(</span><br><span class="line">    torch.nn.Linear(input_data,hidden_layer),</span><br><span class="line">    torch.nn.ReLU(),</span><br><span class="line">    torch.nn.Linear(hidden_layer,output_data)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二种方式</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line">models_2 = torch.nn.Sequential(OrderedDict([</span><br><span class="line">    (<span class="string">&quot;Line1&quot;</span>,torch.nn.Linear(input_data,hidden_layer)),</span><br><span class="line">    (<span class="string">&quot;ReLU1&quot;</span>,torch.nn.ReLU()),</span><br><span class="line">    (<span class="string">&quot;Line2&quot;</span>,torch.nn.Linear(hidden_layer,output_data))])    </span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(models_1)</span><br><span class="line"><span class="built_in">print</span>(models_2)</span><br></pre></td></tr></table></figure><p><strong>使用nn.Module定义一个神经网络</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        <span class="comment"># 输入图像channel：1；输出channel：6；5x5卷积核</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># an affine operation: y = Wx + b</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># 2x2 Max pooling</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="comment"># 如果是方阵,则可以只使用一个数字进行定义</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        x = x.view(-<span class="number">1</span>, self.num_flat_features(x))</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">num_flat_features</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        size = x.size()[<span class="number">1</span>:]  <span class="comment"># 除去批处理维度的其他所有维度</span></span><br><span class="line">        num_features = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        <span class="keyword">return</span> num_features</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br></pre></td></tr></table></figure><h4 id="3-1-5-torch-optim-优化"><a href="#3-1-5-torch-optim-优化" class="headerlink" title="3.1.5 torch.optim(优化)"></a>3.1.5 torch.optim(优化)</h4><p>优化算法分为两大类：</p><p>（1）一阶优化算法<br>使用各个参数的梯度值来更新参数，最常用的是梯度下降。梯度下降的功能是通过寻找最小值，控制方差，更新模型参数，最终使模型收敛，网络的参数更新公式<br>$$\theta = \theta - \eta × \frac{\partial J(\theta)}{\partial \theta}$$<br>其中$\eta$是学习率，$\frac{\partial J(\theta)}{\partial \theta}$是函数的梯度</p><p>（2）二阶优化算法<br>二阶优化算法使用了二阶导数（Hessian方法）来最小化或最大化损失函数，主要是基于牛顿法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer=torch.optim.SGD(model.parameters(),lr=<span class="number">0.01</span>,momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure><h4 id="3-1-6-模型的保存和加载"><a href="#3-1-6-模型的保存和加载" class="headerlink" title="3.1.6 模型的保存和加载"></a>3.1.6 模型的保存和加载</h4><p>1.保存</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#保存模型</span></span><br><span class="line">torch.save(model,path)</span><br><span class="line"><span class="comment">#保存模型的状态</span></span><br><span class="line">torch.save(model.state_dict(),path)</span><br></pre></td></tr></table></figure><p>2.加载</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#加载完整的模型</span></span><br><span class="line">load_model = torch.load(path)</span><br><span class="line"><span class="comment">#加载模型参数，需要先导入模型的结构</span></span><br><span class="line">model.load_state_dic(torch.load(path))</span><br></pre></td></tr></table></figure><h3 id="3-2-线性模型"><a href="#3-2-线性模型" class="headerlink" title="3.2 线性模型"></a>3.2 线性模型</h3><h4 id="3-2-1-介绍"><a href="#3-2-1-介绍" class="headerlink" title="3.2.1 介绍"></a>3.2.1 介绍</h4><p>f(x)=wx+b</p><p>f(x)=w1x1+w2x2+…+wdxd+b</p><p>w和b都是需要学习的参数</p><h4 id="3-2-2-一维线性回归"><a href="#3-2-2-一维线性回归" class="headerlink" title="3.2.2 一维线性回归"></a>3.2.2 一维线性回归</h4><p>给定数据集D={(x1,y1),(x2,y2),…,(xm,ym)}，线性回归希望得到一个f(x)=wx+b能够很好的拟合y</p><p>方法是利用$Loss=\sum_{i=1}^m(f(x_i)-y_i)^2$来衡量误差，即均方误差，那么<br>$$(w^*,b^*)=arg\min_{w,b}\sum_{i=1}^m(f(x_i)-y_i)^2=arg\min_{w,b}\sum_{i=1}^m(y_i-wx_i-b)^2$$</p><p>求解办法：求它的偏导数,并让其为0来估计参数<br>$$\frac{\partial Loss_{(w,b)}}{\partial w} = 2(w\sum_{i=1}^{m}x_i^2-\sum_{i=1}^{m}(y_i-b)x_i)=0$$<br>$$\frac{\partial Loss_{(w,b)}}{\partial b} = 2(mb-\sum_{i=1}^{m}(y_i-wx_i))=0$$<br>得到w和b的最优解<br>$$w=\frac{\sum_{i=1}^{m}y_i(x_i- \bar x)}{\sum_{i=1}^{m}x_i^2-\frac{1}{m}(\sum_{i=1}^{m}x_i)^2}$$<br>$$b=\frac{1}{m}\sum_{i=1}^{m}(y_i-wx_i)$$<br>其中$\bar x$是x的均值<br>$$\bar x = \frac{1}{m}\sum_{i=1}^{m}x_i$$</p><h4 id="3-2-3-多维线性回归"><a href="#3-2-3-多维线性回归" class="headerlink" title="3.2.3 多维线性回归"></a>3.2.3 多维线性回归</h4><p>$$f(x_i)=w^Tx_i+b$$<br>为使得$\sum_{i=1}^{m}(f(x_i)-y_i)^2$最小，这也称为“多元线性回归”，使用最小二乘法对w和b进行估计，假设有d个属性，将w和d写入同一个矩`阵，将数据集D表示成一个m×(d+1)的矩阵X，即</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">X=\begin&#123;bmatrix&#125;</span><br><span class="line">x_&#123;11&#125; &amp; x_&#123;12&#125; &amp; \cdots &amp; x_&#123;1d&#125; &amp; 1 \\</span><br><span class="line">x_&#123;21&#125; &amp; x_&#123;22&#125; &amp; \cdots &amp; x_&#123;2d&#125; &amp; 1 \\</span><br><span class="line">\vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\</span><br><span class="line">x_&#123;m1&#125; &amp; x_&#123;m2&#125; &amp; \cdots &amp; x_&#123;md&#125; &amp; 1</span><br><span class="line">\end&#123;bmatrix&#125;=</span><br><span class="line">\begin&#123;bmatrix&#125;</span><br><span class="line">x_1^T &amp; 1\\</span><br><span class="line">x_2^T &amp; 1\\</span><br><span class="line">\vdots &amp; \vdots\\</span><br><span class="line">x_m^T &amp; 1</span><br><span class="line">\end&#123;bmatrix&#125;</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/image-20211125184508266.png" alt="image-20211125184508266"></p><p>将目标y也写成乘向量的形式y=(y1,y2,…,ym),那么可得<br>$$w^* = arg \min_w(y-Xw)^T(y-Xw)$$<br>对其求导，令它为0<br>$$\frac{\partial Loss_w}{\partial w}=2X^T(Xw-y)=0$$</p><blockquote><p>上面涉及到矩阵的逆运算，所以需要$X^TX$是一个满秩矩阵或者正定矩阵</p></blockquote><p>可以得到:<br>$$w ^ * =(X^TX)^{-1}X^Ty$$<br>故回归模型可以写成：<br>$$f(x _ i)=x _ i^T(X^TX)^{-1}X^Ty$$</p><h4 id="3-2-4-一维线性回归的代码实现"><a href="#3-2-4-一维线性回归的代码实现" class="headerlink" title="3.2.4 一维线性回归的代码实现"></a>3.2.4 一维线性回归的代码实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x_train = np.array([[<span class="number">3.3</span>],[<span class="number">4.4</span>],[<span class="number">5.5</span>],[<span class="number">6.71</span>],[<span class="number">6.93</span>],[<span class="number">4.168</span>],[<span class="number">9.779</span>],[<span class="number">6.182</span>],[<span class="number">7.59</span>],[<span class="number">2.167</span>],[<span class="number">7.042</span>],[<span class="number">10.791</span>],[<span class="number">5.313</span>],[<span class="number">7.997</span>],[<span class="number">3.1</span>]],dtype=np.float32)</span><br><span class="line">y_train = np.array([[<span class="number">1.7</span>],[<span class="number">2.76</span>],[<span class="number">2.09</span>],[<span class="number">3.19</span>],[<span class="number">1.694</span>],[<span class="number">1.573</span>],[<span class="number">3.366</span>],[<span class="number">2.596</span>],[<span class="number">2.53</span>],[<span class="number">1.221</span>],[<span class="number">2.827</span>],[<span class="number">3.465</span>],[<span class="number">1.65</span>],[<span class="number">2.904</span>],[<span class="number">1.3</span>]],dtype=np.float32)</span><br><span class="line"></span><br><span class="line">x_train = torch.from_numpy(x_train)</span><br><span class="line">y_train = torch.from_numpy(y_train)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearRegression</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LinearRegression,self).__init__() <span class="comment">#继承父类</span></span><br><span class="line">        self.linear = nn.Linear(<span class="number">1</span>,<span class="number">1</span>) <span class="comment"># 1*1</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        out=self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    model = LinearRegression().cuda()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    model = LinearRegression()</span><br><span class="line"></span><br><span class="line">criterion = torch.nn.MSELoss() <span class="comment"># 均方误差</span></span><br><span class="line"><span class="comment">#优化函数，model.parameters()为该实例中可优化的参数，lr为参数优化的选项（学习率等）</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(),lr=<span class="number">1e-3</span>) <span class="comment">#梯度下降</span></span><br><span class="line"></span><br><span class="line">num_epochs = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        inputs = Variable(x_train).cuda()</span><br><span class="line">        target = Variable(y_train).cuda()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        inputs = Variable(x_train)</span><br><span class="line">        target = Variable(y_train)</span><br><span class="line">    <span class="comment"># forward</span></span><br><span class="line">    out = model(inputs)</span><br><span class="line">    loss = criterion(out,target) <span class="comment">#均方误差</span></span><br><span class="line">    <span class="comment"># backward</span></span><br><span class="line">    optimizer.zero_grad() <span class="comment">#置0</span></span><br><span class="line">    loss.backward() <span class="comment">#求梯度</span></span><br><span class="line">    optimizer.step() <span class="comment">#更新所有的参数，梯度下降</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span>(epoch+<span class="number">1</span>)%<span class="number">50</span>==<span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Epoch[&#123;&#125;/&#123;&#125;],Loss:&#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch+<span class="number">1</span>,num_epochs,loss))</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">eval</span>() <span class="comment">#将模型变成测试模式</span></span><br><span class="line">predict = model(Variable(x_train))</span><br><span class="line">predict = predict.data.numpy()</span><br><span class="line"><span class="comment">#画图</span></span><br><span class="line"><span class="comment">#plt.plot(x_train.numpy(),y_train.numpy(),&#x27;ro&#x27;,label=&#x27;Original data&#x27;)</span></span><br><span class="line"><span class="comment">#plt.plot(x_train.numpy(),predict,label=&quot;Fitting Line&quot;)</span></span><br><span class="line"><span class="comment">#plt.show()</span></span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20201022195143.png"></p><h4 id="3-2-5-多项式回归"><a href="#3-2-5-多项式回归" class="headerlink" title="3.2.5 多项式回归"></a>3.2.5 多项式回归</h4><p>对于$y=b+w_1×x+w_2×x^2+w_3×x^3$，预处理数据，变成矩阵形式</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X=\begin&#123;bmatrix&#125;</span><br><span class="line">x_1 &amp; x_1^2 &amp; x_1^3 \\</span><br><span class="line">x_2 &amp; x_2^2 &amp; x_2^3 \\</span><br><span class="line">\vdots &amp; \ddots &amp; \vdots \\</span><br><span class="line">x_n &amp; x_n^2 &amp; x_n^3</span><br><span class="line">\end&#123;bmatrix&#125;</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/image-20211125184611404.png" alt="image-20211125184611404"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_features</span>(<span class="params">x</span>):</span></span><br><span class="line">    x=x.unsqueeze(<span class="number">1</span>)  <span class="comment"># 在第1维（从0开始）增加一维</span></span><br><span class="line">    <span class="keyword">return</span> torch.cat([x ** i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">4</span>)],<span class="number">1</span>) <span class="comment">#1代表横着拼接x,x^2,x^3</span></span><br><span class="line"></span><br><span class="line">w_target = torch.FloatTensor([<span class="number">0.5</span>,<span class="number">3</span>,<span class="number">2.4</span>]).unsqueeze(<span class="number">1</span>) <span class="comment"># 在第1维（从0开始）加一层</span></span><br><span class="line">b_target = torch.FloatTensor([<span class="number">0.9</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="comment">#定义∑wix^i+b</span></span><br><span class="line">    <span class="keyword">return</span> x.mm(w_target) + b_target[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_batch</span>(<span class="params">batch_size=<span class="number">32</span></span>):</span></span><br><span class="line">    <span class="comment">#产生数据</span></span><br><span class="line">    random = torch.randn(batch_size)</span><br><span class="line">    x = make_features(random)</span><br><span class="line">    y = f(x)</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        <span class="keyword">return</span> Variable(x).cuda(),Variable(y).cuda()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> Variable(x),Variable(y)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">poly_model</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(poly_model,self).__init__()</span><br><span class="line">        self.poly = nn.Linear(<span class="number">3</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        out = self.poly(x)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    model = poly_model().cuda()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    model = poly_model()</span><br><span class="line">    </span><br><span class="line">criterion = nn.MSELoss() <span class="comment"># 均方误差</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(),lr=<span class="number">1e-3</span>)<span class="comment">#梯度下降</span></span><br><span class="line"></span><br><span class="line">epoch = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    batch_x,batch_y = get_batch()</span><br><span class="line">    <span class="comment">#前向传播</span></span><br><span class="line">    output = model(batch_x)</span><br><span class="line">    loss = criterion(output,batch_y)</span><br><span class="line">    epoch+=<span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> epoch%<span class="number">50</span> ==<span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Epoch:&#123;&#125;,Loss:&#123;:.6f&#125;&quot;</span>.<span class="built_in">format</span>(epoch,loss.data.item()))</span><br><span class="line">    optimizer.zero_grad() <span class="comment">#置0</span></span><br><span class="line">    loss.backward() <span class="comment">#后向传播</span></span><br><span class="line">    optimizer.step() <span class="comment">#优化参数</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> loss &lt;<span class="number">1e-2</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    </span><br></pre></td></tr></table></figure><blockquote><p>注意：<br><code>torch.nn</code>只支持小批量处理<code>(mini-batches）</code>。整个<code>torch.nn</code>包只支持小批量样本的输入，不支持单个样本的输入。<br>比如，<code>nn.Conv2d</code> 接受一个4维的张量，即<code>nSamples x nChannels x Height x Width</code>.<br>如果是一个单独的样本，只需要使用<code>input.unsqueeze(0)</code>来添加一个“假的”批大小维度。</p></blockquote><h3 id="3-3-分类问题"><a href="#3-3-分类问题" class="headerlink" title="3.3 分类问题"></a>3.3 分类问题</h3><h4 id="3-3-1-问题介绍"><a href="#3-3-1-问题介绍" class="headerlink" title="3.3.1 问题介绍"></a>3.3.1 问题介绍</h4><p>机器学习中的监督学习主要分为回归问题和分类问题，对于回归问题，希望预测的结果是连续的，对于分类问题所预测的结果是离散的。</p><p>监督学习从数据中学习一个分类模型或者分类决策函数，被称为分类器</p><h4 id="3-3-2-Logistic起源"><a href="#3-3-2-Logistic起源" class="headerlink" title="3.3.2 Logistic起源"></a>3.3.2 Logistic起源</h4><p>著名的二分类算法，Logistic回归。起源于对人口数量增长情况的研究</p><h4 id="3-3-3-Logistic分布"><a href="#3-3-3-Logistic分布" class="headerlink" title="3.3.3 Logistic分布"></a>3.3.3 Logistic分布</h4><p>设x是连续的随机变量，服从Logistic分布是指X的分布函数和密度函数是如下<br>$$F(x)=P(X≤x)=\frac{1}{1+e^{-(x-\mu)/\gamma}}$$<br>$$f(x)=\frac{e^{-(x-\mu)/\gamma}}{\gamma (1+e^{-(x-\mu)/\gamma})^2}$$<br>其中μ影响中心对称点的位置，γ越小中心点附件的增长速度越快<br>Sigmoid函数是Logistic分布函数中γ=1，μ=0的特殊形式，表达式如下：$$p(x)=\frac{1}{1+e^{-x}}$$</p><h4 id="3-3-4-二分类的Logistic回归"><a href="#3-3-4-二分类的Logistic回归" class="headerlink" title="3.3.4 二分类的Logistic回归"></a>3.3.4 二分类的Logistic回归</h4><p>假设输入的数据的特征向量$x∈R^n$，那么决策边界可以表示为$\sum_{i=1}^{n}w_ix_i+b=0$，建设存在一个样本点使得$h_w(x)=\sum_{i=1}^{n}w_ix_i+b&gt;0$，那么判定它的类别是1，如果&lt;0，判定其类别是0.<br>Logistic回归通过找到分类概率P(Y=1)与输入变量x的直接关系，然后通过比较概率值来判断类别，简单来说就是通过计算下面两个概率分布<br>$$P(Y=0|x)=\frac{1}{1+e^{wx+b}}$$<br>$$P(Y=1|x)=\frac{e^{wx+b}}{1+e^{wx+b}}$$<br>其中w是权重，b是偏置</p><blockquote><p>一个事件发生的几率（odds）是指该事件发生的概率（p）与不发生的概率的比值（1-p），该事件的对数几率或logit函数是：$logit(p)=log\frac{p}{1-p}$</p></blockquote><p>对于Logistic回归而言，可以得到：<br>$$log \frac{P(Y=1|x)}{1-P(Y=1|x)}=wx+b$$</p><h4 id="3-3-5-模型的参数估计"><a href="#3-3-5-模型的参数估计" class="headerlink" title="3.3.5 模型的参数估计"></a>3.3.5 模型的参数估计</h4><p>对于给定的训练集数据T={(x1,y1),(x2,y2),…,(xn,yn)}，其中$x_i \in R^n,y_i \in ${0,1}，假设P(Y=1|x)=Π(x)，那么P(Y=0|x)=1-Π(x)，所以似然函数为：<br>$$\prod_{i=1}^{n}[\pi (x_i)]^{y_1}[1-\pi (x_i)]^{1-y_i}$$<br>取对数后的对数似然函数：<br>$$L(w)=\sum_{i=1}^{n}[y_i(wx_i+b)-log(1+e^{wx_i+b})]$$<br>用L(w)对w求导：<br>$$\frac{\partial L(w)}{\partial w}=\sum_{i=1}^{n}y_ix_i-\sum_{i=1}^{n}\frac{e^{wx_i+b}}{1+e^{wx_i+b}}x_i=\sum_{i=1}^{n}(y_i-logit(wx_i))x_i$$<br>$$\frac{\partial L(w)}{\partial b}=\sum_{i=1}^{n}y_i-\sum_{i=1}^{n}\frac{e^{wx_i+b}}{1+e^{wx_i+b}}=\sum_{i=1}^{n}(y_i-logit(wx_i))$$</p><h4 id="3-3-6-Logistic回归的代码实现"><a href="#3-3-6-Logistic回归的代码实现" class="headerlink" title="3.3.6 Logistic回归的代码实现"></a>3.3.6 Logistic回归的代码实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="comment">#获取数据</span></span><br><span class="line">url=<span class="string">&quot;https://cdn.jsdelivr.net/gh/Justlovesmile/code-of-learn-deep-learning-with-pytorch/chapter3_NN/logistic-regression/data.txt&quot;</span></span><br><span class="line">data = requests.get(url)</span><br><span class="line">data_list=data.text.split(<span class="string">&#x27;\n&#x27;</span>)[:-<span class="number">1</span>]</span><br><span class="line">data_list=[i.split(<span class="string">&#x27;,&#x27;</span>) <span class="keyword">for</span> i <span class="keyword">in</span> data_list]</span><br><span class="line">data = [(<span class="built_in">float</span>(i[<span class="number">0</span>]),<span class="built_in">float</span>(i[<span class="number">1</span>]),<span class="built_in">float</span>(i[<span class="number">2</span>])) <span class="keyword">for</span> i <span class="keyword">in</span> data_list]</span><br><span class="line"></span><br><span class="line">np_data = np.array(data, dtype=<span class="string">&#x27;float32&#x27;</span>) <span class="comment"># 转换成 numpy array</span></span><br><span class="line">x_data = torch.from_numpy(np_data[:, <span class="number">0</span>:<span class="number">2</span>]) <span class="comment"># 转换成 Tensor, 大小是 [100, 2]</span></span><br><span class="line">y_data = torch.from_numpy(np_data[:, -<span class="number">1</span>]).unsqueeze(<span class="number">1</span>) <span class="comment"># 转换成 Tensor，大小是 [100, 1]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#print(x_data,y_data)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#画数据的散点图</span></span><br><span class="line">x0=<span class="built_in">list</span>(<span class="built_in">filter</span>(<span class="keyword">lambda</span> x:x[-<span class="number">1</span>]==<span class="number">0.0</span>,data))</span><br><span class="line">x1=<span class="built_in">list</span>(<span class="built_in">filter</span>(<span class="keyword">lambda</span> x:x[-<span class="number">1</span>]==<span class="number">1.0</span>,data))</span><br><span class="line">plot_x0_0 = [i[<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> x0]</span><br><span class="line">plot_x0_1 = [i[<span class="number">1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> x0]</span><br><span class="line">plot_x1_0 = [i[<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> x1]</span><br><span class="line">plot_x1_1 = [i[<span class="number">1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> x1]</span><br><span class="line"></span><br><span class="line">plt.plot(plot_x0_0,plot_x0_1,<span class="string">&#x27;ro&#x27;</span>,label=<span class="string">&quot;x_0&quot;</span>) <span class="comment">#0类用红色</span></span><br><span class="line">plt.plot(plot_x1_0,plot_x1_1,<span class="string">&#x27;bo&#x27;</span>,label=<span class="string">&quot;x_1&quot;</span>) <span class="comment">#1类用蓝色</span></span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>) <span class="comment">#图例的位置</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#分类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogisticRegression</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LogisticRegression,self).__init__() <span class="comment">#继承</span></span><br><span class="line">        self.lr = nn.Linear(<span class="number">2</span>,<span class="number">1</span>) <span class="comment">#2*1</span></span><br><span class="line">        self.sm = nn.Sigmoid() <span class="comment">#sigmoid函数</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        x=self.lr(x)</span><br><span class="line">        x=self.sm(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">logistic_model = LogisticRegression()</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    logistic_model.cuda()</span><br><span class="line"></span><br><span class="line">criterion = nn.BCELoss() <span class="comment">#二分类的损失函数</span></span><br><span class="line"><span class="comment">#随机梯度下降优化，parameters是可优化参数，lr是学习率，momentum是动量因子</span></span><br><span class="line">optimizer = torch.optim.SGD(logistic_model.parameters(),lr=<span class="number">1e-3</span>,momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20000</span>):</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        x=Variable(x_data).cuda()</span><br><span class="line">        y=Variable(y_data).cuda()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        x=Variable(x_data)</span><br><span class="line">        y=Variable(y_data)</span><br><span class="line">    <span class="comment">#forward</span></span><br><span class="line">    out = logistic_model(x)</span><br><span class="line">    loss = criterion(out,y)</span><br><span class="line">    mask = out.ge(<span class="number">0.5</span>).<span class="built_in">float</span>() <span class="comment">#if out&gt;0.5,out=1,else out=0</span></span><br><span class="line">    acc = <span class="built_in">float</span>((mask == y_data).<span class="built_in">sum</span>().item()) / y_data.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment">#backward</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="keyword">if</span>(epoch+<span class="number">1</span>)%<span class="number">2000</span> ==<span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;*&#x27;</span>*<span class="number">10</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Epoch: &#123;&#125;,Loss: &#123;:.4f&#125;,Acc: &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch+<span class="number">1</span>,loss,acc))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画线w1x+w2y+b=0</span></span><br><span class="line">w0,w1 = logistic_model.lr.weight[<span class="number">0</span>]</span><br><span class="line">b = logistic_model.lr.bias.data[<span class="number">0</span>]</span><br><span class="line">plot_x = np.arange(<span class="number">30</span>,<span class="number">100</span>,<span class="number">0.1</span>)</span><br><span class="line">w0=w0.data</span><br><span class="line">w1=w1.data</span><br><span class="line">plot_y = (-w0*plot_x-b) /w1</span><br><span class="line">plt.plot(plot_x,plot_y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20201023124844.png"></p><h3 id="3-4-简单多层全连接前向网络"><a href="#3-4-简单多层全连接前向网络" class="headerlink" title="3.4 简单多层全连接前向网络"></a>3.4 简单多层全连接前向网络</h3><h4 id="3-4-1-模拟神经元"><a href="#3-4-1-模拟神经元" class="headerlink" title="3.4.1 模拟神经元"></a>3.4.1 模拟神经元</h4><p>神经网络就是受到了模拟脑神经元的启发</p><h4 id="3-4-2-单层神经网络的分类器"><a href="#3-4-2-单层神经网络的分类器" class="headerlink" title="3.4.2 单层神经网络的分类器"></a>3.4.2 单层神经网络的分类器</h4><p>例如之前的Logistic回归，是使用了sigmoid函数作为激活函数的一层神经网络</p><h4 id="3-4-3-激活函数"><a href="#3-4-3-激活函数" class="headerlink" title="3.4.3 激活函数"></a>3.4.3 激活函数</h4><p>1.Sigmoid函数</p><p>$$\sigma (x)=\frac{1}{1+e^{-x}}$$</p><p>缺点：<br>（1）造成梯度消失。在靠近0，1两端，梯度几乎为0，导致没有信息来更新参数<br>（2）输出不是以0为均值。</p><p>2.Tanh</p><p>$$tanh(x)=2\sigma(2x)-1$$</p><p>Tanh激活函数是sigmoid函数的变形，将输入的数据转化到-1到1之间，解决了sigmoid函数第二个问题，但仍存在梯度消失的问题</p><p>3.ReLU</p><p>ReLU的数学表达式为$f(x)=max(0,x)$</p><p>优点：<br>（1）相比较sigmoid和tanh，ReLU可以极大地加速随机梯度下降法的收敛速度，因为是线性的，不存在梯度消失<br>（2）计算方法更简单</p><p>缺点：<br>训练的时候很脆弱，一个很大的梯度经过ReLU激活函数，更新参数之后，会使得这个神经元不会对任何数据有激活现象，之后再经过ReLU的梯度都是0，参数无法更新。可以通过设置较小的学习率来避免这个问题</p><p>4.Leaky ReLU</p><p>ReLU的变式，为了修复ReLU脆弱的缺点，将x&lt;0的部分变成一个很小的负的斜率，但是效果时好时不好</p><p>5.Maxout</p><p>$$f(x)=max(w_1x+b_1,w_2x+b_2)$$<br>ReLU只是Maxout中w1=0，b1=0的特殊形式</p><p>优点：包含ReLU的优点，避免了ReLU的脆弱性<br>缺点：参数存储变大</p><h4 id="3-4-4-神经网络的结构"><a href="#3-4-4-神经网络的结构" class="headerlink" title="3.4.4 神经网络的结构"></a>3.4.4 神经网络的结构</h4><p>神经网络是一个由神经元组成的无环图</p><p>nn.Linear(in,out，bias=False)是全连接神经网络层的函数</p><h4 id="3-4-5-模型的表示能力与容量"><a href="#3-4-5-模型的表示能力与容量" class="headerlink" title="3.4.5 模型的表示能力与容量"></a>3.4.5 模型的表示能力与容量</h4><p>在实际中，我们可能发现一个三层的全连接神经网络比一个两层的全连接神经网络表现更好，但是更深的网络结构对全连接神经网络效果提升表现不大。<br>我们需要注意的是，增大网络的层数和每层的节点数，相当于在增大网络的容量，容量的增大意味着网络有着更大的潜在表现能力。</p><p>但是当我们在做一个二分类问题时，更复杂的模型或许有着更复杂的形状，能将测试用例完美的分类，但是却忽略了潜在的数学关系，将噪声的干扰放大，这种效果被称为过拟合</p><h3 id="3-5-深度学习的基石：反向传播算法"><a href="#3-5-深度学习的基石：反向传播算法" class="headerlink" title="3.5 深度学习的基石：反向传播算法"></a>3.5 深度学习的基石：反向传播算法</h3><h4 id="3-5-1-链式法则"><a href="#3-5-1-链式法则" class="headerlink" title="3.5.1 链式法则"></a>3.5.1 链式法则</h4><p>求导的链式法则（高数知识）</p><h4 id="3-5-2-反向传播算法"><a href="#3-5-2-反向传播算法" class="headerlink" title="3.5.2 反向传播算法"></a>3.5.2 反向传播算法</h4><p>是链式求导法则的应用</p><p>局部求导，不断迭代传播</p><h3 id="3-6-各种优化算法的变式"><a href="#3-6-各种优化算法的变式" class="headerlink" title="3.6 各种优化算法的变式"></a>3.6 各种优化算法的变式</h3><h4 id="3-6-1-梯度下降法"><a href="#3-6-1-梯度下降法" class="headerlink" title="3.6.1 梯度下降法"></a>3.6.1 梯度下降法</h4><p>梯度下降的更新公式<br>$$x^i=x^{i-1}-\eta \nabla L(x^{i-1})$$</p><h4 id="3-6-2-梯度下降法的变式"><a href="#3-6-2-梯度下降法的变式" class="headerlink" title="3.6.2 梯度下降法的变式"></a>3.6.2 梯度下降法的变式</h4><p>1.SGD<br>随机梯度下降法，每次使用一批（batch）数据进行梯度的计算，而不是全部数据的梯度</p><p>2.Momentum<br>在随机梯度下降的同时，增加动量（momentum），帮助跳出一些鞍点或局部极小值点</p><p>3.Adagrad<br>自适应学习率的方法，公式是<br>$$w^{t+1}←w^{t}-\frac{\eta}{\sqrt{\sum_{i=0}^{t}(g^i)^2}+\varepsilon }$$</p><p>学习率在不断变小，但是在某些情况下会导致学习过早停止</p><p>4.RMSprop<br>一种非常有效的自适应学习率的改进方法，公式是<br>$$cache^t=\alpha * cache^{t-1}+(1-\alpha)(g^t)^2$$<br>$$w^{t+1}←w^{t}-\frac{\eta}{\sqrt{cache^t+\varepsilon}}g^t$$<br>其中α是衰减率，能有效避免Adagrad学习率一直递减太多的问题，能够更快地收敛</p><p>5.Adam<br>一种综合型学习方法，可以看成RMSprop加上momentum的学习方法</p><h3 id="3-7-处理数据和训练模型的技巧"><a href="#3-7-处理数据和训练模型的技巧" class="headerlink" title="3.7 处理数据和训练模型的技巧"></a>3.7 处理数据和训练模型的技巧</h3><h4 id="3-7-1-数据预处理"><a href="#3-7-1-数据预处理" class="headerlink" title="3.7.1 数据预处理"></a>3.7.1 数据预处理</h4><p>1.中心化<br>变成0均值</p><p>2.标准化<br>使得每个特征维度的最大值和最小值按比例缩放到-1到1之间</p><p>3.PCA（主成分分析）<br>将数据去相关性，将其投影到一个特征空间，取一些较大的，主要的特征向量来降低数据的维度</p><p>4.白噪声<br>将数据投影到一个特征空间，然后每个维度除以特征值来标准化这些数据</p><h4 id="3-7-2-权重初始化"><a href="#3-7-2-权重初始化" class="headerlink" title="3.7.2 权重初始化"></a>3.7.2 权重初始化</h4><p>1.全0初始化<br>不应该采用这种策略</p><p>2.随机初始化<br>包括了高斯随机化，均匀随机化</p><p>3.稀疏初始化</p><p>4.初始化偏置</p><p>5.批标准化</p><h4 id="3-7-3-防止过拟合"><a href="#3-7-3-防止过拟合" class="headerlink" title="3.7.3 防止过拟合"></a>3.7.3 防止过拟合</h4><p>1.正则化<br>2.Dropout</p><h3 id="3-8-多层全连接神经网络实现MNIST手写数字分类"><a href="#3-8-多层全连接神经网络实现MNIST手写数字分类" class="headerlink" title="3.8 多层全连接神经网络实现MNIST手写数字分类"></a>3.8 多层全连接神经网络实现MNIST手写数字分类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn,optim</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets,transforms</span><br><span class="line"></span><br><span class="line"><span class="comment">#带有批标准化和激活函数的三层全连接神经网络</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Batch_Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,in_dim,n_hidden_1,n_hidden_2,out_dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Batch_Net,self).__init__()</span><br><span class="line">        self.layer1 = nn.Sequential(nn.Linear(in_dim,n_hidden_1),nn.BatchNorm1d(n_hidden_1),nn.ReLU(<span class="literal">True</span>))</span><br><span class="line">        self.layer2 = nn.Sequential(nn.Linear(n_hidden_1,n_hidden_2),nn.BatchNorm1d(n_hidden_2),nn.ReLU(<span class="literal">True</span>))</span><br><span class="line">        self.layer3 = nn.Sequential(nn.Linear(n_hidden_2,out_dim))</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        x=self.layer1(x)</span><br><span class="line">        x=self.layer2(x)</span><br><span class="line">        x=self.layer3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">learning_rate = <span class="number">1e-2</span></span><br><span class="line">num_epoch = <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#transforms.ToTensor()将图片转换成PyTorch中从处理的对象，并自动将图片标准化了，即范围0到1</span></span><br><span class="line"><span class="comment">#transforms.Normalize(均值，方差)，处理：减均值，除以方差</span></span><br><span class="line"><span class="comment">#图片为灰度图，只有一个通道，如果是三通道则为transforms.Normalize([a,b,c],[d,e,f])</span></span><br><span class="line">data_tf = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor(),transforms.Normalize([<span class="number">0.5</span>],[<span class="number">0.5</span>])]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取数据集</span></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&quot;./data&quot;</span>,train=<span class="literal">True</span>,transform=data_tf,download=<span class="literal">True</span>)</span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&quot;./data&quot;</span>,train=<span class="literal">False</span>,transform=data_tf)</span><br><span class="line"><span class="comment"># 数据迭代器，传入数据集和batch_size，通过shuffle=True来表示是否将数据打乱</span></span><br><span class="line">train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = DataLoader(test_dataset,batch_size=batch_size,shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">model = Batch_Net(<span class="number">28</span>*<span class="number">28</span>,<span class="number">300</span>,<span class="number">100</span>,<span class="number">10</span>)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    model = model.cuda()</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss() <span class="comment">#交叉熵</span></span><br><span class="line"><span class="comment"># 优化</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(),lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epoch):</span><br><span class="line">    eval_loss = <span class="number">0</span></span><br><span class="line">    eval_acc = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> train_loader:</span><br><span class="line">        img,label=data</span><br><span class="line">        img = img.view(img.size(<span class="number">0</span>),-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">            img = Variable(img).cuda()</span><br><span class="line">            label = Variable(label).cuda()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            img = Variable(img)</span><br><span class="line">            label = Variable(label)</span><br><span class="line">        out=model(img)</span><br><span class="line">        loss = criterion(out,label)</span><br><span class="line">        <span class="comment"># backward</span></span><br><span class="line">        optimizer.zero_grad() <span class="comment">#置0</span></span><br><span class="line">        loss.backward() <span class="comment">#求梯度</span></span><br><span class="line">        optimizer.step() <span class="comment">#更新所有的参数，梯度下降</span></span><br><span class="line">        <span class="comment">#acc</span></span><br><span class="line">        eval_loss +=loss*label.size(<span class="number">0</span>)</span><br><span class="line">        _,pred = torch.<span class="built_in">max</span>(out,<span class="number">1</span>)</span><br><span class="line">        num_correct = (pred == label).<span class="built_in">sum</span>()</span><br><span class="line">        eval_acc +=num_correct</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Epoch:&#123;&#125;,Loss: &#123;:.6f&#125;,Acc:&#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(epoch,eval_loss/(<span class="built_in">len</span>(train_dataset)),<span class="built_in">float</span>(eval_acc)/(<span class="built_in">len</span>(train_dataset))))</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"><span class="comment">#测试</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">eval_loss = <span class="number">0</span></span><br><span class="line">eval_acc = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">    img,label=data</span><br><span class="line">    img = img.view(img.size(<span class="number">0</span>),-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        img = Variable(img).cuda()</span><br><span class="line">        label = Variable(label).cuda()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        img = Variable(img)</span><br><span class="line">        label = Variable(label)</span><br><span class="line">    out=model(img)</span><br><span class="line">    loss = criterion(out,label)</span><br><span class="line">    eval_loss +=loss.data*label.size(<span class="number">0</span>)</span><br><span class="line">    _,pred = torch.<span class="built_in">max</span>(out,<span class="number">1</span>)</span><br><span class="line">    num_correct = (pred == label).<span class="built_in">sum</span>()</span><br><span class="line">    eval_acc +=num_correct.data</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Test Loss: &#123;:.6f&#125;,Acc:&#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(eval_loss/(<span class="built_in">len</span>(test_dataset)),<span class="built_in">float</span>(eval_acc)/(<span class="built_in">len</span>(test_dataset))))</span><br></pre></td></tr></table></figure><h2 id="第四章-卷积神经网络"><a href="#第四章-卷积神经网络" class="headerlink" title="第四章 卷积神经网络"></a>第四章 卷积神经网络</h2><p>1998年由Yann Lecun提出，2012年Alex Krizhecsky凭借它赢得了ImageNet挑战赛</p><h3 id="4-1-主要任务及起源"><a href="#4-1-主要任务及起源" class="headerlink" title="4.1 主要任务及起源"></a>4.1 主要任务及起源</h3><p>对于计算机视觉，主要用提取图像中的特征</p><h3 id="4-2-卷积神经网络的原理和结构"><a href="#4-2-卷积神经网络的原理和结构" class="headerlink" title="4.2 卷积神经网络的原理和结构"></a>4.2 卷积神经网络的原理和结构</h3><p>一，卷积神经网络的三种思想</p><p>1.局部性</p><p>对于图片而言，需要检测图片中的特征来决定图片的类别，通常情况下这些特征都不是由整张图片决定的，而是由一些局部的区域决定的</p><p>2.相同性</p><p>对不同图片，如果具有同样的特征，这些特征会出现在不同位置，但特征检测所作的操作几乎一样</p><p>3.不变性</p><p>对于一张大图片，如果进行下采样，那么图片的性质基本保持不变</p><p>二，卷积神经网络的层结构</p><p>对于全连接神经网络，其由一系列隐藏层构成，每个隐藏层由若干个神经元构成，其中每个神经元都和前一层的所有神经元相关联，但是每一层中的神经元是相互独立的。全连接神经网络在处理图片时，比如在minist数据集上，图片大小是28×28，那么每层的单个神经元的权重数目就是28×28=784，但这知识一张小图片，且只有一个通道，如果是大图片，那么就会导致参数增长特别快，所以全连接神经网络在处理图像并不是好的选择</p><p>而卷积神经网络是一个3D容量的神经元，每个神经元由三个维度排列：宽带，高度和深度。如果输入的图片是32×32×3，那么这张图片的宽度就是32，高度也是32，深度是3</p><p>卷积神经网络的主要层结构有三个：卷积层，池化层，全连接层，通过堆叠这些层结构形成了一个完整的卷积神经网络结构，其中一些层包含参数（如：卷积层，全连接层），一些层不包含参数（如：激活层，池化层）。</p><h4 id="4-2-1-卷积层"><a href="#4-2-1-卷积层" class="headerlink" title="4.2.1 卷积层"></a>4.2.1 卷积层</h4><p>卷积层是卷积神经网络的核心</p><p>1.概述</p><p>卷积神经网络的参数，是由一些可学习的滤波器集合构成，每个滤波器在空间上（宽度和高度）都比较小，但深度和输入数据的深度保持一致。在前向传播时，让每个滤波器都在输入数据的宽度和高度上滑动（卷积），然后计算整个滤波器和输入数据任意一处的内积。<br>当滤波器沿着输入数据的宽度和高度滑动时，会生成一个二维的激活图。每个卷积层上，会有一整个集合的滤波器，这样会形成多个二维的不同的激活图，将这些激活图在深度方向堆叠起来形成卷积层的输出</p><p>2.局部连接</p><p>与神经元连接的空间大小叫做神经元的感受野，其大小是一个人为设置的超参数，其实是滤波器的宽和高</p><p>3.空间排列</p><p>卷积层的输出深度是一个超参数，与使用的滤波器数量一致，并且在滑动滤波器的时候必须指定步长</p><p>4.边界填充</p><p>可以将输入数据用0在边界进行填充，用来控制输出数据在空间上的尺寸，输出的尺寸可以用一个公式来计算，$\frac{W-F+2P}{S}+1$，其中W是输入的数据大小，F表示卷积层中神经元的感受野尺寸，S表示步长，P表示边界填充0的数量</p><p>5.步长的限制</p><p>步长的选择是有所限制的。当输入尺寸W是10时，如果不使用0填充，即P=0，滤波器尺寸F=3，这样步长S=2就行不通，因为(10-3+0)/2+1=4.5，不是一个整数，说明神经元不能整齐对称地滑过输入数据体，这样的超参数是无效的</p><p>6.参数共享</p><p>输出体数据在深度切片上所有的权重都使用同一个权重向量，那么卷积层在向前传播的过程中每个深度切片都可以看成是神经元的权重对输入数据体做卷积，这也就是为什么把这些3D的权重集合称为滤波器或者卷积核</p><p>7.总结</p><p>卷积层的性质</p><ul><li>（1）输入数据体尺寸是W1×H1×D1</li><li>（2）4个超参数：卷积核数量K，卷积核空间尺寸F，滑动步长S，零填充的数量P</li><li>（3）输出数据体的尺寸为W2×H2×D2，其中$W_2=\frac{W_1-F+2P}{S}+1$,$H_2=\frac{H_1-F+2P}{S}+1$,D2=K</li><li>（4）由于参数共享，每个卷积核包含的权重数目为F×F×D1，卷积层一共有F×F×D1×K个权重和K个偏置</li><li>（5）在输出体数据中，第d个深度切片（空间尺寸是W2×H2），用第d个卷积器和输入数据进行有效卷积运算的结果，再加上第d个偏置</li></ul><p>对于卷积神经网络的一些超参数，常见的设置是F=3，S=1，P=1</p><h4 id="4-2-2-池化层"><a href="#4-2-2-池化层" class="headerlink" title="4.2.2 池化层"></a>4.2.2 池化层</h4><p>通常或者卷积层之间周期性插入一个池化层，作用是逐渐减低数据体的空间尺寸，这样能减少网络中参数的数量，减少计算资源耗费，同时也能有效地控制过拟合</p><p>步骤：设定一个空间窗口，不断滑动窗口，取这些窗口中的最大值作为输出结果</p><p>池化层之所有有效，是因为之前介绍的图片特征具有不变性，也就是通过下采样不会丢失图片拥有的特征</p><p>常用的池化层形式是尺寸为2×2的窗口，滑动步长是2，对图像进行下采样，将其中75%的激活信息都丢掉，选择其中最大的保留，池化层很少引入零填充</p><p>除最大值池化外，还有平均池化，或者L2范数池化，实际证明，最大池化效果最好，平均池化一般放在卷积神经网络最后一层</p><h4 id="4-2-3-全连接层"><a href="#4-2-3-全连接层" class="headerlink" title="4.2.3 全连接层"></a>4.2.3 全连接层</h4><p>全连接层的每个神经元与前一层所有的神经元全部连接，在这个过程中为了防止过拟合会引入<code>Dropout</code>。在进入全连接层之前，使用全局平均池化能够有效地降低过拟合</p><h4 id="4-2-4-卷积神经网络的基本形式"><a href="#4-2-4-卷积神经网络的基本形式" class="headerlink" title="4.2.4 卷积神经网络的基本形式"></a>4.2.4 卷积神经网络的基本形式</h4><p>卷积神经网络最常见的形式就是将一些卷积层和<code>ReLU</code>层放在一起，有可能在<code>ReLU</code>层前面加上批标准化层，随后紧跟池化层，再不断重复，直到图像被缩小到一个足够小的尺寸，然后将特征图展开，连接几层全连接层，最后输出结果</p><p>1.小滤波器的有效性</p><p>2.网络的尺寸</p><p>经验<br>（1）输入层：一般而言，输入层的大小应该能够被2整除很多次，常用的数字包括32，44，96，224<br>（2）卷积层：卷积层应该尽可能使用小尺寸，比如3×3或5×5，滑动步长取1。7×7通常用在第一个面对原始图像的卷积层上<br>（3）池化层：池化层负责对输入的数据空间维度进行下采样，常用的设置使用2×2的感受野做最大值池化，步长取2<br>（4）零填充：零填充的使用可以让卷积层的输入和输出在空间上的维度保持一致</p><h3 id="4-3-Pytorch卷积模块"><a href="#4-3-Pytorch卷积模块" class="headerlink" title="4.3 Pytorch卷积模块"></a>4.3 Pytorch卷积模块</h3><h4 id="4-3-1-卷积层"><a href="#4-3-1-卷积层" class="headerlink" title="4.3.1 卷积层"></a>4.3.1 卷积层</h4><p><code>nn.Conv2d(in_channels,out_channels,kernel_size,stride,padding,dilation,groups,bias)</code><br>其中</p><ul><li><code>in_channels</code>对应输入数据体的深度</li><li><code>out_channels</code>对应输出数据体的深度</li><li><code>kernel_size</code>表示滤波器（卷积核）的大小，例如：<code>kernel_size=3</code>或<code>kernel_size=(3,2)</code></li><li><code>stride</code>表示滑动步长，默认<code>1</code></li><li><code>padding=0</code>表示四周不进行零填充，<code>padding=1</code>表示四周进行<code>1</code>个像素点的零填充，默认<code>0</code></li><li><code>bias</code>是一个布尔值，默认为<code>True</code>，表示使用偏置</li><li><code>groups</code>表示输出数据体深度上的联系，默认<code>groups=1</code>，即所有的输出和输入都是相关联的，如果<code>groups=2</code>表示输入的深度被分割成两份，输出的深度也被分割成两份，他们之间分别对应起来，所以要求输出和输入都必须要能被<code>groups</code>整除</li><li><code>dilation</code>表示卷积对于输入数据体的空间间隔，默认为<code>1</code></li></ul><h4 id="4-3-2-池化层"><a href="#4-3-2-池化层" class="headerlink" title="4.3.2 池化层"></a>4.3.2 池化层</h4><p><code>nn.MaxPool2d(kernel_size,stride,padding,dilation,return_indices,ceil_model)</code><br>其中</p><ul><li><code>kernel_size</code>,<code>stride</code>,<code>padding</code>,<code>dilation</code>和卷积层相同</li><li><code>return_indices</code>表示是否返回最大值所处的下标，默认为<code>False</code></li><li><code>ceil_mode</code>表示使用一些方格代替层结构，默认<code>False</code></li></ul><p><code>nn.AvgPool2d()</code>表示均值池化，里面的参数和MaxPool2d类似，但多一个参数<code>count_include_pad</code>表示计算均值的时候是否包含零填充，默认为<code>True</code></p><p>其他还有<code>nn.LPPool2d()</code>,<code>nn.AdaptiveMaxPool2d()</code></p><p><strong>下面是一个简单的多层卷积神经网络</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleCNN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SimpleCNN,self).__init__()</span><br><span class="line">        layer1 = nn.Sequential()</span><br><span class="line">        layer1.add_module(<span class="string">&#x27;conv1&#x27;</span>,nn.Conv2d(<span class="number">3</span>,<span class="number">32</span>,<span class="number">3</span>,<span class="number">1</span>,padding=<span class="number">1</span>))</span><br><span class="line">        layer1.add_module(<span class="string">&#x27;relu1&#x27;</span>,nn.ReLU(<span class="literal">True</span>))</span><br><span class="line">        layer1.add_module(<span class="string">&#x27;pool1&#x27;</span>,nn.MaxPool2d(<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">        self.layer1=layer1</span><br><span class="line"></span><br><span class="line">        layer2 = nn.Sequential()</span><br><span class="line">        layer2.add_module(<span class="string">&#x27;conv2&#x27;</span>,nn.Conv2d(<span class="number">32</span>,<span class="number">64</span>,<span class="number">3</span>,<span class="number">1</span>,padding=<span class="number">1</span>))</span><br><span class="line">        layer2.add_module(<span class="string">&#x27;relu2&#x27;</span>,nn.ReLU(<span class="literal">True</span>))</span><br><span class="line">        layer2.add_module(<span class="string">&#x27;pool2&#x27;</span>,nn.MaxPool2d(<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">        self.layer2=layer2</span><br><span class="line">        </span><br><span class="line">        layer3 = nn.Sequential()</span><br><span class="line">        layer3.add_module(<span class="string">&#x27;conv3&#x27;</span>,nn.Conv2d(<span class="number">64</span>,<span class="number">128</span>,<span class="number">3</span>,<span class="number">1</span>,padding=<span class="number">1</span>))</span><br><span class="line">        layer3.add_module(<span class="string">&#x27;relu3&#x27;</span>,nn.ReLU(<span class="literal">True</span>))</span><br><span class="line">        layer3.add_module(<span class="string">&#x27;pool3&#x27;</span>,nn.MaxPool2d(<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">        self.layer3=layer3</span><br><span class="line"></span><br><span class="line">        layer4 = nn.Sequential()</span><br><span class="line">        layer4.add_module(<span class="string">&#x27;fc1&#x27;</span>,nn.Linear(<span class="number">2048</span>,<span class="number">512</span>))</span><br><span class="line">        layer4.add_module(<span class="string">&#x27;fc_relu1&#x27;</span>,nn.ReLU(<span class="literal">True</span>))</span><br><span class="line">        layer4.add_module(<span class="string">&#x27;fc2&#x27;</span>,nn.Linear(<span class="number">512</span>,<span class="number">64</span>))</span><br><span class="line">        layer4.add_module(<span class="string">&#x27;fc_relu2&#x27;</span>,nn.ReLU(<span class="literal">True</span>))</span><br><span class="line">        layer4.add_module(<span class="string">&#x27;fc3&#x27;</span>,nn.Linear(<span class="number">64</span>,<span class="number">10</span>))</span><br><span class="line">        self.layer4=layer4</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        conv1 = self.layer1(x)</span><br><span class="line">        conv2 = self.layer2(conv1)</span><br><span class="line">        conv3 = self.layer3(conv2)</span><br><span class="line">        fc_input = conv3.view(conv3.size(<span class="number">0</span>),-<span class="number">1</span>)</span><br><span class="line">        fc_out = slef.layer4(fc_input)</span><br><span class="line">        <span class="keyword">return</span> fc_out</span><br><span class="line"></span><br><span class="line">model = SimpleCNN()</span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure><h4 id="4-3-3-提取层结构"><a href="#4-3-3-提取层结构" class="headerlink" title="4.3.3 提取层结构"></a>4.3.3 提取层结构</h4><p>nn.Module具有几个重要属性</p><ul><li><code>children()</code>，会返回下一级模块的迭代器，比如上面这个模型，直会返回在<code>self.layer1</code>,<code>slef.layer2</code>,<code>slef.layer3</code>以及<code>self.layer4</code>上的迭代器，不会返回他们内部的东西</li><li><code>modules()</code>，会返回模型中所有模块的迭代器，这样就有了一个好处，即它能够访问到最内层，比如<code>self.layer1.conv1</code>这个模块</li><li><code>named_children()</code>和<code>named_modules()</code>不仅会返回模块的迭代器，还会返回网络层的名字</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#提取前面两层</span></span><br><span class="line"><span class="built_in">print</span>(nn.Sequential(*<span class="built_in">list</span>(model.children())[:<span class="number">2</span>]))</span><br></pre></td></tr></table></figure><p><strong>提取所有的卷积层</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">conv_model = nn.Sequential()</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> model.named_modules():</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(layer[<span class="number">1</span>],nn.Conv2d):</span><br><span class="line">        conv_model.add_module(layer[<span class="number">0</span>].split(<span class="string">&#x27;.&#x27;</span>)[-<span class="number">1</span>],layer[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(conv_model)</span><br></pre></td></tr></table></figure><h4 id="4-3-4-提取参数及自定义初始化"><a href="#4-3-4-提取参数及自定义初始化" class="headerlink" title="4.3.4 提取参数及自定义初始化"></a>4.3.4 提取参数及自定义初始化</h4><p><code>nn.Module</code>关于参数的属性</p><ul><li><code>named_parameters()</code>，给出网络层的名字和参数的迭代器</li><li><code>parameters()</code>，给出一个网络的全部参数的迭代器</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(param[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><p><strong>对权重初始化</strong>，因为权重是Variable，只需要取出data属性就能处理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> m <span class="keyword">in</span> model.modules():</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(m,nn.Conv2d):</span><br><span class="line">        nn.init.normal(m.weight.data)</span><br><span class="line">        nn.init.xavier_normal(m.weight.data)</span><br><span class="line">        nn.init.kaiming_normal(m.weight.data)<span class="comment">#卷积层参数初始化</span></span><br><span class="line">        m.bias.data.fill_(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m,nn.Linear):</span><br><span class="line">        m.weight.data.normal_()<span class="comment">#全连接层参数初始化</span></span><br></pre></td></tr></table></figure><h3 id="4-4-卷积神经网络案例分析"><a href="#4-4-卷积神经网络案例分析" class="headerlink" title="4.4 卷积神经网络案例分析"></a>4.4 卷积神经网络案例分析</h3><h4 id="4-4-1-LeNet"><a href="#4-4-1-LeNet" class="headerlink" title="4.4.1 LeNet"></a>4.4.1 LeNet</h4><p>LeNet是整个卷积神经网络的开山之作，共有7层，其中2层卷积和2层池化层交替出现，最后输出3层全连接层得到整体的效果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Lenet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Lenet,self).__init__()</span><br><span class="line">        layer1 = nn.Sequential()</span><br><span class="line">        layer1.add_module(<span class="string">&#x27;conv1&#x27;</span>,nn.Conv2d(<span class="number">1</span>,<span class="number">6</span>,<span class="number">3</span>,padding=<span class="number">1</span>))</span><br><span class="line">        layer1.add_module(<span class="string">&#x27;pool1&#x27;</span>,nn.MaxPool2d(<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">        self.layer1 = layer1</span><br><span class="line">        </span><br><span class="line">        layer2 = nn.Sequential()</span><br><span class="line">        layer2.add_module(<span class="string">&#x27;conv2&#x27;</span>,nn.Conv2d(<span class="number">6</span>,<span class="number">16</span>,<span class="number">5</span>))</span><br><span class="line">        layer2.add_module(<span class="string">&#x27;pool2&#x27;</span>,nn.MaxPool2d(<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">        self.layer2 = layer2</span><br><span class="line">        </span><br><span class="line">        layer3 = nn.Sequential()</span><br><span class="line">        layer3.add_module(<span class="string">&#x27;fc1&#x27;</span>,nn.Linear(<span class="number">400</span>,<span class="number">120</span>))</span><br><span class="line">        layer3.add_module(<span class="string">&#x27;fc2&#x27;</span>,nn.Linear(<span class="number">120</span>,<span class="number">84</span>))</span><br><span class="line">        layer3.add_module(<span class="string">&#x27;fc3&#x27;</span>,nn.Linear(<span class="number">84</span>,<span class="number">10</span>))</span><br><span class="line">        self.layer3 = layer3</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = self.layer2(x)</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>),-<span class="number">1</span>) <span class="comment"># 将第二次卷积的输出拉伸为一行</span></span><br><span class="line">        x = self.layer3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h4 id="4-4-2-AlexNet"><a href="#4-4-2-AlexNet" class="headerlink" title="4.4.2 AlexNet"></a>4.4.2 AlexNet</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AlexNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,num_classes</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(AlexNet,self).__init__()</span><br><span class="line">        self.features = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>,<span class="number">64</span>,kernel_size=<span class="number">11</span>,stride=<span class="number">4</span>,padding=<span class="number">2</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">64</span>,<span class="number">192</span>,kernel_size=<span class="number">5</span>,padding=<span class="number">2</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">192</span>,<span class="number">384</span>,kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">384</span>,<span class="number">256</span>,kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">256</span>,<span class="number">256</span>,kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">3</span>,stride=<span class="number">2</span>),</span><br><span class="line">        )</span><br><span class="line">        self.classifier = nn.Sequential(</span><br><span class="line">            nn.Dropout(),</span><br><span class="line">            nn.Linear(<span class="number">256</span>*<span class="number">6</span>*<span class="number">6</span>,<span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Dropout(),</span><br><span class="line">            nn.Linear(<span class="number">4096</span>,<span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(<span class="number">4096</span>,num_classes),</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        x = self.features(x)</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>),<span class="number">256</span>*<span class="number">6</span>*<span class="number">6</span>)</span><br><span class="line">        x = self.classifier(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h4 id="4-4-3-VGGNet"><a href="#4-4-3-VGGNet" class="headerlink" title="4.4.3 VGGNet"></a>4.4.3 VGGNet</h4><p>使用了更小的滤波器，同时使用了更深的结构</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VGG</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,num_classes</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(VGG,self).__init__()</span><br><span class="line">        self.features = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>,<span class="number">64</span>,kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">64</span>,<span class="number">64</span>,kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>,stride=<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">64</span>,<span class="number">128</span>,kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">128</span>,<span class="number">128</span>,kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>,stride=<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">128</span>,<span class="number">256</span>,kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">256</span>,<span class="number">256</span>,kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">256</span>,<span class="number">256</span>,kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>,stride=<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">256</span>,<span class="number">512</span>,kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">512</span>,<span class="number">512</span>,kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">512</span>,<span class="number">512</span>,kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>,stride=<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">512</span>,<span class="number">512</span>,kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">512</span>,<span class="number">512</span>,kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">512</span>,<span class="number">512</span>,kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>,stride=<span class="number">2</span>),</span><br><span class="line">        )</span><br><span class="line">        self.classifier = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">512</span>*<span class="number">7</span>*<span class="number">7</span>,<span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Dropout(),</span><br><span class="line">            nn.Linear(<span class="number">4096</span>,<span class="number">4096</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Dropout(),</span><br><span class="line">            nn.Linear(<span class="number">4096</span>,num_classes),</span><br><span class="line">        )</span><br><span class="line">        self._initialize_weights()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        x = self.features(x)</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>),-<span class="number">1</span>)</span><br><span class="line">        x = self.classifier(x)</span><br></pre></td></tr></table></figure><h4 id="4-4-4-GoogleNet"><a href="#4-4-4-GoogleNet" class="headerlink" title="4.4.4 GoogleNet"></a>4.4.4 GoogleNet</h4><p>也叫InceptionNet，采用了比VGG更深的网络结构，一共22层，但是参数却比AlexNet少了12倍，同时有很高的计算效率，因为它采用了一种很有效的Inception模块，而且没有全连接层。</p><p><strong>Inception模块</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicConv2d</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,in_channels,out_channels,**kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(BasicConv2d,self).__init__()</span><br><span class="line">        self.conv = nn.Conv2d(in_channels,out_channels,bias=<span class="literal">False</span>,**kwargs)</span><br><span class="line">        self.bn = nn.BatchNorm2d(out_channels,eps=<span class="number">0.001</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        x = self.bn(x)</span><br><span class="line">        <span class="keyword">return</span> F.relu(x,inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Inception</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,in_channels,pool_features</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Inception,self).__init__()</span><br><span class="line">        self.branch1x1 = BasicConv2d(in_channels,<span class="number">64</span>,kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch5x5_1 = BasicConv2d(in_channels,<span class="number">48</span>,kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch5x5_2 = BasicConv2d(<span class="number">48</span>,<span class="number">64</span>,kernel_size=<span class="number">5</span>,padding=<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        self.branch3x3db1_1 = BasicConv2d(in_channels,<span class="number">64</span>,kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3db1_2 = BasicConv2d(<span class="number">64</span>,<span class="number">96</span>,kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">        self.branch3x3db1_3 = BasicConv2d(<span class="number">96</span>,<span class="number">96</span>,kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        self.branch_pool = BasicConv2d(in_channels,pool_features,kernel_size=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        branch1x1 = self.branch1x1(x)</span><br><span class="line">        </span><br><span class="line">        branch5x5 = self.branch5x5_1(x)</span><br><span class="line">        branch5x5 = self.branch5x5_2(branch5x5)</span><br><span class="line">        </span><br><span class="line">        branch3x3db1 = self.branch3x3db1_1(x)</span><br><span class="line">        branch3x3db1 = self.branch3x3db1_2(branch3x3db1)</span><br><span class="line">        branch3x3db1 = self.branch3x3db1_3(branch3x3db1)</span><br><span class="line">        </span><br><span class="line">        branch_pool = F.avg_pool2d(x,kernel_size=<span class="number">3</span>,stride=<span class="number">1</span>,padding=<span class="number">1</span>)</span><br><span class="line">        branch_pool = self.branch_pool(branch_pool)</span><br><span class="line">        </span><br><span class="line">        outputs = [branch1x1,branch5x5,branch3x3db1,branch_pool]</span><br><span class="line">        <span class="keyword">return</span> torch.cat(outputs,<span class="number">1</span>) <span class="comment">#按深度拼接</span></span><br></pre></td></tr></table></figure><h4 id="4-4-5-ResNet"><a href="#4-4-5-ResNet" class="headerlink" title="4.4.5 ResNet"></a>4.4.5 ResNet</h4><p>由微软研究院提出，通过残差模块能够成功地训练高达152层深的神经网络</p><p>ResNet 最初的设计灵感来自这个问题:在不断加深神经网络的时候，会出现一个Degradation ，即准确率会先上升然后达到饱和，再持续增加深度则会导致模型准确率下降。</p><p>这并不是过拟合的问题，因为不仅在验证集上误差增加，训练集本身误差也会增加，假设一个比较浅的网络达到了饱和的准确率，那么在后面加上几个恒等映射层，误差不会增加，也就说更深的模型起码不会使得模型效果下降。</p><p>这里提到的使用恒等映射直接将前一层输出传到后面的思想，就是 ResNet 的灵感来源。假设某个神经网络的输入是x， 期望输出是 H(x)，如果直接把输入x传到输出作为初始结果，那么此时需要学习的目标就是 F(x) = H (x) - x<br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN/post/20201027143439.png"><br>左边是一个普通的网络，右边是一个 ResNet 的残差学习 单元， ResNet 相当于将学习目 标改变了.不再是学习一个完整的输出H ( x ) ， 而是学习输出和输入的差别H (x) - x，即残差。</p><p>除了这些比较出名的以外还有很多。并且并不需要重复造轮子，PyTorch内为我们实现了以上网络，都在<code>torchvision.model</code>里面，并且大部分网络都有预训练好的参数</p><h3 id="4-5-再实现MNIST手写数字分类"><a href="#4-5-再实现MNIST手写数字分类" class="headerlink" title="4.5 再实现MNIST手写数字分类"></a>4.5 再实现MNIST手写数字分类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn,optim</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets,transforms</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(CNN,self).__init__()</span><br><span class="line">        self.layer1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>,<span class="number">16</span>,kernel_size=<span class="number">3</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">16</span>),<span class="comment"># 归一化处理，使得数据分布一致，避免梯度消失或梯度爆炸</span></span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        self.layer2 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">16</span>,<span class="number">32</span>,kernel_size=<span class="number">3</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">32</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>,stride=<span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.layer3 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>,<span class="number">64</span>,kernel_size=<span class="number">3</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.layer4 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">64</span>,<span class="number">128</span>,kernel_size=<span class="number">3</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">128</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(kernel_size=<span class="number">2</span>,stride=<span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">128</span>*<span class="number">4</span>*<span class="number">4</span>,<span class="number">1024</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>,<span class="number">128</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(<span class="number">128</span>,<span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        x  = self.layer1(x)</span><br><span class="line">        x  = self.layer2(x)</span><br><span class="line">        x  = self.layer3(x)</span><br><span class="line">        x  = self.layer4(x)</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>),-<span class="number">1</span>)</span><br><span class="line">        x  = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">learning_rate = <span class="number">1e-2</span></span><br><span class="line">num_epoch = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">data_tf = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor(),transforms.Normalize([<span class="number">0.5</span>],[<span class="number">0.5</span>])]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取数据集</span></span><br><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&quot;./data&quot;</span>,train=<span class="literal">True</span>,transform=data_tf,download=<span class="literal">True</span>)</span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&quot;./data&quot;</span>,train=<span class="literal">False</span>,transform=data_tf)</span><br><span class="line"><span class="comment"># 数据迭代器，传入数据集和batch_size，通过shuffle=True来表示是否将数据打乱</span></span><br><span class="line">train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = DataLoader(test_dataset,batch_size=batch_size,shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">model = CNN()</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    model = model.cuda()</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(),lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epoch):</span><br><span class="line">    eval_loss = <span class="number">0.0</span></span><br><span class="line">    eval_acc = <span class="number">0.0</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Epoch &#123;&#125;/&#123;&#125;&quot;</span>.<span class="built_in">format</span>(epoch,num_epoch))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;-&quot;</span>*<span class="number">20</span>)</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> train_loader:</span><br><span class="line">        img,label=data</span><br><span class="line">        <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">            img = Variable(img).cuda()</span><br><span class="line">            label = Variable(label).cuda()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            img = Variable(img)</span><br><span class="line">            label = Variable(label)</span><br><span class="line">        out=model(img)</span><br><span class="line">        loss = criterion(out,label)</span><br><span class="line">        <span class="comment"># backward</span></span><br><span class="line">        optimizer.zero_grad() <span class="comment">#置0</span></span><br><span class="line">        loss.backward() <span class="comment">#求梯度</span></span><br><span class="line">        optimizer.step() <span class="comment">#更新所有的参数，梯度下降</span></span><br><span class="line">        <span class="comment">#acc</span></span><br><span class="line">        eval_loss += loss.data</span><br><span class="line">        _,pred = torch.<span class="built_in">max</span>(out,<span class="number">1</span>)</span><br><span class="line">        eval_acc += (pred == label).<span class="built_in">sum</span>()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Epoch:&#123;&#125;,Loss: &#123;:.4f&#125;,Acc:&#123;:.4f&#125;%&#x27;</span>.<span class="built_in">format</span>(epoch,eval_loss/(<span class="built_in">len</span>(train_dataset)),<span class="number">100</span>*<span class="built_in">float</span>(eval_acc)/(<span class="built_in">len</span>(train_dataset))))</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">PATH=<span class="string">&#x27;./minist_net.pth&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Train finished!&quot;</span>)</span><br><span class="line">torch.save(model.state_dict(), PATH)</span><br></pre></td></tr></table></figure><h3 id="4-6-图像增强的方法"><a href="#4-6-图像增强的方法" class="headerlink" title="4.6 图像增强的方法"></a>4.6 图像增强的方法</h3><p>torchvision.transforms包括所有图像增强的方法</p><ul><li>Scale，对图片的尺寸进行缩小和放大</li><li>CenterCrop，对图像正中心进行给定大小的随机裁剪</li><li>RandomCrop，对图片进行给定大小的随机裁剪</li><li>RandomHorizaontalFlip，对图像进行概率为0.5的随机水平翻转</li><li>RandomSizedCrop，首先对图片进行随机尺寸的裁剪，然后对裁剪图片进行一个随即比例的缩放，最后将图片变成给定的大小</li><li>Pad，对图片进行边界零填充</li></ul><p>除此之外，还可以使用OpenCV或者PIL等第三方图形库来实现</p><h3 id="4-7-实现cifar10分类"><a href="#4-7-实现cifar10分类" class="headerlink" title="4.7 实现cifar10分类"></a>4.7 实现cifar10分类</h3><p>cifar10数据集中有60000张图片，每张图片的大小都是32×32的三通道彩色图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="comment">#数据处理</span></span><br><span class="line">train_transform = transforms.Compose([</span><br><span class="line">    transforms.Scale(<span class="number">40</span>),</span><br><span class="line">    transforms.RandomHorizontalFlip(),</span><br><span class="line">    transforms.RandomCrop(<span class="number">32</span>),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize([<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>], [<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>])</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">test_transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize([<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>], [<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>])</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment">#数据集获取</span></span><br><span class="line">train_set = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=train_transform)</span><br><span class="line">train_data = torch.utils.data.DataLoader(train_set, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=test_transform)</span><br><span class="line">test_data = torch.utils.data.DataLoader(test_set, batch_size=<span class="number">32</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">classes = (<span class="string">&#x27;plane&#x27;</span>, <span class="string">&#x27;car&#x27;</span>, <span class="string">&#x27;bird&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>,</span><br><span class="line">           <span class="string">&#x27;deer&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;frog&#x27;</span>, <span class="string">&#x27;horse&#x27;</span>, <span class="string">&#x27;ship&#x27;</span>, <span class="string">&#x27;truck&#x27;</span>)</span><br><span class="line"><span class="comment">#3×3卷积层</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv3x3</span>(<span class="params">in_channel, out_channel, stride=<span class="number">1</span></span>):</span></span><br><span class="line">    <span class="keyword">return</span> nn.Conv2d(in_channel, out_channel, <span class="number">3</span>, stride=stride, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">residual_block</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channel, out_channel, same_shape=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(residual_block, self).__init__()</span><br><span class="line">        self.same_shape = same_shape</span><br><span class="line">        stride = <span class="number">1</span> <span class="keyword">if</span> self.same_shape <span class="keyword">else</span> <span class="number">2</span></span><br><span class="line">          </span><br><span class="line">        self.conv1 = conv3x3(in_channel, out_channel, stride=stride)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(out_channel)</span><br><span class="line">          </span><br><span class="line">        self.conv2 = conv3x3(out_channel, out_channel)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(out_channel)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.same_shape:</span><br><span class="line">            self.conv3 = nn.Conv2d(in_channel, out_channel, <span class="number">1</span>, stride=stride)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = F.relu(self.bn1(out), <span class="literal">True</span>)</span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = F.relu(self.bn2(out), <span class="literal">True</span>)</span><br><span class="line">          </span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.same_shape:</span><br><span class="line">            x = self.conv3(x)</span><br><span class="line">        <span class="keyword">return</span> F.relu(x+out, <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">resnet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channel, num_classes</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(resnet, self).__init__()</span><br><span class="line">        self.block1 = nn.Conv2d(in_channel, <span class="number">64</span>, <span class="number">7</span>, <span class="number">2</span>,<span class="number">3</span>) <span class="comment"># 32-7+2*3/2+1=16</span></span><br><span class="line">        self.block2 = nn.Sequential(</span><br><span class="line">            nn.MaxPool2d(<span class="number">3</span>, <span class="number">1</span>),</span><br><span class="line">            residual_block(<span class="number">64</span>, <span class="number">64</span>),</span><br><span class="line">            residual_block(<span class="number">64</span>, <span class="number">64</span>)</span><br><span class="line">        )</span><br><span class="line">        self.block3 = nn.Sequential(</span><br><span class="line">            residual_block(<span class="number">64</span>, <span class="number">128</span>, <span class="literal">False</span>),</span><br><span class="line">            residual_block(<span class="number">128</span>, <span class="number">128</span>)</span><br><span class="line">        )</span><br><span class="line">        self.block4 = nn.Sequential(</span><br><span class="line">            residual_block(<span class="number">128</span>, <span class="number">256</span>, <span class="literal">False</span>),</span><br><span class="line">            residual_block(<span class="number">256</span>, <span class="number">256</span>)</span><br><span class="line">        )</span><br><span class="line">        self.block5 = nn.Sequential(</span><br><span class="line">            residual_block(<span class="number">256</span>, <span class="number">512</span>, <span class="literal">False</span>),</span><br><span class="line">            residual_block(<span class="number">512</span>, <span class="number">512</span>)</span><br><span class="line">        )</span><br><span class="line">        self.avg_pool = nn.AvgPool2d(<span class="number">2</span>)</span><br><span class="line">        self.classifier = nn.Linear(<span class="number">512</span>, num_classes)</span><br><span class="line">          </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.block1(x)</span><br><span class="line">        <span class="comment">#print(x.shape)</span></span><br><span class="line">        x = self.block2(x)</span><br><span class="line">        <span class="comment">#print(x.shape)</span></span><br><span class="line">        x = self.block3(x)</span><br><span class="line">        <span class="comment">#print(x.shape)</span></span><br><span class="line">        x = self.block4(x)</span><br><span class="line">        <span class="comment">#print(x.shape)</span></span><br><span class="line">        x = self.block5(x)</span><br><span class="line">        <span class="comment">#print(x.shape)</span></span><br><span class="line">        x = self.avg_pool(x)</span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        x = self.classifier(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">PATH = <span class="string">&#x27;./cifar_net.pth&#x27;</span></span><br><span class="line">net = resnet(<span class="number">3</span>, <span class="number">10</span>)</span><br><span class="line"><span class="comment">#if os.path.exists(PATH):</span></span><br><span class="line"><span class="comment">#    net.load_state_dict(torch.load(PATH))</span></span><br><span class="line">criterion = nn.CrossEntropyLoss() <span class="comment">#交叉熵</span></span><br><span class="line">optimizer = optim.Adam(net.parameters(), lr=<span class="number">0.01</span>) </span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="comment">#计算正确率</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_acc</span>(<span class="params">output, label</span>):</span></span><br><span class="line">    total = output.shape[<span class="number">0</span>]</span><br><span class="line">    _, pred_label = output.<span class="built_in">max</span>(<span class="number">1</span>)</span><br><span class="line">    num_correct = (pred_label == label).<span class="built_in">sum</span>().data</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(num_correct) / total</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">net, train_data, valid_data, num_epochs, optimizer, criterion</span>):</span></span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        net = net.cuda()</span><br><span class="line">    <span class="comment">#计时</span></span><br><span class="line">    prev_time = datetime.now()</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;*&quot;</span>*<span class="number">10</span>)</span><br><span class="line">        train_loss = <span class="number">0.0</span></span><br><span class="line">        train_acc = <span class="number">0.0</span></span><br><span class="line">        net = net.train() <span class="comment">#训练模式</span></span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> train_data:</span><br><span class="line">            im,label = data</span><br><span class="line">            <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">                im = Variable(im.cuda())</span><br><span class="line">                label = Variable(label.cuda())</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                im = Variable(im)</span><br><span class="line">                label = Variable(label)</span><br><span class="line">            <span class="comment">#forward</span></span><br><span class="line">            output = net(im)</span><br><span class="line">            loss = criterion(output, label)</span><br><span class="line">            <span class="comment">#forward</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">               </span><br><span class="line">            train_loss += loss.data</span><br><span class="line">            train_acc += get_acc(output, label)</span><br><span class="line">        <span class="comment">#计时</span></span><br><span class="line">        cur_time = datetime.now()</span><br><span class="line">        h, remainder = <span class="built_in">divmod</span>((cur_time-prev_time).seconds, <span class="number">3600</span>)</span><br><span class="line">        m, s = <span class="built_in">divmod</span>(remainder, <span class="number">60</span>)</span><br><span class="line">        time_str = <span class="string">&quot;Time %02d:%02d:%02d&quot;</span> % (h, m, s)</span><br><span class="line">        <span class="comment">#测试</span></span><br><span class="line">        <span class="keyword">if</span> valid_data <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            valid_loss = <span class="number">0.0</span></span><br><span class="line">            valid_acc = <span class="number">0.0</span></span><br><span class="line">            net = net.<span class="built_in">eval</span>() <span class="comment"># 切换测试模式</span></span><br><span class="line">            <span class="keyword">for</span> data <span class="keyword">in</span> valid_data:</span><br><span class="line">                im, label = data</span><br><span class="line">                <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">                    im = Variable(im.cuda())</span><br><span class="line">                    label = Variable(label.cuda())</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    im = Variable(im)</span><br><span class="line">                    label = Variable(label)</span><br><span class="line">                output = net(im)</span><br><span class="line">                loss = criterion(output, label)</span><br><span class="line">                valid_loss += loss.item()</span><br><span class="line">                valid_acc += get_acc(output, label)</span><br><span class="line">            epoch_str = (</span><br><span class="line">                <span class="string">&quot;Epoch %d. Train Loss: %f, Train Acc: %f, Valid Loss: %f, Valid Acc: %f, &quot;</span></span><br><span class="line">                % (epoch, train_loss / <span class="built_in">len</span>(train_data),</span><br><span class="line">                   train_acc / <span class="built_in">len</span>(train_data), valid_loss / <span class="built_in">len</span>(valid_data),</span><br><span class="line">                   valid_acc / <span class="built_in">len</span>(valid_data)))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            epoch_str = (<span class="string">&quot;Epoch %d. Train Loss: %f, Train Acc: %f, &quot;</span> %</span><br><span class="line">                         (epoch, train_loss / <span class="built_in">len</span>(train_data),</span><br><span class="line">                          train_acc / <span class="built_in">len</span>(train_data)))</span><br><span class="line">               </span><br><span class="line">        prev_time = cur_time</span><br><span class="line">        <span class="built_in">print</span>(epoch_str + time_str)</span><br><span class="line"></span><br><span class="line">train(net, train_data, test_data, <span class="number">10</span>, optimizer, criterion) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Finished Training&#x27;</span>)</span><br><span class="line"></span><br><span class="line">torch.save(net.state_dict(), PATH)</span><br></pre></td></tr></table></figure><p><strong>测试</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=test_transform)</span><br><span class="line">test_data = torch.utils.data.DataLoader(test_set, batch_size=<span class="number">4</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出图像的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imshow</span>(<span class="params">img</span>):</span></span><br><span class="line">    img = img / <span class="number">2</span> + <span class="number">0.5</span>     <span class="comment"># unnormalize</span></span><br><span class="line">    npimg = img.numpy()</span><br><span class="line">    plt.imshow(np.transpose(npimg, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">dataiter = <span class="built_in">iter</span>(test_data)</span><br><span class="line">images, labels = dataiter.<span class="built_in">next</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment">#print(images.shape)</span></span><br><span class="line"><span class="comment"># 输出图片</span></span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;GroundTruth: &#x27;</span>, <span class="string">&#x27; &#x27;</span>.join(<span class="string">&#x27;%5s&#x27;</span> % classes[labels[j]] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)))</span><br><span class="line"></span><br><span class="line">PATH = <span class="string">&#x27;./cifar_net.pth&#x27;</span></span><br><span class="line">net.load_state_dict(torch.load(PATH))</span><br><span class="line"></span><br><span class="line">outputs = net(images)</span><br><span class="line"></span><br><span class="line">_, predicted = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Predicted: &#x27;</span>, <span class="string">&#x27; &#x27;</span>.join(<span class="string">&#x27;%5s&#x27;</span> % classes[predicted[j]] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)))</span><br><span class="line"></span><br><span class="line">class_correct = <span class="built_in">list</span>(<span class="number">0.</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>))</span><br><span class="line">class_total = <span class="built_in">list</span>(<span class="number">0.</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>))</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> test_data:</span><br><span class="line">        images, labels = data</span><br><span class="line">        outputs = net(images)</span><br><span class="line">        _, predicted = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line">        c = (predicted == labels).squeeze()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">            label = labels[i]</span><br><span class="line">            class_correct[label] += c[i].item()</span><br><span class="line">            class_total[label] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Accuracy of %5s : %2d %%&#x27;</span> % (</span><br><span class="line">        classes[i], <span class="number">100</span> * class_correct[i] / class_total[i]))</span><br></pre></td></tr></table></figure><h2 id="第五章-循环神经网络"><a href="#第五章-循环神经网络" class="headerlink" title="第五章 循环神经网络"></a>第五章 循环神经网络</h2><p>RNN，在序列问题和自然语言处理等领域取得很大的成功</p><h3 id="5-1-循环神经网络"><a href="#5-1-循环神经网络" class="headerlink" title="5.1 循环神经网络"></a>5.1 循环神经网络</h3><p>卷积神经网络相当于人类的视觉，但是它没有记忆能力，所以它只能处理一种特定的视觉任务，没办法根据以前的记忆来处理新的任务。</p><p>循环神经网络的提出便是居于记忆模型的想法，期望网络能够记住前面出现的特征，并依据特征推断后面的结果，而且整体的网络结构不断循环，因而得名循环神经网络</p><p>比如：某一个单词的意思会因为上文提到的内容不同而有不同的含义，RNN可以很好的解决这类问题</p><h4 id="5-1-1-问题介绍"><a href="#5-1-1-问题介绍" class="headerlink" title="5.1.1 问题介绍"></a>5.1.1 问题介绍</h4><p>对于下面两句话</p><ul><li>arrive beijing on November 2nd</li><li>leave beijing on November 2nd</li></ul><p>第一句话表达到达，第二句话表示离开，如果网络能构记忆“beijing”前面的词，就会预测出不同的结果。</p><h4 id="5-1-2-循环神经网络的基本结构"><a href="#5-1-2-循环神经网络的基本结构" class="headerlink" title="5.1.2 循环神经网络的基本结构"></a>5.1.2 循环神经网络的基本结构</h4><p>将网络的输出保存在一个记忆单元中，这个记忆单元和下一次的输入一起进入神经网络中。因此，输入序列（sequences）的顺序改变，会改变网络的输出结果。</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN/post/v2-206db7ba9d32a80ff56b6cc988a62440_r.jpg"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN/post/v2-b0175ebd3419f9a11a3d0d8b00e28675_r.jpg"></p><p>这个网络在t时刻接收到输入$x_t$之后，隐藏层的值是$S_t$，输出值是$O_t$。关键一点是，$S_t$的值不仅仅取决于$x_t$，还取决于$S_{t-1}$。我们可以用下面的公式来表示循环神经网络的计算方法：</p><p>$$O _ t = g(VS_t)$$<br>$$S _ t = f(UX_t+WS_{t-1})$$</p><h4 id="5-1-3-存在的问题"><a href="#5-1-3-存在的问题" class="headerlink" title="5.1.3 存在的问题"></a>5.1.3 存在的问题</h4><p>循环神经网络具有很好的记忆特性，能够将记忆内容应用到当前情景下，但是记忆最大的问题在于遗忘性</p><h3 id="5-2-循环神经网络的变式：LSTM和GRU"><a href="#5-2-循环神经网络的变式：LSTM和GRU" class="headerlink" title="5.2 循环神经网络的变式：LSTM和GRU"></a>5.2 循环神经网络的变式：LSTM和GRU</h3><h4 id="5-2-1-LSTM"><a href="#5-2-1-LSTM" class="headerlink" title="5.2.1 LSTM"></a>5.2.1 LSTM</h4><p>LSTM是Long Short Term Memory Networks的缩写，是一种链式循环的网络结构，在网络内部有着更复杂的结构，主要为了解决长序列训练过程中的梯度下降和梯度爆炸问题。</p><p>LSTM由三个门来控制，分别是输入门，遗忘门和输出门。顾名思义，输入门控制着网络的输入，遗忘门控制着记忆单元，输出门控制着网络的输出。这其中最重要的就是遗忘门，遗忘门的作用是决定之前的哪些记忆及那个被保留，那些记忆将被去掉，正是由于遗忘门的作用，使得LSTM具有了长时记忆的功能</p><h4 id="5-2-2-GRU"><a href="#5-2-2-GRU" class="headerlink" title="5.2.2 GRU"></a>5.2.2 GRU</h4><p>GRU是Gated Recurrent Unit的缩写，由Cho于2014年提出，GRU和LSTM最大的不同在于GRU将遗忘门和输入门合成了一个“更新门”，同时网络不再额外给出记忆状态Ct，而是将输出结果ht作为记忆状态不断向后循环传递，网络的输出和出入变得简单</p><h4 id="5-2-3-收敛性问题"><a href="#5-2-3-收敛性问题" class="headerlink" title="5.2.3 收敛性问题"></a>5.2.3 收敛性问题</h4><p>如果写了一个简单的LSTM网络去训练数据，会发现loss并不会按照想象的方式下降，而是在乱跳，这是因为RNN的误差曲面粗糙不平导致的，而解决方法是梯度裁剪（gradient clipping）</p><h3 id="5-3-循环神经网络的PyTorch实现"><a href="#5-3-循环神经网络的PyTorch实现" class="headerlink" title="5.3 循环神经网络的PyTorch实现"></a>5.3 循环神经网络的PyTorch实现</h3><h4 id="5-3-1-PyTorch的循环网络模块"><a href="#5-3-1-PyTorch的循环网络模块" class="headerlink" title="5.3.1 PyTorch的循环网络模块"></a>5.3.1 PyTorch的循环网络模块</h4><p><strong>1.标准RNN</strong></p><p><code>nn.RNN()</code><br><strong>参数</strong></p><ul><li><code>input_size</code>表示输入$x_t$的维度</li><li><code>hidden_size</code>表示输出$h_t$的维度</li><li><code>num_layers</code>表示网络层数，默认为1层</li><li><code>nonlinearity</code>表示非线性激活函数，默认为tanh，可选relu</li><li><code>bias</code>表示是否使用偏置，默认为True</li><li><code>batch_first</code>决定网络输入的维度顺序，默认输入顺序（seq,batch,feature），如果设置为True，则顺序为（batch，seq，feature）</li><li><code>dropout</code>，接受一个0到1的数值，并在除最后一层的其他输出层加上dropout层</li><li><code>bidirectional</code>默认是False，如果设置为True，就是双向循环神经网络的结构</li></ul><p><strong>网络接受的输入</strong></p><ul><li>序列输入$x_t$：$x_t$的维度是（seq，batch，feature），分别表示序列长度，批量和输入的特征维度</li><li>记忆输入$h_0$：$h_0$也叫隐藏状态，它的维度是（layers×direction，batch，hidden），分别表示层数乘方向（单向1，双向2），批量和输出的维度</li></ul><p><strong>网络的输出</strong></p><ul><li>output，表示网络实际的输出，维度是（seq，batch，hidden×direction），分别表示序列长度，批量和输出维度乘方向</li><li>$h_n$表示记忆单元，维度是（layer×direction，batch，hidden）分别表示层数乘方向，批量，输出维度</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">basic_rnn = nn.RNN(input_size=<span class="number">20</span>,hidden_size=<span class="number">50</span>,num_layers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">toy_input = Variable(torch.randn(<span class="number">100</span>,<span class="number">32</span>,<span class="number">20</span>)) <span class="comment"># seq,batch,input_size</span></span><br><span class="line">h_0 = Variable(torch.rand(<span class="number">2</span>,<span class="number">32</span>,<span class="number">50</span>)) <span class="comment"># layer * direction,batch,hidden_size</span></span><br><span class="line"></span><br><span class="line">toy_output,h_n = basic_rnn(toy_input,h_0)</span><br></pre></td></tr></table></figure><p><strong>2.LSTM</strong></p><p><code>nn.LSTM()</code><br>参数和标准RNN一样</p><p>LSTM与RNN不同的地方：</p><ul><li>LSTM的参数比标准RNN多，是标准RNN维度的4倍，但是访问的方式仍然是相同的</li><li>LSTM的输入还多了一个$C_0$，它们合在一起称为网络的隐藏状态，即（layer×direction，batch，hidden），当然输出也会有$h_0$,$C_0$</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lstm = nn.LSTM(input_size=<span class="number">20</span>,hidden_size=<span class="number">50</span>,num_layers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">lstm_input = Variable(torch.randn(<span class="number">10</span>, <span class="number">3</span>, <span class="number">20</span>))</span><br><span class="line">out, (h, c) = lstm(lstm_input)</span><br></pre></td></tr></table></figure><p><strong>3.GRU</strong></p><p>GRU本质上和LSTM一样</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">gru_seq = nn.GRU(<span class="number">10</span>, <span class="number">20</span>)</span><br><span class="line">gru_input = Variable(torch.randn(<span class="number">3</span>, <span class="number">32</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">out, h = gru_seq(gru_input)</span><br></pre></td></tr></table></figure><p>它和LSTM不同的地方：</p><ul><li>参数是标准RNN的三倍</li><li>网络的隐藏状态只有h0</li></ul><h4 id="5-3-2-实例介绍"><a href="#5-3-2-实例介绍" class="headerlink" title="5.3.2 实例介绍"></a>5.3.2 实例介绍</h4><p>序列预测</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment">#希望通过前两个月的流量来预测当月的流量</span></span><br><span class="line"><span class="comment">#将前两个月的流量当做输入，当月的流量当做输出</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_dataset</span>(<span class="params">dataset,look_back=<span class="number">2</span></span>):</span></span><br><span class="line">    dataX,dataY = [],[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(dataset)-look_back):</span><br><span class="line">        a = dataset[i:(i+look_back)]</span><br><span class="line">        dataX.append(a)</span><br><span class="line">        dataY.append(dataset[i+look_back])</span><br><span class="line">    <span class="keyword">return</span> np.array(dataX),np.array(dataY)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">lstm_reg</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_size, hidden_size, output_size=<span class="number">1</span>, num_layers=<span class="number">2</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(lstm_reg, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.rnn = nn.LSTM(input_size, hidden_size, num_layers) <span class="comment"># rnn</span></span><br><span class="line">        self.reg = nn.Linear(hidden_size, output_size) <span class="comment"># 回归</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x, _ = self.rnn(x) <span class="comment"># (seq, batch, hidden)</span></span><br><span class="line">        s, b, h = x.shape</span><br><span class="line">        x = x.view(s*b, h) <span class="comment"># 转换成线性层的输入格式</span></span><br><span class="line">        x = self.reg(x)</span><br><span class="line">        x = x.view(s, b, -<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment">#读取数据</span></span><br><span class="line">data_csv = pd.read_csv(<span class="string">&#x27;./data.csv&#x27;</span>, usecols=[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预处理，将数据中na的数据去掉，然后将数据标准化到0~1之间</span></span><br><span class="line">data_csv = data_csv.dropna()</span><br><span class="line">dataset = data_csv.values</span><br><span class="line">dataset = dataset.astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">max_value = np.<span class="built_in">max</span>(dataset)</span><br><span class="line">min_value = np.<span class="built_in">min</span>(dataset)</span><br><span class="line">scalar = max_value - min_value</span><br><span class="line">dataset = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x: x / scalar, dataset))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建好输入输出</span></span><br><span class="line">data_X, data_Y = create_dataset(dataset)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集和测试集，70% 作为训练集</span></span><br><span class="line">train_size = <span class="built_in">int</span>(<span class="built_in">len</span>(data_X) * <span class="number">0.7</span>)</span><br><span class="line">test_size = <span class="built_in">len</span>(data_X) - train_size</span><br><span class="line">train_X = data_X[:train_size]</span><br><span class="line">train_Y = data_Y[:train_size]</span><br><span class="line">test_X = data_X[train_size:]</span><br><span class="line">test_Y = data_Y[train_size:]</span><br><span class="line"></span><br><span class="line"><span class="comment">#将数据改变一下形状 (seq, batch, feature)</span></span><br><span class="line"><span class="comment">#只有一个序列，所以 batch 是 1</span></span><br><span class="line"><span class="comment">#输入的feature是希望依据的几个月份，这里定的是两个月份，feature=2.</span></span><br><span class="line">train_X = train_X.reshape(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">train_Y = train_Y.reshape(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">test_X = test_X.reshape(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">train_x = torch.from_numpy(train_X)</span><br><span class="line">train_y = torch.from_numpy(train_Y)</span><br><span class="line">test_x = torch.from_numpy(test_X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失和优化</span></span><br><span class="line">net = lstm_reg(<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=<span class="number">1e-2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    var_x = Variable(train_x)</span><br><span class="line">    var_y = Variable(train_y)</span><br><span class="line">    <span class="comment"># 前向传播</span></span><br><span class="line">    out = net(var_x)</span><br><span class="line">    loss = criterion(out, var_y)</span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="keyword">if</span> (e + <span class="number">1</span>) % <span class="number">100</span> == <span class="number">0</span>: <span class="comment"># 每 100 次输出结果</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Epoch: &#123;&#125;, Loss: &#123;:.5f&#125;&#x27;</span>.<span class="built_in">format</span>(e + <span class="number">1</span>, loss.data))</span><br><span class="line">        </span><br><span class="line"><span class="comment">#测试</span></span><br><span class="line">net = net.<span class="built_in">eval</span>() <span class="comment"># 转换成测试模式</span></span><br><span class="line">data_X = data_X.reshape(-<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">data_X = torch.from_numpy(data_X)</span><br><span class="line">var_data = Variable(data_X)</span><br><span class="line">pred_test = net(var_data) <span class="comment"># 测试集的预测结果</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 改变输出的格式</span></span><br><span class="line">pred_test = pred_test.view(-<span class="number">1</span>).data.numpy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画出实际结果和预测的结果</span></span><br><span class="line">plt.plot(pred_test, <span class="string">&#x27;r&#x27;</span>, label=<span class="string">&#x27;prediction&#x27;</span>)</span><br><span class="line">plt.plot(dataset, <span class="string">&#x27;b&#x27;</span>, label=<span class="string">&#x27;real&#x27;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20201102181225.png"></p><h3 id="5-4-自然语言处理的应用"><a href="#5-4-自然语言处理的应用" class="headerlink" title="5.4 自然语言处理的应用"></a>5.4 自然语言处理的应用</h3><h4 id="5-4-1-词嵌入"><a href="#5-4-1-词嵌入" class="headerlink" title="5.4.1 词嵌入"></a>5.4.1 词嵌入</h4><p>词嵌入（word embedding），也称为词向量，即对于每个词，可以使用一个高维向量去表示它</p><p>例如：</p><ul><li>(1)The cat likes playing ball</li><li>(2)The kitty likes playing wool</li><li>(3)The dog likes playing ball</li><li>(4)The boy doesn’t like playing ball</li></ul><p>对于这四句话里的四个词，cat，kitty，dog，boy，如果用one-hot编码，那么cat可以是（1，0，0，0），kitty可以是（0，1，0，0），但是cat和kitty都是小猫，所以这两个词实际语义是接近的，但是one-hot不能体现这个特点，于是可以用词嵌入的方式表示这四个词。</p><p>假设使用一个二维向量（a，b）来表示一个词，其中a代表是否喜欢玩球，b代表是否喜欢玩毛线，且数值越大代表越喜欢，那么对于cat可以表示（-1，4），对于kitty可以表示为（-2，5），对于dog可以表示为（3，-2），对于boy可以表示为（-2，-3）</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20201102190952.png"></p><p>可以发现kitty和cat的夹角更小，所以它们更加相似</p><h4 id="5-4-2-词嵌入的PyTorch实现"><a href="#5-4-2-词嵌入的PyTorch实现" class="headerlink" title="5.4.2 词嵌入的PyTorch实现"></a>5.4.2 词嵌入的PyTorch实现</h4><p>PyTorch中的词嵌入是通过函数<code>nn.Embedding(m,n)</code>来实现的，其中m表示所有的单词数目，n表示词嵌入的维度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">word_to_ix = &#123;<span class="string">&#x27;hello&#x27;</span>:<span class="number">0</span>,<span class="string">&#x27;world&#x27;</span>:<span class="number">1</span>&#125;</span><br><span class="line">embeds = nn.Embeding(<span class="number">2</span>,<span class="number">5</span>)</span><br><span class="line">hello_idx = torch.LongTensor([word_to_ix[<span class="string">&#x27;hello&#x27;</span>]])</span><br><span class="line">hello_idx = Variable(hello_idx)</span><br><span class="line">hello_embed = embeds(hello_idx)</span><br><span class="line"><span class="built_in">print</span>(hello_embed)</span><br></pre></td></tr></table></figure><h4 id="5-4-3-N-Gram模型"><a href="#5-4-3-N-Gram模型" class="headerlink" title="5.4.3 N Gram模型"></a>5.4.3 N Gram模型</h4><p>对于一句话，单词的排列顺序是非常重要的，所以我们能否由前面的几个词来预测后面的几个单词呢，比如 ‘I lived in France for 10 years, I can speak _ ‘ 这句话中，我们能够预测出最后一个词是 French。</p><p>对于一句话T，它由w1，w2,…,wn这n个词构成，可以得到下面的公式<br>$$<br>P(T) = P(w_1)P(w_2 | w_1)P(w_3 |w_2 w_1) \cdots P(w_n |w_{n-1} w_{n-2}\cdots w_2w_1)<br>$$<br>但是该模型存在如参数空间过大等缺陷，因此引入了马尔科夫假设，也就是说这个单词只与前面的几个词有关系。</p><p>对于这个条件概率，传统的方式是统计语料中每个单词出现的频率，据此来估计这个条件概率，这里使用词嵌入的办法，直接在语料中计算这个条件概率，然后最大化条件概率从而优化词向量，据此进行预测</p><h4 id="5-4-4-单词预测的PyTorch实现"><a href="#5-4-4-单词预测的PyTorch实现" class="headerlink" title="5.4.4 单词预测的PyTorch实现"></a>5.4.4 单词预测的PyTorch实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line">CONTEXT_SIZE = <span class="number">2</span> <span class="comment"># 依据的单词数</span></span><br><span class="line">EMBEDDING_DIM = <span class="number">10</span> <span class="comment"># 词向量的维度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">n_gram</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, context_size=CONTEXT_SIZE, n_dim=EMBEDDING_DIM</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(n_gram, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.embed = nn.Embedding(vocab_size, n_dim)</span><br><span class="line">        self.classify = nn.Sequential(</span><br><span class="line">            nn.Linear(context_size * n_dim, <span class="number">128</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(<span class="number">128</span>, vocab_size)</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        voc_embed = self.embed(x) <span class="comment"># 得到词嵌入</span></span><br><span class="line">        voc_embed = voc_embed.view(<span class="number">1</span>, -<span class="number">1</span>) <span class="comment"># 将两个词向量拼在一起</span></span><br><span class="line">        out = self.classify(voc_embed)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们使用莎士比亚的诗</span></span><br><span class="line">test_sentence = <span class="string">&quot;&quot;&quot;When forty winters shall besiege thy brow,</span></span><br><span class="line"><span class="string">And dig deep trenches in thy beauty&#x27;s field,</span></span><br><span class="line"><span class="string">Thy youth&#x27;s proud livery so gazed on now,</span></span><br><span class="line"><span class="string">Will be a totter&#x27;d weed of small worth held:</span></span><br><span class="line"><span class="string">Then being asked, where all thy beauty lies,</span></span><br><span class="line"><span class="string">Where all the treasure of thy lusty days;</span></span><br><span class="line"><span class="string">To say, within thine own deep sunken eyes,</span></span><br><span class="line"><span class="string">Were an all-eating shame, and thriftless praise.</span></span><br><span class="line"><span class="string">How much more praise deserv&#x27;d thy beauty&#x27;s use,</span></span><br><span class="line"><span class="string">If thou couldst answer &#x27;This fair child of mine</span></span><br><span class="line"><span class="string">Shall sum my count, and make my old excuse,&#x27;</span></span><br><span class="line"><span class="string">Proving his beauty by succession thine!</span></span><br><span class="line"><span class="string">This were to be new made when thou art old,</span></span><br><span class="line"><span class="string">And see thy blood warm when thou feel&#x27;st it cold.&quot;&quot;&quot;</span>.split()</span><br><span class="line"></span><br><span class="line">trigram = [((test_sentence[i], test_sentence[i+<span class="number">1</span>]), test_sentence[i+<span class="number">2</span>]) </span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(test_sentence)-<span class="number">2</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立每个词与数字的编码，据此构建词嵌入</span></span><br><span class="line">vocb = <span class="built_in">set</span>(test_sentence) <span class="comment"># 使用 set 将重复的元素去掉</span></span><br><span class="line">word_to_idx = &#123;word: i <span class="keyword">for</span> i, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocb)&#125;</span><br><span class="line">idx_to_word = &#123;word_to_idx[word]: word <span class="keyword">for</span> word <span class="keyword">in</span> word_to_idx&#125;</span><br><span class="line"></span><br><span class="line">net = n_gram(<span class="built_in">len</span>(word_to_idx))</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">1e-2</span>, weight_decay=<span class="number">1e-5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    train_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> word, label <span class="keyword">in</span> trigram: <span class="comment"># 使用前 100 个作为训练集</span></span><br><span class="line">        word = Variable(torch.LongTensor([word_to_idx[i] <span class="keyword">for</span> i <span class="keyword">in</span> word])) <span class="comment"># 将两个词作为输入</span></span><br><span class="line">        label = Variable(torch.LongTensor([word_to_idx[label]]))</span><br><span class="line">        <span class="comment"># 前向传播</span></span><br><span class="line">        out = net(word)</span><br><span class="line">        loss = criterion(out, label)</span><br><span class="line">        train_loss += loss.data</span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    <span class="keyword">if</span> (e + <span class="number">1</span>) % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch: &#123;&#125;, Loss: &#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(e + <span class="number">1</span>, train_loss / <span class="built_in">len</span>(trigram)))</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line">word, label = trigram[<span class="number">19</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;input: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(word))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;label: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(label))</span><br><span class="line"></span><br><span class="line">word = Variable(torch.LongTensor([word_to_idx[i] <span class="keyword">for</span> i <span class="keyword">in</span> word]))</span><br><span class="line">out = net(word)</span><br><span class="line">pred_label_idx = out.<span class="built_in">max</span>(<span class="number">1</span>)[<span class="number">1</span>].item()</span><br><span class="line">predict_word = idx_to_word[pred_label_idx]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;real word is &#123;&#125;, predicted word is &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(label, predict_word))</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">epoch: 20, Loss: 0.873597</span><br><span class="line">epoch: 40, Loss: 0.153170</span><br><span class="line">epoch: 60, Loss: 0.090456</span><br><span class="line">epoch: 80, Loss: 0.071410</span><br><span class="line">epoch: 100, Loss: 0.061979</span><br><span class="line">input: (&#x27;so&#x27;, &#x27;gazed&#x27;)</span><br><span class="line">label: on</span><br><span class="line"></span><br><span class="line">real word is on, predicted word is on</span><br></pre></td></tr></table></figure><h4 id="5-4-5-词性判断"><a href="#5-4-5-词性判断" class="headerlink" title="5.4.5 词性判断"></a>5.4.5 词性判断</h4><p><strong>1.LSTM做词性判断的基本原理</strong></p><p>同构LSTM，根据它记忆的特性，能够通过这个单词前面记忆的一些词语来对它做一个判断，比如前面的单词如果是my，那么紧跟的词很可能是一个名词，这样就能充分利用上文来处理这个问题</p><p><strong>2.字符增强</strong></p><p>通过引入字符来增强表达，比如有些单词存在前缀或者后缀，比如<code>-ly</code>这种后缀很有可能是副词，这样我们就能在字符水平对词性进一步判断，把两种方法集成起来，能够得到一个更好的结果</p><h4 id="5-4-6-词性判断的PyTorch实现"><a href="#5-4-6-词性判断的PyTorch实现" class="headerlink" title="5.4.6 词性判断的PyTorch实现"></a>5.4.6 词性判断的PyTorch实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line">training_data = [(<span class="string">&quot;The dog ate the apple&quot;</span>.split(),</span><br><span class="line">                  [<span class="string">&quot;DET&quot;</span>, <span class="string">&quot;NN&quot;</span>, <span class="string">&quot;V&quot;</span>, <span class="string">&quot;DET&quot;</span>, <span class="string">&quot;NN&quot;</span>]),</span><br><span class="line">                 (<span class="string">&quot;Everybody read that book&quot;</span>.split(), </span><br><span class="line">                  [<span class="string">&quot;NN&quot;</span>, <span class="string">&quot;V&quot;</span>, <span class="string">&quot;DET&quot;</span>, <span class="string">&quot;NN&quot;</span>])]</span><br><span class="line"></span><br><span class="line"><span class="comment">#对单词和标签进行编码</span></span><br><span class="line">word_to_idx = &#123;&#125;</span><br><span class="line">tag_to_idx = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> context, tag <span class="keyword">in</span> training_data:</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> context:</span><br><span class="line">        <span class="keyword">if</span> word.lower() <span class="keyword">not</span> <span class="keyword">in</span> word_to_idx:</span><br><span class="line">            word_to_idx[word.lower()] = <span class="built_in">len</span>(word_to_idx)</span><br><span class="line">    <span class="keyword">for</span> label <span class="keyword">in</span> tag:</span><br><span class="line">        <span class="keyword">if</span> label.lower() <span class="keyword">not</span> <span class="keyword">in</span> tag_to_idx:</span><br><span class="line">            tag_to_idx[label.lower()] = <span class="built_in">len</span>(tag_to_idx)</span><br><span class="line"></span><br><span class="line"><span class="comment">#对字母编码</span></span><br><span class="line">alphabet = <span class="string">&#x27;abcdefghijklmnopqrstuvwxyz&#x27;</span></span><br><span class="line">char_to_idx = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(alphabet)):</span><br><span class="line">    char_to_idx[alphabet[i]] = i</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_sequence</span>(<span class="params">x, dic</span>):</span> <span class="comment"># 字符编码</span></span><br><span class="line">    idx = [dic[i.lower()] <span class="keyword">for</span> i <span class="keyword">in</span> x]</span><br><span class="line">    idx = torch.LongTensor(idx)</span><br><span class="line">    <span class="keyword">return</span> idx</span><br><span class="line"></span><br><span class="line"><span class="comment">#构建单个字符的lstm模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">char_lstm</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_char, char_dim, char_hidden</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(char_lstm, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.char_embed = nn.Embedding(n_char, char_dim)</span><br><span class="line">        self.lstm = nn.LSTM(char_dim, char_hidden)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.char_embed(x)</span><br><span class="line">        out, _ = self.lstm(x)</span><br><span class="line">        <span class="keyword">return</span> out[-<span class="number">1</span>] <span class="comment"># (batch, hidden)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#构建词性分类的lstm模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">lstm_tagger</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_word, n_char, char_dim, word_dim, </span></span></span><br><span class="line"><span class="params"><span class="function">                 char_hidden, word_hidden, n_tag</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(lstm_tagger, self).__init__()</span><br><span class="line">        self.word_embed = nn.Embedding(n_word, word_dim)</span><br><span class="line">        self.char_lstm = char_lstm(n_char, char_dim, char_hidden)</span><br><span class="line">        self.word_lstm = nn.LSTM(word_dim + char_hidden, word_hidden)</span><br><span class="line">        self.classify = nn.Linear(word_hidden, n_tag)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, word</span>):</span></span><br><span class="line">        char = []</span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> word: <span class="comment"># 对于每个单词做字符的 lstm</span></span><br><span class="line">            char_list = make_sequence(w, char_to_idx)</span><br><span class="line">            char_list = char_list.unsqueeze(<span class="number">1</span>) <span class="comment"># (seq, batch, feature) 满足 lstm 输入条件</span></span><br><span class="line">            char_infor = self.char_lstm(Variable(char_list)) <span class="comment"># (batch, char_hidden)</span></span><br><span class="line">            char.append(char_infor)</span><br><span class="line">        char = torch.stack(char, dim=<span class="number">0</span>) <span class="comment"># (seq, batch, feature)</span></span><br><span class="line">        </span><br><span class="line">        x = self.word_embed(x) <span class="comment"># (batch, seq, word_dim)</span></span><br><span class="line">        x = x.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>) <span class="comment"># 改变顺序</span></span><br><span class="line">        x = torch.cat((x, char), dim=<span class="number">2</span>) <span class="comment"># 沿着特征通道将每个词的词嵌入和字符 lstm 输出的结果拼接在一起</span></span><br><span class="line">        x, _ = self.word_lstm(x)</span><br><span class="line">        </span><br><span class="line">        s, b, h = x.shape</span><br><span class="line">        x = x.view(-<span class="number">1</span>, h) <span class="comment"># 重新 reshape 进行分类线性层</span></span><br><span class="line">        out = self.classify(x)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">net = lstm_tagger(<span class="built_in">len</span>(word_to_idx), <span class="built_in">len</span>(char_to_idx), <span class="number">10</span>, <span class="number">100</span>, <span class="number">50</span>, <span class="number">128</span>, <span class="built_in">len</span>(tag_to_idx))</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">1e-2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">300</span>):</span><br><span class="line">    train_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> word, tag <span class="keyword">in</span> training_data:</span><br><span class="line">        word_list = make_sequence(word, word_to_idx).unsqueeze(<span class="number">0</span>) <span class="comment"># 添加第一维 batch</span></span><br><span class="line">        tag = make_sequence(tag, tag_to_idx)</span><br><span class="line">        word_list = Variable(word_list)</span><br><span class="line">        tag = Variable(tag)</span><br><span class="line">        <span class="comment"># 前向传播</span></span><br><span class="line">        out = net(word_list, word)</span><br><span class="line">        loss = criterion(out, tag)</span><br><span class="line">        train_loss += loss.data</span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    <span class="keyword">if</span> (e + <span class="number">1</span>) % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Epoch: &#123;&#125;, Loss: &#123;:.5f&#125;&#x27;</span>.<span class="built_in">format</span>(e + <span class="number">1</span>, train_loss / <span class="built_in">len</span>(training_data)))</span><br><span class="line"></span><br><span class="line"><span class="comment">#测试</span></span><br><span class="line">net = net.<span class="built_in">eval</span>()</span><br><span class="line">test_sent = <span class="string">&#x27;Everybody ate the apple&#x27;</span></span><br><span class="line">test = make_sequence(test_sent.split(), word_to_idx).unsqueeze(<span class="number">0</span>)</span><br><span class="line">out = net(Variable(test), test_sent.split())</span><br><span class="line"><span class="built_in">print</span>(out)</span><br><span class="line"><span class="built_in">print</span>(tag_to_idx)</span><br></pre></td></tr></table></figure><h3 id="5-5-循环神经网络的更多应用"><a href="#5-5-循环神经网络的更多应用" class="headerlink" title="5.5 循环神经网络的更多应用"></a>5.5 循环神经网络的更多应用</h3><h4 id="5-5-1-Many-to-one"><a href="#5-5-1-Many-to-one" class="headerlink" title="5.5.1 Many to one"></a>5.5.1 Many to one</h4><p>循环神经网络不仅能够输入序列，输出序列，还能后输入序列，输出单个向量。只需要再输出的序列里面取其中一个就可以，通常是取最后一个。这样的结构被称为Many to one。</p><p>Many to one的结构可以用来执行什么任务：</p><ul><li>情感分析</li><li>关键字提取</li></ul><h4 id="5-5-2-Many-to-Many-shorter"><a href="#5-5-2-Many-to-Many-shorter" class="headerlink" title="5.5.2 Many to Many (shorter)"></a>5.5.2 Many to Many (shorter)</h4><p>这种结构是输入和输出都是序列，但是输出的序列比输入的序列短。这种类型的结构通常在语音识别中遇到，因为一段话如果用语言表达往往会比这段话更长。这种情况需要使用CTC算法解决重复的问题，CTC就是将输出的所有可能列举出来，然后通过去重复，去空格的方式来选择最大的概率。</p><h4 id="5-5-3-Seq2seq"><a href="#5-5-3-Seq2seq" class="headerlink" title="5.5.3 Seq2seq"></a>5.5.3 Seq2seq</h4><p>这种情况是输出的长度不确定，一般是在机器翻译的任务中出现。</p><h4 id="5-5-4-CNN-RNN"><a href="#5-5-4-CNN-RNN" class="headerlink" title="5.5.4 CNN+RNN"></a>5.5.4 CNN+RNN</h4><p>RNN和CNN可以联合在一起完成图像描述任务，简而言之，就是通过预训练的卷积神经网络提取图片特征，接着通过循环网络将特征变成文字描述</p><h2 id="第6章-生成对抗网络"><a href="#第6章-生成对抗网络" class="headerlink" title="第6章 生成对抗网络"></a>第6章 生成对抗网络</h2><p>2014年，lan Goodfellow提出的生成对抗网络（Generative Adversarial Networks，GANs）推进了整个无监督学习的发展进程，让机器实现一些创造性工作，如画画，写诗，创作歌词等成为可能…</p><h3 id="6-1-生成模型"><a href="#6-1-生成模型" class="headerlink" title="6.1 生成模型"></a>6.1 生成模型</h3><p>生成模型(Generative Model)这一概念属于概率统计和机器学习,是指一系列用于随机生成可观测数据的模型.简而言之,就是”生成”的样本和”真实”的样本尽可能地相似.</p><p>生成模型的两个主要功能就是学习一个概率分布$P_{model}(x)$和生成数据</p><h4 id="6-1-1-自动编码器"><a href="#6-1-1-自动编码器" class="headerlink" title="6.1.1 自动编码器"></a>6.1.1 自动编码器</h4><p>自动编码器(AutoEncoder)最开始作为一种数据的压缩方法,其特点有:</p><ul><li>和数据相关程度很高</li><li>压缩后数据是有损的</li></ul><p>所以现在自动编码器主要应用在几个方面:</p><ul><li>数据去噪</li><li>可视化降维</li><li>生成数据</li></ul><p>自动编码器的一般结构</p><ul><li>编码器(Encoder)</li><li>解码器(Decoder)</li></ul><p>编码器和解码器可以是任意的模型,通常使用神经网络模型作为编码器和解码器.输入的数据经过神经网络降维到一个编码(code),接着又通过另一个神经网络去解码得到一个与输入原数据一模一样的生成数据,然后通过比较这两个数据,最小化它们之间的差异来训练这个网络中编码器和解码器的参数.当这个过程训练完之后,拿出这个解码器,随机传入一个编码,通过解码器能够生成一个和原数据差不多的数据</p><p>下面我们使用 mnist 数据集来说明一个如何构建一个简单的自动编码器</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> MNIST</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms <span class="keyword">as</span> tfs</span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> save_image</span><br><span class="line"></span><br><span class="line"><span class="comment">#进行数据预处理和迭代器的构建</span></span><br><span class="line">im_tfs = tfs.Compose([</span><br><span class="line">    tfs.ToTensor(),</span><br><span class="line">    tfs.Normalize([<span class="number">0.5</span>], [<span class="number">0.5</span>]) <span class="comment"># 标准化</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">train_set = MNIST(<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>,transform=im_tfs,download=<span class="literal">True</span>)</span><br><span class="line">train_data = DataLoader(train_set, batch_size=<span class="number">128</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义网络</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">autoencoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(autoencoder,self).__init__()</span><br><span class="line">        self.encoder = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">28</span>*<span class="number">28</span>,<span class="number">128</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(<span class="number">128</span>,<span class="number">64</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(<span class="number">64</span>,<span class="number">12</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(<span class="number">12</span>,<span class="number">3</span>) <span class="comment"># 输出的 code 是 3 维，便于可视化</span></span><br><span class="line">        )</span><br><span class="line">        self.decoder = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">3</span>,<span class="number">12</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(<span class="number">12</span>,<span class="number">64</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(<span class="number">64</span>,<span class="number">128</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(<span class="number">128</span>,<span class="number">28</span>*<span class="number">28</span>),</span><br><span class="line">            nn.Tanh()</span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        encode = self.encoder(x)</span><br><span class="line">        decode = self.decoder(encode)</span><br><span class="line">        <span class="keyword">return</span> encode,decode</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">这里定义的编码器和解码器都是 4 层神经网络作为模型，</span></span><br><span class="line"><span class="string">中间使用 relu 激活函数，最后输出的 code 是三维，</span></span><br><span class="line"><span class="string">注意解码器最后我们使用tanh作为激活函数，</span></span><br><span class="line"><span class="string">因为输入图片标准化在 -1 ~ 1 之间，</span></span><br><span class="line"><span class="string">所以输出也要在 -1 ~ 1 这个范围内</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">net = autoencoder()</span><br><span class="line">criterion = nn.MSELoss(size_average=<span class="literal">False</span>)</span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=<span class="number">1e-3</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_img</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="comment"># 定义一个函数将最后的结果转换回图片</span></span><br><span class="line">    x = <span class="number">0.5</span> * (x + <span class="number">1.</span>)</span><br><span class="line">    x = x.clamp(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    x = x.view(x.shape[<span class="number">0</span>], <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始训练自动编码器</span></span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">for</span> im, _ <span class="keyword">in</span> train_data:</span><br><span class="line">        im = im.view(im.shape[<span class="number">0</span>], -<span class="number">1</span>)</span><br><span class="line">        im = Variable(im)</span><br><span class="line">        <span class="comment"># 前向传播</span></span><br><span class="line">        _, output = net(im)</span><br><span class="line">        loss = criterion(output, im) / im.shape[<span class="number">0</span>] <span class="comment"># 平均</span></span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (e+<span class="number">1</span>) % <span class="number">20</span> == <span class="number">0</span>: <span class="comment"># 每 20 次，将生成的图片保存一下</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch: &#123;&#125;, Loss: &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(e + <span class="number">1</span>, loss.data))</span><br><span class="line">        pic = to_img(output.cpu().data)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&#x27;./simple_autoencoder&#x27;</span>):</span><br><span class="line">            os.mkdir(<span class="string">&#x27;./simple_autoencoder&#x27;</span>)</span><br><span class="line">        save_image(pic, <span class="string">&#x27;./simple_autoencoder/image_&#123;&#125;.png&#x27;</span>.<span class="built_in">format</span>(e + <span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>训练完成之后看看效果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> cm</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化结果</span></span><br><span class="line">view_data = Variable((train_set.train_data[:<span class="number">200</span>].<span class="built_in">type</span>(torch.FloatTensor).view(-<span class="number">1</span>, <span class="number">28</span>*<span class="number">28</span>) / <span class="number">255.</span> - <span class="number">0.5</span>) / <span class="number">0.5</span>)</span><br><span class="line">encode, _ = net(view_data)    <span class="comment"># 提取压缩的特征值</span></span><br><span class="line">fig = plt.figure(<span class="number">2</span>)</span><br><span class="line">ax = Axes3D(fig)    <span class="comment"># 3D 图</span></span><br><span class="line"><span class="comment"># x, y, z 的数据值</span></span><br><span class="line">X = encode.data[:, <span class="number">0</span>].numpy()</span><br><span class="line">Y = encode.data[:, <span class="number">1</span>].numpy()</span><br><span class="line">Z = encode.data[:, <span class="number">2</span>].numpy()</span><br><span class="line">values = train_set.train_labels[:<span class="number">200</span>].numpy()  <span class="comment"># 标签值</span></span><br><span class="line"><span class="keyword">for</span> x, y, z, s <span class="keyword">in</span> <span class="built_in">zip</span>(X, Y, Z, values):</span><br><span class="line">    c = cm.rainbow(<span class="built_in">int</span>(<span class="number">255</span>*s/<span class="number">9</span>))    <span class="comment"># 上色</span></span><br><span class="line">    ax.text(x, y, z, s, backgroundcolor=c)  <span class="comment"># 标位子</span></span><br><span class="line">ax.set_xlim(X.<span class="built_in">min</span>(), X.<span class="built_in">max</span>())</span><br><span class="line">ax.set_ylim(Y.<span class="built_in">min</span>(), Y.<span class="built_in">max</span>())</span><br><span class="line">ax.set_zlim(Z.<span class="built_in">min</span>(), Z.<span class="built_in">max</span>())</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/untitled.png"><br>可以看到，不同种类的图片进入自动编码器之后会被编码得不同，而相同类型的图片经过自动编码之后的编码在几何示意图上距离较近，在训练好自动编码器之后，我们可以给一个随机的 code，通过 decoder 生成图片</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">code = Variable(torch.FloatTensor([[-<span class="number">20.19</span>, <span class="number">10.36</span>, -<span class="number">0.06</span>]])) <span class="comment"># 给一个 code</span></span><br><span class="line">decode = net.decoder(code)</span><br><span class="line">decode_img = to_img(decode).squeeze()</span><br><span class="line">decode_img = decode_img.data.numpy() * <span class="number">255</span></span><br><span class="line">plt.imshow(decode_img.astype(<span class="string">&#x27;uint8&#x27;</span>), cmap=<span class="string">&#x27;gray&#x27;</span>) </span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20201104180838.png"><br>这里我们仅仅使用多层神经网络定义了一个自动编码器，当然你会想到，为什么不使用效果更好的卷积神经网络呢？我们当然可以使用卷积神经网络来定义，下面我们就重新定义一个卷积神经网络来进行 autoencoder</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">conv_autoencoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(conv_autoencoder, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.encoder = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">16</span>, <span class="number">3</span>, stride=<span class="number">3</span>, padding=<span class="number">1</span>),  <span class="comment"># (b, 16, 10, 10)</span></span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, stride=<span class="number">2</span>),  <span class="comment"># (b, 16, 5, 5)</span></span><br><span class="line">            nn.Conv2d(<span class="number">16</span>, <span class="number">8</span>, <span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),  <span class="comment"># (b, 8, 3, 3)</span></span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, stride=<span class="number">1</span>)  <span class="comment"># (b, 8, 2, 2)</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.decoder = nn.Sequential(</span><br><span class="line">            nn.ConvTranspose2d(<span class="number">8</span>, <span class="number">16</span>, <span class="number">3</span>, stride=<span class="number">2</span>),  <span class="comment"># (b, 16, 5, 5)</span></span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.ConvTranspose2d(<span class="number">16</span>, <span class="number">8</span>, <span class="number">5</span>, stride=<span class="number">3</span>, padding=<span class="number">1</span>),  <span class="comment"># (b, 8, 15, 15)</span></span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.ConvTranspose2d(<span class="number">8</span>, <span class="number">1</span>, <span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>),  <span class="comment"># (b, 1, 28, 28)</span></span><br><span class="line">            nn.Tanh()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        encode = self.encoder(x)</span><br><span class="line">        decode = self.decoder(encode)</span><br><span class="line">        <span class="keyword">return</span> encode, decode</span><br><span class="line"></span><br><span class="line">conv_net = conv_autoencoder()</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    conv_net = conv_net.cuda()</span><br><span class="line">optimizer = torch.optim.Adam(conv_net.parameters(), lr=<span class="number">1e-3</span>, weight_decay=<span class="number">1e-5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始训练自动编码器</span></span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">40</span>):</span><br><span class="line">    <span class="keyword">for</span> im, _ <span class="keyword">in</span> train_data:</span><br><span class="line">        <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">            im = im.cuda()</span><br><span class="line">            <span class="built_in">print</span>(torch.device(<span class="string">&quot;cuda&quot;</span>))</span><br><span class="line">        im = Variable(im)</span><br><span class="line">        <span class="comment"># 前向传播</span></span><br><span class="line">        _, output = conv_net(im)</span><br><span class="line">        loss = criterion(output, im) / im.shape[<span class="number">0</span>] <span class="comment"># 平均</span></span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> (e+<span class="number">1</span>) % <span class="number">20</span> == <span class="number">0</span>: <span class="comment"># 每 20 次，将生成的图片保存一下</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch: &#123;&#125;, Loss: &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(e+<span class="number">1</span>, loss.data))</span><br><span class="line">        pic = to_img(output.cpu().data)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&#x27;./conv_autoencoder&#x27;</span>):</span><br><span class="line">            os.mkdir(<span class="string">&#x27;./conv_autoencoder&#x27;</span>)</span><br><span class="line">        save_image(pic, <span class="string">&#x27;./conv_autoencoder/image_&#123;&#125;.png&#x27;</span>.<span class="built_in">format</span>(e+<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>为了时间更短，只跑 40 次，如果有条件可以再 gpu 上跑跑.这里我们展示了简单的自动编码器，也用了多层神经网络和卷积神经网络作为例子，但是自动编码器存在一个问题，我们并不能任意生成我们想要的数据，因为我们并不知道 encode 之后的编码到底是什么样的概率分布，所以有一个改进的版本变分自动编码器，其能够解决这个问题</p><h4 id="6-1-2-变分自动编码器"><a href="#6-1-2-变分自动编码器" class="headerlink" title="6.1.2 变分自动编码器"></a>6.1.2 变分自动编码器</h4><p>变分自动编码器（Variational Auto Encoder, VAE）是自动编码器的升级版本，它的结构和自动编码器相似，也是由编码器和解码器构成的。</p><p>自动编码器不能任意生成数据，因为没办法自己去构造隐藏向量，需要通过数据输入编码才知道得到的隐含向量是什么，这个时候变分自动编码器就可以解决这个问题</p><p>它的原理是，在编码过程给他增加一些限制，迫使他生成的隐含向量能够粗略地遵循一个标准正态分布。</p><p>这样我们生成一张新图片就很简单了，我们只需要给它一个标准正态分布的随机隐含向量，这样通过解码器就能够生成我们想要的图片，而不需要给它一张原始图片先编码。</p><p>一般来讲，我们通过 encoder 得到的隐含向量并不是一个标准的正态分布，为了衡量两种分布的相似程度，我们使用 KL divergence，利用其来表示隐含向量与标准正态分布之间差异的 loss，另外一个 loss 仍然使用生成图片与原图片的均方误差来表示。</p><p>KL divergence 的公式如下<br>$$<br>D_{KL} (P || Q) =  \sum_{i} p(i) \log \frac{P(i)}{Q(i)}<br>$$</p><p>$$<br>D_{KL} (P || Q) =  \int_{-\infty}^{\infty} p(x) \log \frac{p(x)}{q(x)} dx<br>$$</p><p><strong>重参数</strong></p><p>为了避免计算 KL divergence 中的积分，我们使用重参数的技巧，不是每次产生一个隐含向量，而是生成两个向量，一个表示均值，一个表示标准差，这里我们默认编码之后的隐含向量服从一个正态分布的之后，就可以用一个标准正态分布先乘上标准差再加上均值来合成这个正态分布，最后 loss 就是希望这个生成的正态分布能够符合一个标准正态分布，也就是希望均值为 0，方差为 1</p><p><a href="https://arxiv.org/pdf/1606.05908.pdf">详细内容见https://arxiv.org/pdf/1606.05908.pdf</a></p><p>所以最后我们可以将我们的 loss 定义为下面的函数，由均方误差和 KL divergence 求和得到一个总的 loss</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">reconstruction_funtion = nn.BCELoss(size_average=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss_function</span>(<span class="params">recon_x, x, mu, logvar</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    recon_x: generating images</span></span><br><span class="line"><span class="string">    x: origin images</span></span><br><span class="line"><span class="string">    mu: latent mean</span></span><br><span class="line"><span class="string">    logvar: latent log variance</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    MSE = reconstruction_function(recon_x, x)</span><br><span class="line">    <span class="comment"># loss = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)</span></span><br><span class="line">    KLD_element = mu.<span class="built_in">pow</span>(<span class="number">2</span>).add_(logvar.exp()).mul_(-<span class="number">1</span>).add_(<span class="number">1</span>).add_(logvar)</span><br><span class="line">    KLD = torch.<span class="built_in">sum</span>(KLD_element).mul_(-<span class="number">0.5</span>)</span><br><span class="line">    <span class="comment"># KL divergence</span></span><br><span class="line">    <span class="keyword">return</span> MSE + KLD</span><br></pre></td></tr></table></figure><p>下面我们用 mnist 数据集来简单说明一下变分自动编码器</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> MNIST</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms <span class="keyword">as</span> tfs</span><br><span class="line"><span class="keyword">from</span> torchvision.utils <span class="keyword">import</span> save_image</span><br><span class="line"></span><br><span class="line">im_tfs = tfs.Compose([</span><br><span class="line">    tfs.ToTensor(),</span><br><span class="line">    tfs.Normalize([<span class="number">0.5</span>], [<span class="number">0.5</span>]) <span class="comment"># 标准化</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">train_set = MNIST(<span class="string">&#x27;./data&#x27;</span>, transform=im_tfs)</span><br><span class="line">train_data = DataLoader(train_set, batch_size=<span class="number">128</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VAE</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(VAE, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">784</span>, <span class="number">400</span>)</span><br><span class="line">        self.fc21 = nn.Linear(<span class="number">400</span>, <span class="number">20</span>) <span class="comment"># mean</span></span><br><span class="line">        self.fc22 = nn.Linear(<span class="number">400</span>, <span class="number">20</span>) <span class="comment"># var</span></span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">20</span>, <span class="number">400</span>)</span><br><span class="line">        self.fc4 = nn.Linear(<span class="number">400</span>, <span class="number">784</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        h1 = F.relu(self.fc1(x))</span><br><span class="line">        <span class="keyword">return</span> self.fc21(h1), self.fc22(h1)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reparametrize</span>(<span class="params">self, mu, logvar</span>):</span></span><br><span class="line">        std = logvar.mul(<span class="number">0.5</span>).exp_()</span><br><span class="line">        eps = torch.FloatTensor(std.size()).normal_()</span><br><span class="line">        <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">            eps = Variable(eps.cuda())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            eps = Variable(eps)</span><br><span class="line">        <span class="keyword">return</span> eps.mul(std).add_(mu)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span>(<span class="params">self, z</span>):</span></span><br><span class="line">        h3 = F.relu(self.fc3(z))</span><br><span class="line">        <span class="keyword">return</span> F.tanh(self.fc4(h3))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        mu, logvar = self.encode(x) <span class="comment"># 编码</span></span><br><span class="line">        z = self.reparametrize(mu, logvar) <span class="comment"># 重新参数化成正态分布</span></span><br><span class="line">        <span class="keyword">return</span> self.decode(z), mu, logvar <span class="comment"># 解码，同时输出均值方差</span></span><br><span class="line"></span><br><span class="line">net = VAE() <span class="comment"># 实例化网络</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    net = net.cuda()</span><br><span class="line"></span><br><span class="line">reconstruction_function = nn.MSELoss(size_average=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss_function</span>(<span class="params">recon_x, x, mu, logvar</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    recon_x: generating images</span></span><br><span class="line"><span class="string">    x: origin images</span></span><br><span class="line"><span class="string">    mu: latent mean</span></span><br><span class="line"><span class="string">    logvar: latent log variance</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    MSE = reconstruction_function(recon_x, x)</span><br><span class="line">    <span class="comment"># loss = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)</span></span><br><span class="line">    KLD_element = mu.<span class="built_in">pow</span>(<span class="number">2</span>).add_(logvar.exp()).mul_(-<span class="number">1</span>).add_(<span class="number">1</span>).add_(logvar)</span><br><span class="line">    KLD = torch.<span class="built_in">sum</span>(KLD_element).mul_(-<span class="number">0.5</span>)</span><br><span class="line">    <span class="comment"># KL divergence</span></span><br><span class="line">    <span class="keyword">return</span> MSE + KLD</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.Adam(net.parameters(), lr=<span class="number">1e-3</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_img</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="comment">#定义一个函数将最后的结果转换回图片</span></span><br><span class="line">    x = <span class="number">0.5</span> * (x + <span class="number">1.</span>)</span><br><span class="line">    x = x.clamp(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    x = x.view(x.shape[<span class="number">0</span>], <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">for</span> im, _ <span class="keyword">in</span> train_data:</span><br><span class="line">        im = im.view(im.shape[<span class="number">0</span>], -<span class="number">1</span>)</span><br><span class="line">        im = Variable(im)</span><br><span class="line">        <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">            im = im.cuda()</span><br><span class="line">            <span class="built_in">print</span>(torch.device(<span class="string">&quot;cuda&quot;</span>))</span><br><span class="line">        recon_im, mu, logvar = net(im)</span><br><span class="line">        loss = loss_function(recon_im, im, mu, logvar) / im.shape[<span class="number">0</span>] <span class="comment"># 将 loss 平均</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (e + <span class="number">1</span>) % <span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch: &#123;&#125;, Loss: &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(e + <span class="number">1</span>, loss.data[<span class="number">0</span>]))</span><br><span class="line">        save = to_img(recon_im.cpu().data)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&#x27;./vae_img&#x27;</span>):</span><br><span class="line">            os.mkdir(<span class="string">&#x27;./vae_img&#x27;</span>)</span><br><span class="line">        save_image(save, <span class="string">&#x27;./vae_img/image_&#123;&#125;.png&#x27;</span>.<span class="built_in">format</span>(e + <span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>可以看看使用变分自动编码器得到的结果，可以发现效果比一般的编码器要好很多</p><h3 id="6-2-生成对抗网络"><a href="#6-2-生成对抗网络" class="headerlink" title="6.2 生成对抗网络"></a>6.2 生成对抗网络</h3><p>前面我们讲了自动编码器和变分自动编码器，不管是哪一个，都是通过计算生成图像和输入图像在每个像素点的误差来生成 loss，这一点是特别不好的，因为不同的像素点可能造成不同的视觉结果，但是可能他们的 loss 是相同的，所以通过单个像素点来得到 loss 是不准确的，这个时候我们需要一种全新的 loss 定义方式，就是通过对抗进行学习。</p><h4 id="6-2-1-什么是生成对抗网络"><a href="#6-2-1-什么是生成对抗网络" class="headerlink" title="6.2.1 什么是生成对抗网络"></a>6.2.1 什么是生成对抗网络</h4><p>这种训练方式定义了一种全新的网络结构，就是生成对抗网络，也就是 GANs。</p><p>根据这个名字就可以知道这个网络是由两部分组成的，第一部分是生成，第二部分是对抗。简单来说，就是有一个生成网络和一个判别网络，通过训练让两个网络相互竞争，生成网络来生成假的数据，对抗网络通过判别器去判别真伪，最后希望生成器生成的数据能够以假乱真。</p><p><strong>对抗：Discriminator Network</strong></p><p>首先我们来讲一下对抗过程，因为这个过程更加简单。</p><p>对抗过程简单来说就是一个判断真假的判别器，相当于一个二分类问题，我们输入一张真的图片希望判别器输出的结果是1，输入一张假的图片希望判别器输出的结果是0。这其实已经和原图片的 label 没有关系了，不管原图片到底是一个多少类别的图片，他们都统一称为真的图片，label 是 1 表示真实的；而生成的假的图片的 label 是 0 表示假的。</p><p>我们训练的过程就是希望这个判别器能够正确的判出真的图片和假的图片，这其实就是一个简单的二分类问题，对于这个问题可以用我们前面讲过的很多方法去处理，比如 logistic 回归，深层网络，卷积神经网络，循环神经网络都可以。</p><p><strong>生成：Generator Network</strong></p><p>接着我们看看生成网络如何生成一张假的图片。首先给出一个简单的高维的正态分布的噪声向量，这个时候我们可以通过仿射变换，也就是 xw+b 将其映射到一个更高的维度，然后将他重新排列成一个矩形，这样看着更像一张图片，接着进行一些卷积、转置卷积、池化、激活函数等进行处理，最后得到了一个与我们输入图片大小一模一样的噪音矩阵，这就是我们所说的假的图片。</p><p>这个时候我们如何去训练这个生成器呢？这就需要通过对抗学习，增大判别器判别这个结果为真的概率，通过这个步骤不断调整生成器的参数，希望生成的图片越来越像真的，而在这一步中我们不会更新判别器的参数，因为如果判别器不断被优化，可能生成器无论生成什么样的图片都无法骗过判别器。</p><p>关于生成对抗网络，出现了很多变形，比如 WGAN，LS-GAN 等等，这里我们只使用 mnist 举一些简单的例子来说明，更复杂的网络结构可以在 github 上找到相应的实现</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> tfs</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, sampler</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> MNIST</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> matplotlib.gridspec <span class="keyword">as</span> gridspec</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] = (<span class="number">10.0</span>, <span class="number">8.0</span>) <span class="comment"># 设置画图的尺寸</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;image.interpolation&#x27;</span>] = <span class="string">&#x27;nearest&#x27;</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;image.cmap&#x27;</span>] = <span class="string">&#x27;gray&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_images</span>(<span class="params">images</span>):</span> <span class="comment"># 定义画图工具</span></span><br><span class="line">    images = np.reshape(images, [images.shape[<span class="number">0</span>], -<span class="number">1</span>])</span><br><span class="line">    sqrtn = <span class="built_in">int</span>(np.ceil(np.sqrt(images.shape[<span class="number">0</span>])))</span><br><span class="line">    sqrtimg = <span class="built_in">int</span>(np.ceil(np.sqrt(images.shape[<span class="number">1</span>])))</span><br><span class="line"></span><br><span class="line">    fig = plt.figure(figsize=(sqrtn, sqrtn))</span><br><span class="line">    gs = gridspec.GridSpec(sqrtn, sqrtn)</span><br><span class="line">    gs.update(wspace=<span class="number">0.05</span>, hspace=<span class="number">0.05</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, img <span class="keyword">in</span> <span class="built_in">enumerate</span>(images):</span><br><span class="line">        ax = plt.subplot(gs[i])</span><br><span class="line">        plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">        ax.set_xticklabels([])</span><br><span class="line">        ax.set_yticklabels([])</span><br><span class="line">        ax.set_aspect(<span class="string">&#x27;equal&#x27;</span>)</span><br><span class="line">        plt.imshow(img.reshape([sqrtimg,sqrtimg]))</span><br><span class="line">    <span class="keyword">return</span> </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_img</span>(<span class="params">x</span>):</span></span><br><span class="line">    x = tfs.ToTensor()(x)</span><br><span class="line">    <span class="keyword">return</span> (x - <span class="number">0.5</span>) / <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deprocess_img</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> (x + <span class="number">1.0</span>) / <span class="number">2.0</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ChunkSampler</span>(<span class="params">sampler.Sampler</span>):</span> <span class="comment"># 定义一个取样的函数</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Samples elements sequentially from some offset. </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        num_samples: # of desired datapoints</span></span><br><span class="line"><span class="string">        start: offset where we should start selecting from</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_samples, start=<span class="number">0</span></span>):</span></span><br><span class="line">        self.num_samples = num_samples</span><br><span class="line">        self.start = start</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">iter</span>(<span class="built_in">range</span>(self.start, self.start + self.num_samples))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.num_samples</span><br><span class="line"></span><br><span class="line">NUM_TRAIN = <span class="number">50000</span></span><br><span class="line">NUM_VAL = <span class="number">5000</span></span><br><span class="line"></span><br><span class="line">NOISE_DIM = <span class="number">96</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line"></span><br><span class="line">train_set = MNIST(<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=preprocess_img)</span><br><span class="line"></span><br><span class="line">train_data = DataLoader(train_set, batch_size=batch_size, sampler=ChunkSampler(NUM_TRAIN, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">val_set = MNIST(<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=preprocess_img)</span><br><span class="line"></span><br><span class="line">val_data = DataLoader(val_set, batch_size=batch_size, sampler=ChunkSampler(NUM_VAL, NUM_TRAIN))</span><br><span class="line"></span><br><span class="line">imgs = deprocess_img(train_data.__iter__().<span class="built_in">next</span>()[<span class="number">0</span>].view(batch_size, <span class="number">784</span>)).numpy().squeeze() <span class="comment"># 可视化图片效果</span></span><br><span class="line">show_images(imgs)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20201105142821.png"></p><p><strong>简单版本的生成对抗网络</strong></p><p>通过前面我们知道生成对抗网络有两个部分构成，一个是生成网络，一个是对抗网络，我们首先写一个简单版本的网络结构，生成网络和对抗网络都是简单的多层神经网络</p><p><strong>判别网络</strong></p><p>判别网络的结构非常简单，就是一个二分类器，结构如下:</p><ul><li>全连接(784 -&gt; 256)</li><li>leakyrelu,  $\alpha$ 是 0.2</li><li>全连接(256 -&gt; 256)</li><li>leakyrelu, $\alpha$ 是 0.2</li><li>全连接(256 -&gt; 1)</li></ul><p>其中 leakyrelu 是指 f(x) = max($\alpha$ x, x)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">discriminator</span>():</span></span><br><span class="line">    net = nn.Sequential(        </span><br><span class="line">            nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">            nn.Linear(<span class="number">256</span>, <span class="number">256</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">            nn.Linear(<span class="number">256</span>, <span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">return</span> net</span><br></pre></td></tr></table></figure><p><strong>生成网络</strong></p><p>接下来我们看看生成网络，生成网络的结构也很简单，就是根据一个随机噪声生成一个和数据维度一样的张量，结构如下：</p><ul><li>全连接(噪音维度 -&gt; 1024)</li><li>relu</li><li>全连接(1024 -&gt; 1024)</li><li>relu</li><li>全连接(1024 -&gt; 784)</li><li>tanh 将数据裁剪到 -1 ~ 1 之间</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator</span>(<span class="params">noise_dim=NOISE_DIM</span>):</span>   </span><br><span class="line">    net = nn.Sequential(</span><br><span class="line">        nn.Linear(noise_dim, <span class="number">1024</span>),</span><br><span class="line">        nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">        nn.Linear(<span class="number">1024</span>, <span class="number">1024</span>),</span><br><span class="line">        nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">        nn.Linear(<span class="number">1024</span>, <span class="number">784</span>),</span><br><span class="line">        nn.Tanh()</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> net</span><br></pre></td></tr></table></figure><p>接下来我们需要定义生成对抗网络的 loss，通过前面的讲解我们知道，对于对抗网络，相当于二分类问题，将真的判别为真的，假的判别为假的，作为辅助，可以参考一下论文中公式</p><p>$$ \ell_D = \mathbb{E}_{x \sim p_\text{data}}\left[\log D(x)\right] + \mathbb{E} _ {z \sim p(z)}\left[\log \left(1-D(G(z))\right)\right]$$</p><p>而对于生成网络，需要去骗过对抗网络，也就是将假的也判断为真的，作为辅助，可以参考一下论文中公式</p><p>$$\ell_G  =  \mathbb{E} _ {z \sim p(z)}\left[\log D(G(z))\right]$$</p><p>如果你还记得前面的二分类 loss，那么你就会发现上面这两个公式就是二分类 loss</p><p>$$ bce(s, y) = y * \log(s) + (1 - y) * \log(1 - s) $$</p><p>如果我们把 D(x) 看成真实数据的分类得分，那么 D(G(z)) 就是假数据的分类得分，所以上面判别器的 loss 就是将真实数据的得分判断为 1，假的数据的得分判断为 0，而生成器的 loss 就是将假的数据判断为 1</p><p>下面我们来实现一下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> tfs</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, sampler</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> MNIST</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">NUM_TRAIN = <span class="number">50000</span></span><br><span class="line">NUM_VAL = <span class="number">5000</span></span><br><span class="line"></span><br><span class="line">NOISE_DIM = <span class="number">96</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">discriminator</span>():</span></span><br><span class="line">    net = nn.Sequential(        </span><br><span class="line">            nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">            nn.Linear(<span class="number">256</span>, <span class="number">256</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">            nn.Linear(<span class="number">256</span>, <span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator</span>(<span class="params">noise_dim=NOISE_DIM</span>):</span>   </span><br><span class="line">    net = nn.Sequential(</span><br><span class="line">        nn.Linear(noise_dim, <span class="number">1024</span>),</span><br><span class="line">        nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">        nn.Linear(<span class="number">1024</span>, <span class="number">1024</span>),</span><br><span class="line">        nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">        nn.Linear(<span class="number">1024</span>, <span class="number">784</span>),</span><br><span class="line">        nn.Tanh()</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line">bce_loss = nn.BCEWithLogitsLoss()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">discriminator_loss</span>(<span class="params">logits_real, logits_fake</span>):</span> <span class="comment"># 判别器的 loss</span></span><br><span class="line">    size = logits_real.shape[<span class="number">0</span>]</span><br><span class="line">    true_labels = Variable(torch.ones(size, <span class="number">1</span>)).<span class="built_in">float</span>().cuda()</span><br><span class="line">    false_labels = Variable(torch.zeros(size, <span class="number">1</span>)).<span class="built_in">float</span>().cuda()</span><br><span class="line">    loss = bce_loss(logits_real, true_labels) + bce_loss(logits_fake, false_labels)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generator_loss</span>(<span class="params">logits_fake</span>):</span> <span class="comment"># 生成器的 loss  </span></span><br><span class="line">    size = logits_fake.shape[<span class="number">0</span>]</span><br><span class="line">    true_labels = Variable(torch.ones(size, <span class="number">1</span>)).<span class="built_in">float</span>().cuda()</span><br><span class="line">    loss = bce_loss(logits_fake, true_labels)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 adam 来进行训练，学习率是 3e-4, beta1 是 0.5, beta2 是 0.999</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_optimizer</span>(<span class="params">net</span>):</span></span><br><span class="line">    optimizer = torch.optim.Adam(net.parameters(), lr=<span class="number">3e-4</span>, betas=(<span class="number">0.5</span>, <span class="number">0.999</span>))</span><br><span class="line">    <span class="keyword">return</span> optimizer</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_img</span>(<span class="params">x</span>):</span></span><br><span class="line">    x = tfs.ToTensor()(x)</span><br><span class="line">    <span class="keyword">return</span> (x - <span class="number">0.5</span>) / <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deprocess_img</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> (x + <span class="number">1.0</span>) / <span class="number">2.0</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ChunkSampler</span>(<span class="params">sampler.Sampler</span>):</span> <span class="comment"># 定义一个取样的函数</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Samples elements sequentially from some offset. </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        num_samples: # of desired datapoints</span></span><br><span class="line"><span class="string">        start: offset where we should start selecting from</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_samples, start=<span class="number">0</span></span>):</span></span><br><span class="line">        self.num_samples = num_samples</span><br><span class="line">        self.start = start</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">iter</span>(<span class="built_in">range</span>(self.start, self.start + self.num_samples))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.num_samples</span><br><span class="line"></span><br><span class="line">train_set = MNIST(<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=preprocess_img)</span><br><span class="line"></span><br><span class="line">train_data = DataLoader(train_set, batch_size=batch_size, sampler=ChunkSampler(NUM_TRAIN, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">val_set = MNIST(<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=preprocess_img)</span><br><span class="line"></span><br><span class="line">val_data = DataLoader(val_set, batch_size=batch_size, sampler=ChunkSampler(NUM_VAL, NUM_TRAIN))</span><br><span class="line"></span><br><span class="line"><span class="comment">#下面我们开始训练一个这个简单的生成对抗网络</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_a_gan</span>(<span class="params">D_net, G_net, D_optimizer, G_optimizer, discriminator_loss, generator_loss, show_every=<span class="number">250</span>, </span></span></span><br><span class="line"><span class="params"><span class="function">                noise_size=<span class="number">96</span>, num_epochs=<span class="number">10</span></span>):</span></span><br><span class="line">    iter_count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> x, _ <span class="keyword">in</span> train_data:</span><br><span class="line">            bs = x.shape[<span class="number">0</span>]</span><br><span class="line">            <span class="comment"># 判别网络</span></span><br><span class="line">            real_data = Variable(x).view(bs, -<span class="number">1</span>).cuda() <span class="comment"># 真实数据</span></span><br><span class="line">            logits_real = D_net(real_data) <span class="comment"># 判别网络得分</span></span><br><span class="line">            </span><br><span class="line">            sample_noise = (torch.rand(bs, noise_size) - <span class="number">0.5</span>) / <span class="number">0.5</span> <span class="comment"># -1 ~ 1 的均匀分布</span></span><br><span class="line">            g_fake_seed = Variable(sample_noise).cuda()</span><br><span class="line">            fake_images = G_net(g_fake_seed) <span class="comment"># 生成的假的数据</span></span><br><span class="line">            logits_fake = D_net(fake_images) <span class="comment"># 判别网络得分</span></span><br><span class="line"></span><br><span class="line">            d_total_error = discriminator_loss(logits_real, logits_fake) <span class="comment"># 判别器的 loss</span></span><br><span class="line">            D_optimizer.zero_grad()</span><br><span class="line">            d_total_error.backward()</span><br><span class="line">            D_optimizer.step() <span class="comment"># 优化判别网络</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 生成网络</span></span><br><span class="line">            g_fake_seed = Variable(sample_noise).cuda()</span><br><span class="line">            fake_images = G_net(g_fake_seed) <span class="comment"># 生成的假的数据</span></span><br><span class="line"></span><br><span class="line">            gen_logits_fake = D_net(fake_images)</span><br><span class="line">            g_error = generator_loss(gen_logits_fake) <span class="comment"># 生成网络的 loss</span></span><br><span class="line">            G_optimizer.zero_grad()</span><br><span class="line">            g_error.backward()</span><br><span class="line">            G_optimizer.step() <span class="comment"># 优化生成网络</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (iter_count % show_every == <span class="number">0</span>):</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;Iter: &#123;&#125;, D: &#123;:.4&#125;, G:&#123;:.4&#125;&#x27;</span>.<span class="built_in">format</span>(iter_count, d_total_error.data, g_error.data))</span><br><span class="line">                imgs_numpy = deprocess_img(fake_images.data.cpu().numpy())</span><br><span class="line">                show_images(imgs_numpy[<span class="number">0</span>:<span class="number">16</span>])</span><br><span class="line">                plt.show()</span><br><span class="line">                <span class="built_in">print</span>()</span><br><span class="line">            iter_count += <span class="number">1</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">D = discriminator().cuda()</span><br><span class="line">G = generator().cuda()</span><br><span class="line"></span><br><span class="line">D_optim = get_optimizer(D)</span><br><span class="line">G_optim = get_optimizer(G)</span><br><span class="line"></span><br><span class="line">train_a_gan(D, G, D_optim, G_optim, discriminator_loss, generator_loss)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20201105174301-min.gif"></p><p>我们已经完成了一个简单的生成对抗网络，是不是非常容易呢。但是可以看到效果并不是特别好，生成的数字也不是特别完整，因为我们仅仅使用了简单的多层全连接网络。</p><p>除了这种最基本的生成对抗网络之外，还有很多生成对抗网络的变式，有结构上的变式，也有 loss 上的变式，我们先讲一讲其中一种在 loss 上的变式，Least Squares GAN</p><p><strong>Least Squares GAN</strong></p><p><a href="https://arxiv.org/abs/1611.04076">Least Squares GAN</a> 比最原始的 GANs 的 loss 更加稳定，通过名字我们也能够看出这种 GAN 是通过最小平方误差来进行估计，而不是通过二分类的损失函数，下面我们看看 loss 的计算公式</p><p>$$\ell_G  =  \frac{1}{2}\mathbb{E} _ {z \sim p(z)}\left[\left(D(G(z))-1\right)^2\right]$$</p><p>$$ \ell_D = \frac{1}{2}\mathbb{E}_{x \sim p_\text{data}}\left[\left(D(x)-1\right)^2\right] + \frac{1}{2}\mathbb{E} _ {z \sim p(z)}\left[ \left(D(G(z))\right)^2\right]$$</p><p>可以看到 Least Squares GAN 通过最小二乘代替了二分类的 loss，下面我们定义一下 loss 函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ls_discriminator_loss</span>(<span class="params">scores_real, scores_fake</span>):</span></span><br><span class="line">    loss = <span class="number">0.5</span> * ((scores_real - <span class="number">1</span>) ** <span class="number">2</span>).mean() + <span class="number">0.5</span> * (scores_fake ** <span class="number">2</span>).mean()</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ls_generator_loss</span>(<span class="params">scores_fake</span>):</span></span><br><span class="line">    loss = <span class="number">0.5</span> * ((scores_fake - <span class="number">1</span>) ** <span class="number">2</span>).mean()</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">D = discriminator().cuda()</span><br><span class="line">G = generator().cuda()</span><br><span class="line"></span><br><span class="line">D_optim = get_optimizer(D)</span><br><span class="line">G_optim = get_optimizer(G)</span><br><span class="line"></span><br><span class="line">train_a_gan(D, G, D_optim, G_optim, ls_discriminator_loss, ls_generator_loss)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20201105174302-min.gif"></p><p>上面我们讲了 最基本的 GAN 和 least squares GAN，最后我们讲一讲使用卷积网络的 GAN，叫做深度卷积生成对抗网络</p><p><strong>Deep Convolutional GANs</strong></p><p>深度卷积生成对抗网络特别简单，就是将生成网络和对抗网络都改成了卷积网络的形式，下面我们来实现一下</p><p>卷积判别网络就是一个一般的卷积网络，结构如下</p><ul><li>32 Filters, 5x5, Stride 1, Leaky ReLU(alpha=0.01)</li><li>Max Pool 2x2, Stride 2</li><li>64 Filters, 5x5, Stride 1, Leaky ReLU(alpha=0.01)</li><li>Max Pool 2x2, Stride 2</li><li>Fully Connected size 4 x 4 x 64, Leaky ReLU(alpha=0.01)</li><li>Fully Connected size 1</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">build_dc_classifier</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(build_dc_classifier, self).__init__()</span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.01</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, <span class="number">1</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.01</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, <span class="number">1024</span>),</span><br><span class="line">            nn.LeakyReLU(<span class="number">0.01</span>),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, <span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        x = x.view(x.shape[<span class="number">0</span>], -<span class="number">1</span>)</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>卷积生成网络需要将一个低维的噪声向量变成一个图片数据，结构如下</p><ul><li>Fully connected of size 1024, ReLU</li><li>BatchNorm</li><li>Fully connected of size 7 x 7 x 128, ReLU</li><li>BatchNorm</li><li>Reshape into Image Tensor</li><li>64 conv2d^T filters of 4x4, stride 2, padding 1, ReLU</li><li>BatchNorm</li><li>1 conv2d^T filter of 4x4, stride 2, padding 1, TanH</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">build_dc_generator</span>(<span class="params">nn.Module</span>):</span> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, noise_dim=NOISE_DIM</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(build_dc_generator, self).__init__()</span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(noise_dim, <span class="number">1024</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">1024</span>),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, <span class="number">7</span> * <span class="number">7</span> * <span class="number">128</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">7</span> * <span class="number">7</span> * <span class="number">128</span>)</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            nn.ConvTranspose2d(<span class="number">128</span>, <span class="number">64</span>, <span class="number">4</span>, <span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.ReLU(<span class="literal">True</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">            nn.ConvTranspose2d(<span class="number">64</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.Tanh()</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        x = x.view(x.shape[<span class="number">0</span>], <span class="number">128</span>, <span class="number">7</span>, <span class="number">7</span>) <span class="comment"># reshape 通道是 128，大小是 7x7</span></span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_dc_gan</span>(<span class="params">D_net, G_net, D_optimizer, G_optimizer, discriminator_loss, generator_loss, show_every=<span class="number">250</span>, </span></span></span><br><span class="line"><span class="params"><span class="function">                noise_size=<span class="number">96</span>, num_epochs=<span class="number">10</span></span>):</span></span><br><span class="line">    iter_count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> x, _ <span class="keyword">in</span> train_data:</span><br><span class="line">            bs = x.shape[<span class="number">0</span>]</span><br><span class="line">            <span class="comment"># 判别网络</span></span><br><span class="line">            real_data = Variable(x).cuda() <span class="comment"># 真实数据</span></span><br><span class="line">            logits_real = D_net(real_data) <span class="comment"># 判别网络得分</span></span><br><span class="line">            </span><br><span class="line">            sample_noise = (torch.rand(bs, noise_size) - <span class="number">0.5</span>) / <span class="number">0.5</span> <span class="comment"># -1 ~ 1 的均匀分布</span></span><br><span class="line">            g_fake_seed = Variable(sample_noise).cuda()</span><br><span class="line">            fake_images = G_net(g_fake_seed) <span class="comment"># 生成的假的数据</span></span><br><span class="line">            logits_fake = D_net(fake_images) <span class="comment"># 判别网络得分</span></span><br><span class="line"></span><br><span class="line">            d_total_error = discriminator_loss(logits_real, logits_fake) <span class="comment"># 判别器的 loss</span></span><br><span class="line">            D_optimizer.zero_grad()</span><br><span class="line">            d_total_error.backward()</span><br><span class="line">            D_optimizer.step() <span class="comment"># 优化判别网络</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 生成网络</span></span><br><span class="line">            g_fake_seed = Variable(sample_noise).cuda()</span><br><span class="line">            fake_images = G_net(g_fake_seed) <span class="comment"># 生成的假的数据</span></span><br><span class="line"></span><br><span class="line">            gen_logits_fake = D_net(fake_images)</span><br><span class="line">            g_error = generator_loss(gen_logits_fake) <span class="comment"># 生成网络的 loss</span></span><br><span class="line">            G_optimizer.zero_grad()</span><br><span class="line">            g_error.backward()</span><br><span class="line">            G_optimizer.step() <span class="comment"># 优化生成网络</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (iter_count % show_every == <span class="number">0</span>):</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;Iter: &#123;&#125;, D: &#123;:.4&#125;, G:&#123;:.4&#125;&#x27;</span>.<span class="built_in">format</span>(iter_count, d_total_error.data, g_error.data))</span><br><span class="line">                imgs_numpy = deprocess_img(fake_images.data.cpu().numpy())</span><br><span class="line">                show_images(imgs_numpy[<span class="number">0</span>:<span class="number">16</span>])</span><br><span class="line">                plt.show()</span><br><span class="line">                <span class="built_in">print</span>()</span><br><span class="line">            iter_count += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">D_DC = build_dc_classifier().cuda()</span><br><span class="line">G_DC = build_dc_generator().cuda()</span><br><span class="line"></span><br><span class="line">D_DC_optim = get_optimizer(D_DC)</span><br><span class="line">G_DC_optim = get_optimizer(G_DC)</span><br><span class="line"></span><br><span class="line">train_dc_gan(D_DC, G_DC, D_DC_optim, G_DC_optim, discriminator_loss, generator_loss, num_epochs=<span class="number">5</span>)</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20201105174303-min.gif"><br>可以看到，通过 DCGANs 能够得到更加清楚的结果</p><h3 id="6-3-Improving-GAN"><a href="#6-3-Improving-GAN" class="headerlink" title="6.3 Improving GAN"></a>6.3 Improving GAN</h3><h4 id="6-3-1-Wasserstein-GAN"><a href="#6-3-1-Wasserstein-GAN" class="headerlink" title="6.3.1 Wasserstein GAN"></a>6.3.1 Wasserstein GAN</h4><p>Wasserstein GAN是GAN的一种变式，WGAN的出现解决了下面这些难点</p><ul><li>彻底解决了训练不稳定的问题</li><li>基本解决了coolapse mode 的问题，确保了生成样本的多样性</li><li>训练中有一个向交叉熵，准确率的数值指标来衡量训练的进程，数值越小代表GAN训练得越好，同时也代表着生成的图片质量越高</li><li>不需要精心设计网络结构也能取得较好的效果</li></ul><h3 id="6-4-应用介绍"><a href="#6-4-应用介绍" class="headerlink" title="6.4 应用介绍"></a>6.4 应用介绍</h3><h4 id="6-4-1-Conditional-GAN"><a href="#6-4-1-Conditional-GAN" class="headerlink" title="6.4.1 Conditional GAN"></a>6.4.1 Conditional GAN</h4><p>Conditional GAN的一个应用是文字生成图片</p><h4 id="6-4-2-Cycle-GAN"><a href="#6-4-2-Cycle-GAN" class="headerlink" title="6.4.2 Cycle GAN"></a>6.4.2 Cycle GAN</h4><p>根据一个人的作品，想象他完成其他场景会是什么样</p><h2 id="第七章-深度学习实战"><a href="#第七章-深度学习实战" class="headerlink" title="第七章 深度学习实战"></a>第七章 深度学习实战</h2><h3 id="7-1-实例一，猫狗大战：运用预训练卷积神经网络进行特征提取与预训"><a href="#7-1-实例一，猫狗大战：运用预训练卷积神经网络进行特征提取与预训" class="headerlink" title="7.1 实例一，猫狗大战：运用预训练卷积神经网络进行特征提取与预训"></a>7.1 实例一，猫狗大战：运用预训练卷积神经网络进行特征提取与预训</h3><h4 id="7-1-1-背景介绍"><a href="#7-1-1-背景介绍" class="headerlink" title="7.1.1 背景介绍"></a>7.1.1 背景介绍</h4><p>Asirra是一个图像识别机制的验证码，其有很多不同猫狗的照片（三百万张），可以用他的子集当作训练集</p><h4 id="7-1-2-原理分析"><a href="#7-1-2-原理分析" class="headerlink" title="7.1.2 原理分析"></a>7.1.2 原理分析</h4><p>对于这个问题，简单的网络模型可能效果并不好，这个时候，使用一些成熟的模型，比如VggNet，GoogleNet，ResNet等可以帮助我们解决问题，为了节省计算资源和时间，可以通过迁移学习实现。</p><p><strong>迁移学习</strong></p><p>对于一个特定任务，如果没有来自该任务足够的数据集，传统的监督学习无法支持，而迁移学习允许通过借用已经存在的一些相关任务的标签数据来处理这些场景，把解决相关任务时获得的知识存储下来，并将它应用到我们感兴趣的目标任务中。</p><p>卷积神经网络可以理解为两个部分：前面的<strong>卷积</strong>部分和后面的<strong>分类</strong>部分，卷积部分主要用于提取图片特征，而预训练的网络对于特征提取效果已经非常好。我们可以直接用预训练的网络卷积部分来提取我们自己的图片特征，而对于自己的任务，比如猫狗二分类，就用自己的分类全连接层即可。</p><p>当然，迁移学习并不是任何时候都能使用，需要它们<strong>完成的任务是相关的</strong>，所以迁移学习在相似数据集上的应用效果才是良好的。</p><p><strong>实现方法</strong></p><ol><li>第一种方法：导入预训练的卷积网络，将最后的全连接层改成我们自己设计的全连接层，然后更新整个网络，最后能特别快地达到收敛</li><li>第二种方法：锁定前面卷积层的参数，让网络训练只更新最后全连接层的参数，可以使训练时间大大减少</li><li>第三种方法：使用多个预训练好的网络，将它们并联在一起，图片经过每个网络都会得到特征图，我们将这些特征图拼接在一起进入最后的全连接层</li></ol><h4 id="7-1-3-代码实现"><a href="#7-1-3-代码实现" class="headerlink" title="7.1.3 代码实现"></a>7.1.3 代码实现</h4><p>1.数据预处理</p><p>数据集可以去 <a href="https://www.kaggle.com/c/dogs-vs-cats/data">https://www.kaggle.com/c/dogs-vs-cats/data</a> 下载</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"></span><br><span class="line">train_root = <span class="string">&#x27;./data/dogs-vs-cats/train/&#x27;</span></span><br><span class="line">val_root = <span class="string">&#x27;./data/dogs-vs-cats/val/&#x27;</span></span><br><span class="line">data_file=os.listdir(train_root)</span><br><span class="line"><span class="comment">#print(data_file)</span></span><br><span class="line">dog_file = <span class="built_in">list</span>(<span class="built_in">filter</span>(<span class="keyword">lambda</span> x:x.split(<span class="string">&quot;.&quot;</span>)[<span class="number">0</span>]==<span class="string">&#x27;dog&#x27;</span><span class="keyword">and</span> x!=<span class="string">&quot;dog&quot;</span>,data_file))</span><br><span class="line">cat_file = <span class="built_in">list</span>(<span class="built_in">filter</span>(<span class="keyword">lambda</span> x:x.split(<span class="string">&quot;.&quot;</span>)[<span class="number">0</span>]==<span class="string">&#x27;cat&#x27;</span><span class="keyword">and</span> x!=<span class="string">&quot;cat&quot;</span>,data_file))</span><br><span class="line"></span><br><span class="line">root = <span class="string">&#x27;./data/dogs-vs-cats/&#x27;</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(train_root+<span class="string">&#x27;dog/&#x27;</span>):</span><br><span class="line">    os.makedirs(train_root+<span class="string">&#x27;dog/&#x27;</span>)</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(train_root+<span class="string">&#x27;cat/&#x27;</span>):</span><br><span class="line">    os.makedirs(train_root+<span class="string">&#x27;cat/&#x27;</span>)</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(val_root+<span class="string">&#x27;dog/&#x27;</span>):</span><br><span class="line">    os.makedirs(val_root+<span class="string">&#x27;dog/&#x27;</span>)</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(val_root+<span class="string">&#x27;cat/&#x27;</span>):</span><br><span class="line">    os.makedirs(val_root+<span class="string">&#x27;cat/&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(dog_file)):</span><br><span class="line">    pic_path = root+<span class="string">&#x27;train/&#x27;</span>+dog_file[i]</span><br><span class="line">    <span class="keyword">if</span> i &lt; <span class="built_in">len</span>(dog_file)*<span class="number">0.9</span>:</span><br><span class="line">        obj_path = train_root+<span class="string">&#x27;dog/&#x27;</span>+dog_file[i]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        obj_path = val_root+<span class="string">&#x27;dog/&#x27;</span>+dog_file[i]</span><br><span class="line">    shutil.move(pic_path,obj_path)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(cat_file)):</span><br><span class="line">    pic_path = root+<span class="string">&#x27;train/&#x27;</span>+cat_file[i]</span><br><span class="line">    <span class="keyword">if</span> i &lt; <span class="built_in">len</span>(cat_file)*<span class="number">0.9</span>:</span><br><span class="line">        obj_path = train_root+<span class="string">&#x27;cat/&#x27;</span>+cat_file[i]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        obj_path = val_root+<span class="string">&#x27;cat/&#x27;</span>+cat_file[i]</span><br><span class="line">    shutil.move(pic_path,obj_path)</span><br></pre></td></tr></table></figure><p>上面的操作实现了，将猫狗照片分别移动到训练集和验证集，其中90%的数据作为训练集，10%的图片作为验证集，使用<code>shutil.move()</code>来移动图片</p><p>2.迁移学习模型训练</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> models,transforms,datasets</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">img_classes=<span class="number">2</span></span><br><span class="line">epoch_num = <span class="number">2</span></span><br><span class="line">path = <span class="string">&quot;./data/dogs-vs-cats/&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#数据</span></span><br><span class="line">data_transform = transforms.Compose([</span><br><span class="line">    transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize([<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>],[<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>])</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># ImageFOLDER 返回的是一个list，这里的写法是字典的形式</span></span><br><span class="line">data_image = &#123;x: datasets.ImageFolder(root=os.path.join(path, x),transform=data_transform) <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">&quot;train&quot;</span>, <span class="string">&quot;val&quot;</span>]&#125;</span><br><span class="line">data_loader_image = &#123;x: DataLoader(dataset=data_image[x],batch_size=<span class="number">4</span>,shuffle=<span class="literal">True</span>) <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">&quot;train&quot;</span>, <span class="string">&quot;val&quot;</span>]&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分类</span></span><br><span class="line">classes = data_image[<span class="string">&quot;train&quot;</span>].classes <span class="comment"># 按文件夹名字分类</span></span><br><span class="line">classes_index = data_image[<span class="string">&quot;train&quot;</span>].class_to_idx <span class="comment"># 文件夹类名所对应的链值</span></span><br><span class="line"><span class="comment"># 打印类别</span></span><br><span class="line"><span class="built_in">print</span>(classes) </span><br><span class="line"><span class="built_in">print</span>(classes_index)</span><br><span class="line"><span class="comment"># 打印训练集，验证集大小</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;train data set:&quot;</span>, <span class="built_in">len</span>(data_image[<span class="string">&quot;train&quot;</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;val data set:&quot;</span>, <span class="built_in">len</span>(data_image[<span class="string">&quot;val&quot;</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment">#导入预训练的网络，并修改全连接层</span></span><br><span class="line">model = models.resnet18(pretrained=<span class="literal">True</span>) <span class="comment"># 18层的残差网络</span></span><br><span class="line"><span class="comment">#print(model)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> parma <span class="keyword">in</span> model.parameters():</span><br><span class="line">    parma.requires_grad = <span class="literal">False</span>  <span class="comment"># 不进行梯度更新</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 改变模型的全连接层，本项目只需要输出2类</span></span><br><span class="line">model.fc = nn.Sequential(nn.Linear(<span class="number">512</span>, <span class="number">256</span>),</span><br><span class="line">                                       nn.ReLU(),</span><br><span class="line">                                       nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">                                       nn.Linear(<span class="number">256</span>, <span class="number">256</span>),</span><br><span class="line">                                       nn.ReLU(),</span><br><span class="line">                                       nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">                                       nn.Linear(<span class="number">256</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> index, parma <span class="keyword">in</span> <span class="built_in">enumerate</span>(model.fc.parameters()):</span><br><span class="line">    parma.requires_grad = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 是否有GPU</span></span><br><span class="line">use_gpu = torch.cuda.is_available()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Find GPU: &quot;</span>,use_gpu)</span><br><span class="line"><span class="keyword">if</span> use_gpu:</span><br><span class="line">    model = model.cuda()</span><br><span class="line"><span class="comment">#print(model)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义代价函数</span></span><br><span class="line">cost = torch.nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">optimizer = torch.optim.Adam(model.fc.parameters(),lr=<span class="number">1e-4</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>():</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epoch_num):</span><br><span class="line">        since = time.time()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Epoch&#123;&#125;/&#123;&#125;&quot;</span>.<span class="built_in">format</span>(epoch+<span class="number">1</span>, epoch_num))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;-&quot;</span> * <span class="number">10</span>)</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> [<span class="string">&quot;train&quot;</span>, <span class="string">&quot;val&quot;</span>]:</span><br><span class="line">            <span class="keyword">if</span> param == <span class="string">&quot;train&quot;</span>:</span><br><span class="line">                model.train = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                model.train = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line">            running_correct = <span class="number">0</span></span><br><span class="line">            batch = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> data <span class="keyword">in</span> data_loader_image[param]:</span><br><span class="line">                batch += <span class="number">1</span></span><br><span class="line">                X, y = data</span><br><span class="line">                <span class="keyword">if</span> use_gpu:</span><br><span class="line">                    X, y = Variable(X.cuda()), Variable(y.cuda())</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    X, y = Variable(X), Variable(y)</span><br><span class="line"></span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line">                y_pred = model(X)</span><br><span class="line">                _, pred = torch.<span class="built_in">max</span>(y_pred.data, <span class="number">1</span>)</span><br><span class="line">                loss = cost(y_pred,y)</span><br><span class="line">                <span class="keyword">if</span> param == <span class="string">&quot;train&quot;</span>:</span><br><span class="line">                    loss.backward()</span><br><span class="line">                    optimizer.step()</span><br><span class="line">                running_loss += loss.item()</span><br><span class="line">                <span class="comment"># running_loss += loss.data</span></span><br><span class="line">                running_correct += torch.<span class="built_in">sum</span>(pred == y.data)</span><br><span class="line">                <span class="keyword">if</span> batch % <span class="number">5</span> == <span class="number">0</span> <span class="keyword">and</span> param == <span class="string">&quot;train&quot;</span>:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&quot;Batch &#123;&#125;, Train Loss:&#123;:.4f&#125;, Train ACC:&#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">                        batch, running_loss / (<span class="number">4</span> * batch), <span class="number">100</span> * running_correct / (<span class="number">4</span> * batch)))</span><br><span class="line"></span><br><span class="line">            epoch_loss = running_loss / <span class="built_in">len</span>(data_image[param])</span><br><span class="line">            epoch_correct = <span class="number">100</span> * running_correct / <span class="built_in">len</span>(data_image[param])</span><br><span class="line"></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;&#123;&#125; Loss:&#123;:.4f&#125;, Correct:&#123;:.4f&#125;&quot;</span>.<span class="built_in">format</span>(param, epoch_loss, epoch_correct))</span><br><span class="line">        now_time = time.time() - since</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Training time is:&#123;:.0f&#125;m &#123;:.0f&#125;s&quot;</span>.<span class="built_in">format</span>(now_time // <span class="number">60</span>, now_time % <span class="number">60</span>))</span><br><span class="line"></span><br><span class="line">train()</span><br><span class="line">torch.save(model, <span class="string">&#x27;dogsvscats.pth&#x27;</span>)</span><br></pre></td></tr></table></figure><p>测试</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms, models</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">model = torch.load(<span class="string">&#x27;dogsvscats.pth&#x27;</span>)</span><br><span class="line">path = <span class="string">&quot;./data/dogs-vs-cats&quot;</span></span><br><span class="line"></span><br><span class="line">transform = transforms.Compose([transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">                                transforms.ToTensor(),</span><br><span class="line">                                transforms.Normalize([<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>], [<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>])])</span><br><span class="line"></span><br><span class="line">data_test_img = datasets.ImageFolder(root=path+<span class="string">&quot;/val/&quot;</span>, transform = transform) </span><br><span class="line"></span><br><span class="line">data_loader_test_img = torch.utils.data.DataLoader(dataset=data_test_img,</span><br><span class="line">                                                  batch_size = <span class="number">16</span>,shuffle=<span class="literal">True</span>) <span class="comment">#载入测试数据集，并随机打乱</span></span><br><span class="line">classes = data_test_img.classes   <span class="comment">##class</span></span><br><span class="line"></span><br><span class="line">image, label = <span class="built_in">next</span>(<span class="built_in">iter</span>(data_loader_test_img))</span><br><span class="line">images = Variable(image).cuda()</span><br><span class="line">y_pred = model(images)</span><br><span class="line">_,pred = torch.<span class="built_in">max</span>(y_pred.data, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(pred)</span><br><span class="line"><span class="built_in">print</span>(label)</span><br><span class="line"></span><br><span class="line">img = torchvision.utils.make_grid(image)</span><br><span class="line">img = img.numpy().transpose(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)</span><br><span class="line">mean = [<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>]</span><br><span class="line">std = [<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>]</span><br><span class="line">img = img * std + mean</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Pred Label:&quot;</span>, [classes[i] <span class="keyword">for</span> i <span class="keyword">in</span> pred])</span><br><span class="line">plt.imshow(img)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h3 id="7-2-实例二，Deep-Dream：探索卷积神经网络眼中的世界"><a href="#7-2-实例二，Deep-Dream：探索卷积神经网络眼中的世界" class="headerlink" title="7.2 实例二，Deep Dream：探索卷积神经网络眼中的世界"></a>7.2 实例二，Deep Dream：探索卷积神经网络眼中的世界</h3><p>2015年，Google发布了一个有意思的东西，叫做Deep Dream</p><h4 id="7-2-1-原理介绍"><a href="#7-2-1-原理介绍" class="headerlink" title="7.2.1 原理介绍"></a>7.2.1 原理介绍</h4><p><strong>1.反向神经网络</strong></p><p>我们知道经过训练之后，每一层网络足部提取越来越高级的图像特征，知道最后一层将这些特征比较做出分类的结果。比如前面几层也许在寻找边缘和拐角的特征，中间几层分析整体的轮廓特征，这样不断的增加层数就可以发展出越来越多的复杂特征，最后几层将这些特征要素组合起来形成完整的解释，这样到最后网络就会对非常复杂的东西，比如小猫，树叶等图片有所反应</p><p><strong>2.Deep Dream</strong></p><p>如果我们将算法反复地应用到自身的输出上，不断迭代，并在每次迭代后应用一些缩放，就能不断地激活特征，得到无尽的新效果。</p><h4 id="7-2-2-代码实现"><a href="#7-2-2-代码实现" class="headerlink" title="7.2.2 代码实现"></a>7.2.2 代码实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms, utils</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="comment"># PIL.ImageFilter是Python中的图像滤波，主要对图像进行平滑、锐化、边界增强等滤波处理</span></span><br><span class="line"><span class="comment"># PIL.ImageChops模块包含一些算术图形操作，叫做channel operations（“chops”）。这些操作可用于诸多目的，比如图像特效，图像组合，算法绘图等等</span></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image, ImageFilter, ImageChops</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载图像并显示</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_image</span>(<span class="params">path</span>):</span></span><br><span class="line">    image = Image.<span class="built_in">open</span>(path)</span><br><span class="line">    plt.imshow(image)</span><br><span class="line">    plt.title(<span class="string">&quot;Image loaded successfully&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> image</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对数据集的标准化设置——减去均值再除以标准差</span></span><br><span class="line">normalise = transforms.Normalize(</span><br><span class="line">    mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>],</span><br><span class="line">    std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集的预处理，包括缩放、转换成Tensor、标准化</span></span><br><span class="line">preprocess = transforms.Compose([</span><br><span class="line">    transforms.Resize((<span class="number">224</span>,<span class="number">224</span>)),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    normalise</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 逆向处理过程，逆标准化，图像乘以标准差再加上均值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deprocess</span>(<span class="params">image</span>):</span></span><br><span class="line">    <span class="keyword">return</span> image * torch.Tensor([<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]).cuda()  + torch.Tensor([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>]).cuda()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载vgg16的预训练模型，传到GPU上，输出网络结构</span></span><br><span class="line">vgg = models.vgg16(pretrained=<span class="literal">True</span>)</span><br><span class="line">vgg = vgg.cuda()</span><br><span class="line">modulelist = <span class="built_in">list</span>(vgg.features.modules())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这是deep dream的实际代码，特定层的梯度被设置为等于该层的响应，这导致了该层响应最大化。换句话说，我们正在增强一层检测到的特征，对输入图像（octaves）应用梯度上升算法。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dd_helper</span>(<span class="params">image, layer, iterations, lr</span>):</span>        </span><br><span class="line">    <span class="comment"># 一开始的输入是图像经过预处理、在正数第一个维度上增加一个维度以匹配神经网络的输入、传到GPU上</span></span><br><span class="line">    <span class="built_in">input</span> = Variable(preprocess(image).unsqueeze(<span class="number">0</span>).cuda(), requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># vgg梯度清零</span></span><br><span class="line">    vgg.zero_grad()</span><br><span class="line">    <span class="comment"># 开始迭代</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iterations):</span><br><span class="line">        <span class="comment"># 一层一层传递输入</span></span><br><span class="line">        out = <span class="built_in">input</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(layer):</span><br><span class="line">            out = modulelist[j+<span class="number">1</span>](out)</span><br><span class="line">        <span class="comment"># 损失是输出的范数</span></span><br><span class="line">        loss = out.norm()</span><br><span class="line">        <span class="comment"># 损失反向传播</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="comment"># 输入的数据是上次迭代时的输入数据+学习率×输入的梯度</span></span><br><span class="line">        <span class="built_in">input</span>.data = <span class="built_in">input</span>.data + lr * <span class="built_in">input</span>.grad.data</span><br><span class="line">    <span class="comment"># 将从网络结构中取出的输入数据的第一个维度去掉</span></span><br><span class="line">    <span class="built_in">input</span> = <span class="built_in">input</span>.data.squeeze()</span><br><span class="line">    <span class="comment"># 矩阵转置</span></span><br><span class="line">    <span class="built_in">input</span>.transpose_(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">input</span>.transpose_(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">    <span class="comment"># 将输入逆标准化后强制截断在0到1的范围内</span></span><br><span class="line">    <span class="built_in">input</span> = np.clip(deprocess(<span class="built_in">input</span>), <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 得到像素值为0到255的图像</span></span><br><span class="line">    im = Image.fromarray(np.uint8(<span class="built_in">input</span>*<span class="number">255</span>))</span><br><span class="line">    <span class="keyword">return</span> im</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这是一个递归函数，用于创建octaves，并且将由一次递归调用生成的图像与由上一级递归调用生成的图像相融合</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deep_dream_vgg</span>(<span class="params">image, layer, iterations, lr, octave_scale, num_octaves</span>):</span></span><br><span class="line">    <span class="comment"># 若octave序号大于0，即还未到达最底层的octave时，一层一层递归</span></span><br><span class="line">    <span class="keyword">if</span> num_octaves&gt;<span class="number">0</span>:</span><br><span class="line">        <span class="comment"># 对图像进行高斯滤波（高斯模糊）</span></span><br><span class="line">        image1 = image.<span class="built_in">filter</span>(ImageFilter.GaussianBlur(<span class="number">2</span>))</span><br><span class="line">        <span class="comment"># 判断是否缩放</span></span><br><span class="line">        <span class="keyword">if</span>(image1.size[<span class="number">0</span>]/octave_scale &lt; <span class="number">1</span> <span class="keyword">or</span> image1.size[<span class="number">1</span>]/octave_scale&lt;<span class="number">1</span>):</span><br><span class="line">            size = image1.size</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            size = (<span class="built_in">int</span>(image1.size[<span class="number">0</span>]/octave_scale), <span class="built_in">int</span>(image1.size[<span class="number">1</span>]/octave_scale))</span><br><span class="line">        <span class="comment"># 图像缩放    </span></span><br><span class="line">        image1 = image1.resize(size,Image.ANTIALIAS)</span><br><span class="line">        <span class="comment"># 递归调用，直至num_octave==0</span></span><br><span class="line">        image1 = deep_dream_vgg(image1, layer, iterations, lr, octave_scale, num_octaves-<span class="number">1</span>)</span><br><span class="line">        size = (image.size[<span class="number">0</span>], image.size[<span class="number">1</span>])</span><br><span class="line">        <span class="comment"># 将图像缩放到最初输入图像的大小</span></span><br><span class="line">        image1 = image1.resize(size,Image.ANTIALIAS)</span><br><span class="line">        <span class="comment"># 将最初输入的图像与合成的相同尺寸大小的图像融合</span></span><br><span class="line">        image = ImageChops.blend(image, image1, <span class="number">0.6</span>)</span><br><span class="line"><span class="comment">#     print(&quot;-------------- Recursive level: &quot;, num_octaves, &#x27;--------------&#x27;)</span></span><br><span class="line">    <span class="comment"># 按照dd_helper中的流程生成图像</span></span><br><span class="line">    img_result = dd_helper(image, layer, iterations, lr)</span><br><span class="line">    <span class="comment"># 图像缩放并显示</span></span><br><span class="line">    img_result = img_result.resize(image.size)</span><br><span class="line">    plt.imshow(img_result)</span><br><span class="line">    <span class="keyword">return</span> img_result</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 加载图像(原始图像)</span></span><br><span class="line">sky = load_image(<span class="string">&#x27;1.jpg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对于vgg16最后一个卷积层conv5_3,迭代5次，学习率为0.2,octave缩放比例为2,octave从第20层开始</span></span><br><span class="line">sky_28 = deep_dream_vgg(sky, <span class="number">28</span>, <span class="number">5</span>, <span class="number">0.2</span>, <span class="number">2</span>, <span class="number">20</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;深度学习入门之PyTorch&quot;&gt;&lt;a href=&quot;#深度学习入门之PyTorch&quot; class=&quot;headerlink&quot; title=&quot;深度学习入门之PyTorch&quot;&gt;&lt;/a&gt;深度学习入门之PyTorch&lt;/h1&gt;&lt;h2 id=&quot;第一章-深度学习介绍&quot;&gt;&lt;a hr</summary>
      
    
    
    
    <category term="人工智能" scheme="https://blog.justlovesmile.top/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
    <category term="python" scheme="https://blog.justlovesmile.top/tags/python/"/>
    
    <category term="深度学习" scheme="https://blog.justlovesmile.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="pytorch" scheme="https://blog.justlovesmile.top/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>深度学习 | “花书”，Deep Learning笔记</title>
    <link href="https://blog.justlovesmile.top/posts/43678.html"/>
    <id>https://blog.justlovesmile.top/posts/43678.html</id>
    <published>2020-10-09T08:26:12.000Z</published>
    <updated>2020-10-09T08:26:12.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h1><h3 id="第一章-前言"><a href="#第一章-前言" class="headerlink" title="第一章 前言"></a>第一章 前言</h3><p>深度学习(Deep Learning) ∈ 表示学习(Representation Learning) ∈ 机器学习(Machine Learning) ∈ 人工智能(AI)</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20201015120220.png"></p><p>分类，回归，聚类，降维</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20201015120404.png"></p><p>神经网络</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/149537710807280383.jpg"></p><h2 id="第一部分-应用数学与机器学习基础"><a href="#第一部分-应用数学与机器学习基础" class="headerlink" title="第一部分 应用数学与机器学习基础"></a>第一部分 应用数学与机器学习基础</h2><h3 id="第二章-线性代数"><a href="#第二章-线性代数" class="headerlink" title="第二章 线性代数"></a>第二章 线性代数</h3><h4 id="2-1-标量，向量，矩阵和张量"><a href="#2-1-标量，向量，矩阵和张量" class="headerlink" title="2.1 标量，向量，矩阵和张量"></a>2.1 标量，向量，矩阵和张量</h4><p>1.标量(scalar)是一个单独的数<br>2.向量(vector)是一列数，这些数是有序排列的</p><p>$$<br>x = \begin{bmatrix}<br>x_1\<br>x_2\<br>\vdots\<br>x_n\<br>\end{bmatrix}<br>$$</p><p>3.矩阵(matrix)是一个二维数组</p><p>$$<br>\begin{bmatrix}<br>A_{1,1} &amp; A_{1,2}\<br>B_{2,1} &amp; B_{2,2}\<br>\end{bmatrix}<br>$$</p><p>4.张量(tensor)：一般将超过二维的数组称为张量</p><ul><li>转置：以对角线为轴的镜像</li></ul><p>$$<br>(A^T)<em>{i,j}=A</em>{j,i}<br>$$</p><ul><li>广播：将一个向量隐式的复制到每一行生成矩阵（用于运算）的方式</li></ul><p>$$C=A+b$$<br>$$C_{i,j}=A_{i,j}+b$$<br>$$<br>\begin{bmatrix}<br>C_{1,1} &amp; C_{1,2} &amp; C_{1,3}\<br>C_{2,1} &amp; C_{2,2} &amp; C_{2,3}\<br>C_{3,1} &amp; C_{3,2} &amp; C_{3,3}\<br>\end{bmatrix}=<br>\begin{bmatrix}<br>A_{1,1} &amp; A_{1,2} &amp; A_{1,3}\<br>A_{2,1} &amp; A_{2,2} &amp; A_{2,3}\<br>A_{3,1} &amp; A_{3,2} &amp; A_{3,3}\<br>\end{bmatrix}+<br>\begin{bmatrix}<br>b_{1} &amp; b_{2} &amp; b_{3}\<br>\end{bmatrix}=<br>\begin{bmatrix}<br>A_{1,1} &amp; A_{1,2} &amp; A_{1,3}\<br>A_{2,1} &amp; A_{2,2} &amp; A_{2,3}\<br>A_{3,1} &amp; A_{3,2} &amp; A_{3,3}\<br>\end{bmatrix}+<br>\begin{bmatrix}<br>b_{1} &amp; b_{2} &amp; b_{3}\<br>b_{1} &amp; b_{2} &amp; b_{3}\<br>b_{1} &amp; b_{2} &amp; b_{3}\<br>\end{bmatrix}<br>$$</p><h4 id="2-2-矩阵和向量相乘"><a href="#2-2-矩阵和向量相乘" class="headerlink" title="2.2 矩阵和向量相乘"></a>2.2 矩阵和向量相乘</h4><p>如果A的形状是m×n，B的形状是n×p，<code>C=AB</code>，那么C的形状是m×p<br>具体地，该乘法操作定义为：<br>$$C_{i,j}=\sum_kA_{i,k}B_{k,j}$$</p><blockquote><p>注意区分：存在一种两个矩阵对应元素的乘积，叫<code>Hadamard乘积</code>，记为<code>A⊙B</code></p></blockquote><p>性质：</p><ul><li>分配律<br>$$A(B+C)=AB+AC$$</li><li>结合律<br>$$A(BC)=(AB)C$$</li><li><strong>矩阵乘积不具有交换律</strong>，但是两个向量的点积满足交换律<br>$$x^Ty=y^Tx$$<br>$$(AB)^T=B^TA^T$$</li></ul><p>对于AX=b，可表示<br>$$A_{1,1}x_1+A_{1,2}x_2+\cdots+A_{1,n}x_n=b_1$$<br>$$A_{2,1}x_1+A_{2,2}x_2+\cdots+A_{2,n}x_n=b_2$$<br>$$\vdots$$<br>$$A_{m,1}x_1+A_{m,2}x_2+\cdots+A_{m,n}x_n=b_m$$<br>即<br>$$<br>\begin{bmatrix}<br>A_{1,1} &amp; A_{1,2} &amp; \cdots &amp; A_{1,n}\<br>A_{2,1} &amp; A_{2,2} &amp; \cdots &amp; A_{2,n}\<br>\vdots\<br>A_{m,1} &amp; A_{m,2} &amp; \cdots &amp; A_{m,n}\<br>\end{bmatrix}<br>\begin{bmatrix}<br>x_{1}\<br>x_{2}\<br>\vdots\<br>x_{n}\<br>\end{bmatrix}=<br>\begin{bmatrix}<br>b_{1}\<br>b_{2}\<br>\vdots\<br>b_{m}\<br>\end{bmatrix}<br>$$</p><h4 id="2-3-单位矩阵和逆矩阵"><a href="#2-3-单位矩阵和逆矩阵" class="headerlink" title="2.3 单位矩阵和逆矩阵"></a>2.3 单位矩阵和逆矩阵</h4><p>单位矩阵：任何向量和单位矩阵相乘，都不会改变<br>$${\forall}x{\in}R^n,I_nx=x$$</p><p>$$I_3=<br>\begin{bmatrix}<br>1 &amp; 0 &amp; 0\<br>0 &amp; 1 &amp; 0\<br>0 &amp; 0 &amp; 1\<br>\end{bmatrix}<br>$$<br>矩阵逆，记作$$A^{-1}$$，定义<br>$$A^{-1}A=I_n$$</p><h4 id="2-4-线性相关和生成子空间"><a href="#2-4-线性相关和生成子空间" class="headerlink" title="2.4 线性相关和生成子空间"></a>2.4 线性相关和生成子空间</h4><p>对于矩阵逆的定义：<br>$$A^{-1}A=I_n$$<br>可以通过以下步骤求解：<br>$$Ax=b$$<br>$$A^{-1}Ax=A^{-1}b$$<br>$$I_nx=A^{-1}b$$<br>$$x=A^{-1}b$$<br>如果能找到一个逆矩阵$A^{-1}$，那么若$s=w_1x_1+w_2x_2+{\cdots}+w_kx_k$。s变量是对变量x的加权线性”混合”。因此，将s定义为变量的线性组合。</p><ul><li><p>生成子空间：原始向量的一切线性组合生成的子空间</p></li><li><p>线性无关：一组向量中的任意一个向量都不能表示成其他向量的线性组合，那么这组向量被称为线性无关</p></li></ul><blockquote><p>如果数组向量中的某一个或多个向量可以由数组内的其余向量通过加法或数乘表达，则该向量组线性相关，反之则线性无关。</p></blockquote><ul><li>方阵：行列大小相同的矩阵</li></ul><h4 id="2-5-范数"><a href="#2-5-范数" class="headerlink" title="2.5 范数"></a>2.5 范数</h4><p>为了衡量一个向量的大小，在机器学习中，我们经常使用范数（norm）的函数衡量。形式上，$L^p$范数定义如下：<br>$$<br>\left | x \right | _ p=\left ( \sum_i\left | x_i \right |^p\right )^\frac{1}{p}<br>$$<br>其中$p \in \mathbb{R},p \geqslant 1$</p><ul><li>范数，是将向量映射到非负值的函数。直观上来说，向量x的范数衡量从原点到点x的距离。更严格的说，范数是满足下列性质的任意函数：<br>$$f\left (x \right )=0\Rightarrow x=0$$<br>$$<br>f\left (x + y \right )\leqslant f \left ( x \right ) + f\left(y \right)<br>$$</li></ul><p>$$<br>\forall \alpha \in \mathbb{R},f({\alpha}x)=|\alpha|f(x)<br>$$</p><ul><li><p>当p=2时，$L^2$范数被称为<strong>欧几里得范数</strong>。平方$L^2$范数，也经常用来衡量向量的大小。</p></li><li><p>$L^1$范数的定义，$||x|| _ 1 = \sum_i|x_i|$</p></li><li><p>另一个常在机器学习中出现的范数是$L^∞$范数，也被称为<strong>最大范数</strong>。这个范数表示向量中具有最大幅值的元素的绝对值：<br>$$||x|| _ ∞ =\max_i|x_i|$$</p></li><li><p>Frobenius范数：$$\left |A\right | _ F = \sqrt{\sum _ {i,j}{A^2} _ {i,j}}$$</p></li></ul><p>两个向量的<strong>点积</strong>，可以用范数来表示：<br>$$x^Ty=||x|| _ 2 ||y|| _ 2 \cos \theta $$<br>其中$\theta$，表示x和y之间的夹角</p><h4 id="2-6-特殊类型的矩阵和向量"><a href="#2-6-特殊类型的矩阵和向量" class="headerlink" title="2.6 特殊类型的矩阵和向量"></a>2.6 特殊类型的矩阵和向量</h4><p><strong>对角矩阵</strong>:只在主对角线上含有非零元素，其他位置都是零的矩阵。单位矩阵是对角元素都为1的对角矩阵。</p><ul><li>如果用diag(v)表示一个对角元素由向量v中的元素给定的对角方阵,那么$diag(v)x=v\odot x$，并且计算对角方阵的逆矩阵也很高效，如果对角方阵的逆矩阵存在，当且仅当对角元素都是非零值，这种情况下有，$diag(v)^{-1}=diag([\frac{1}{v_1},\cdots,\frac{1}{v_n}]^T)$</li><li>不是所有的对角矩阵都是方阵，非方阵的对角矩阵没有逆矩阵</li></ul><p><strong>对称矩阵</strong>是转置和自己相等的矩阵<br>$$A=A^T$$</p><p><strong>单位向量</strong>是具有<strong>单位范数</strong>的向量<br>$$||x|| _ 2 = 1$$</p><p><strong>正交</strong>：如果$x^Ty=0$，那么向量x和向量y互相正交，如果两个向量都有非零范数，那么他们之间的夹角为90度。<br><strong>标准正交</strong>：如果这些向量的不仅互相正交，并且范数都为1，那么我们称他们为标准正交<br><strong>正交矩阵</strong>：指行向量和列向量是分别标准正交的方针<br>$$A^TA=AA^T=I$$<br>$$A^{-1}=A^T$$</p><blockquote><p>补充关于点乘和叉乘：<br>点乘，也叫数量积，结果是一个向量在另一个向量方向上的投影的长度，是一个标量；<br>$A·B=|A||B|\cos\theta$，点积为0，说明两个向量正交<br>叉乘，也叫向量积，结果是一个和已有两个向量都垂直的向量，向量模长是向量A，B组成平行四边形的面积，即$\left | A×B\right|=\left|A \right|\left|B \right|\sin\theta$；向量方向垂直于向量A,B组成的平面；<br>$$A×B=\begin{vmatrix}<br>i &amp; j &amp; k\<br>a_1 &amp; a_2 &amp; a_3\<br>b_1 &amp; b_2 &amp; b_3<br>\end{vmatrix}=<br>\begin{vmatrix}<br>a_2 &amp; a_3\<br>b_2 &amp; b_3<br>\end{vmatrix}i-<br>\begin{vmatrix}<br>a_1 &amp; a_3\<br>b_1 &amp; b_3<br>\end{vmatrix}j+<br>\begin{vmatrix}<br>a_1 &amp; a_2\<br>b_1 &amp; b_2<br>\end{vmatrix}k$$<br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/v2-1bc29eef9c32d93b6b9d6604b0ce65ea_720w.png"></p></blockquote><h4 id="2-7-特征分解"><a href="#2-7-特征分解" class="headerlink" title="2.7 特征分解"></a>2.7 特征分解</h4><p><strong>特征分解</strong>是使用最广的矩阵分解之一，即我们将矩阵分解成一组特征向量和特征值</p><ul><li><strong>方阵A</strong>的特征向量是指与A相乘后相当于对该向量进行缩放的非零向量v：$Av=λv$</li></ul><p>$({\lambda}E-A)v=0$，求特征值$\lambda$，其中$|{\lambda}E-A|$被称为特征多项式</p><ul><li>标量λ被称为这个特征向量对应的特征值。如果v是A的特征向量，那么任何缩放后的向量sv（s∈R，s≠0）也是A的特征向量。此外sv和v有相同的特征值。</li></ul><p>假设矩阵A有n个线性无关的特征向量</p><p>$$v^{(1)},\cdots,v^{(n)}$$</p><p>对应着特征值</p><p>$${\lambda} _ 1,\cdots,{\lambda} _ n$$</p><p>将特征向量连接成一个矩阵，使得每一列是一个特征向量：</p><p>$$V=[v^{(1)},\cdots,v^{(n)}]$$</p><p>类似地，将特征值连接成一个向量</p><p>$$\lambda =[{\lambda} _ 1,\cdots,{\lambda} _ n]^T$$</p><p>因此A的特征分解可以记作：</p><p>$$A=Vdiag(\lambda)V^{-1}$$</p><ul><li>特征分解唯一当且仅当所有的特征值都是唯一的</li><li>矩阵是奇异的当且仅当含有零特征值（奇异：非满秩）</li><li>正定（所有特征值为正），半正定（所有特征值非负），负定（所有特征值为负），半负定（所有特征值非正）</li></ul><p>对于每个实对称矩阵，都可以分解成实特征向量和实特征值<br>$$A=QΛQ^T$$<br>其中Ｑ是Ａ的特征向量组成的正交矩阵，Λ是对角矩阵。特征值$Λ_{i,j}$对应的特征向量是矩阵Q的第i列，记作$Q_{:,j}$</p><h4 id="2-8-奇异值分解"><a href="#2-8-奇异值分解" class="headerlink" title="2.8 奇异值分解"></a>2.8 奇异值分解</h4><p><a href="https://www.bilibili.com/video/BV1N4411a78K?from=search&seid=2326855606635926731">奇异值分解视频</a></p><p>奇异值分解：将矩阵分解为<strong>奇异向量</strong>和<strong>奇异值</strong><br>假设A是一个m×n的矩阵，那么U是一个m×m的矩阵，D是一个m×n的矩阵，V是一个n×n的矩阵，其中U和V是正交矩阵，D是对角矩阵（不一定是方阵）<br>$$A=UDV^T$$<br>对角矩阵D对角线上的元素被称为矩阵A的<strong>奇异值</strong>，矩阵U的列向量被称为<strong>左奇异向量</strong>(是$AA^T$的特征向量)，矩阵V的列向量被称为<strong>右奇异向量</strong>（是$A^TA$的特征向量）。A的非零奇异值是$A^TA$特征值的平方根，也是$AA^T$特征值的平方根</p><p>正交-对角-正交：旋转-拉伸-旋转</p><p>$$A^TA=(VD^TU^T)(UDV^T)=V(D^TD)V^T$$<br>$$AA^T=(UDV^T)(VD^TU^T)=U(D^TD)U^T$$<br>$$A=<br>\begin{bmatrix}<br>u_1 &amp; u_2<br>\end{bmatrix}<br>\begin{bmatrix}<br>\sigma_1 &amp; \<br> &amp; \sigma_2<br>\end{bmatrix}<br>\begin{bmatrix}<br>v_1^T\<br>v_2^T<br>\end{bmatrix}<br>$$</p><p>例子：</p><p>$$A=\begin{bmatrix}<br>2 &amp; 2\<br>1 &amp; 1<br>\end{bmatrix}=<br>\begin{bmatrix}<br>\frac{2}{\sqrt{5}} &amp; \frac{1}{\sqrt{5}}\<br>\frac{-1}{\sqrt{5}} &amp; \frac{2}{\sqrt{5}}<br>\end{bmatrix}<br>\begin{bmatrix}<br>\sqrt{10} &amp; \<br> &amp; 0<br>\end{bmatrix}<br>\begin{bmatrix}<br>\frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}}\<br>\frac{1}{\sqrt{2}} &amp; \frac{-1}{\sqrt{2}}<br>\end{bmatrix}<br>$$</p><h4 id="2-9-Moore-Penrose伪逆"><a href="#2-9-Moore-Penrose伪逆" class="headerlink" title="2.9 Moore-Penrose伪逆"></a>2.9 Moore-Penrose伪逆</h4><p>对于非方阵，将矩阵A 的伪逆定义为：<br>$$A^+=\lim_{a\rightarrow0}(A^TA+{\alpha}I)^{-1}A^T$$<br>实际计算公式<br>$$A^+=VD^+U^T$$<br>其中，矩阵U，D和V是矩阵A奇异值分解后得到的矩阵，对角矩阵D的伪逆$D^+$是其非零元素取倒数之后再转置得到的</p><h4 id="2-10-迹运算"><a href="#2-10-迹运算" class="headerlink" title="2.10 迹运算"></a>2.10 迹运算</h4><p>迹运算返回的是矩阵对角元素的和：<br>$$Tr(A)=\sum_iA_{i,i}$$</p><p>迹运算提供了另一种描述矩阵Frobenius范数的方式$$\left|A\right|<em>F=\sqrt{Tr(AA^T)}$$<br>迹运算在转置条件下是不变的<br>$$Tr(A)=Tr(A^T)$$<br>如果多个矩阵相乘交换顺序后仍有定义，那么有<br>$$Tr(ABC)=Tr(CAB)=Tr(BCA)$$<br>$$Tr(\prod</em>{i=1}^{n}F^{(i)})=Tr(F^{(n)}\prod_{i=1}^{n-1}F^{(i)})$$<br>标量的迹运算还是它自己</p><h4 id="2-11-行列式"><a href="#2-11-行列式" class="headerlink" title="2.11 行列式"></a>2.11 行列式</h4><p>记作det(A),是一个将方阵A映射到实数的函数。行列式等于矩阵特征值的乘积</p><h4 id="2-12-实例：主成分分析"><a href="#2-12-实例：主成分分析" class="headerlink" title="2.12 实例：主成分分析"></a>2.12 实例：主成分分析</h4><p>主成分分析（PCA,Principle component analysis）是一个简单的机器学习算法，可以通过基础的线性代数知识推导。</p><p>假设，在$\mathbb{R}^n$空间中我们有m个点，为了对这些点进行有损压缩，可以采取低维表示（线性降维），对于每个点$x^{(i)}\in \mathbb{R}^n$,会有一个对应的编码向量$c^{(i)}\in \mathbb{R}^l$。如果l比n小，那么便实现了压缩。需要设置一个编码函数，根据输入返回编码$f(x)=c$，也希望设置一个解码函数，给定编码重构输入$x≈g(f(x))$，为了简化解码器，使用矩阵乘法将编码映射回$\mathbb{R}^n$，即g(c)=Dc，其中D∈$\mathbb{R}^{n×l}$是定义解码的矩阵</p><p>首先，我们需要明确如何根据每一个输入x得到一个最优编码<br>$$c ^ <em>$$<br>一种方法是最小化原始输入向量x和重构向量<br>$$g(c^</em>)$$<br>之间的距离,在PCA中，我们使用L2范数：<br>$$c ^ * =arg\min _ c \left|x-g(c)\right| _ 2$$</p><blockquote><p>arg min f(x) 是指使得函数 f(x) 取得其最小值的所有自变量 x 的集合。</p></blockquote><p>当然也可以用平方L2范数来替代L2范数：<br>$$c^*=arg\min_c\left|x-g(c)\right|<em>2^2$$<br>该最小化函数可以简化成：<br>$$(x-g(c))^T(x-g(c))$$<br>$$=x^Tx-x^Tg(c)-g(c)^Tx+g(c)^Tg(c)$$<br>$$=x^Tx-2x^Tg(c)+g(c)^Tg(c)$$<br>因为第一项$x^Tx$不依赖于c所以我们可以忽略它，得到：<br>$$c^*=arg\min_c - 2x^Tg(c)+g(c)^Tg(c)$$<br>代入g(c)的定义：<br>$$c^*=arg\min_c - 2x^TDc+c^TD^TDc=arg\min_c - 2x^TDc+c^TI_lc=arg\min_c - 2x^TDc+c^Tc$$<br>$$\nabla_c(-2x^TDc+c^Tc)=0$$<br>$$-2D^Tx+2c=0$$<br>$$c=D^Tx$$<br>于是，最优编码x只需要一个矩阵-向量乘法操作<br>$$f(x)=D^Tx$$<br>PCA重构操作<br>$$r(x)=g(f(x))=DD^Tx$$<br>接下来需要挑选编码矩阵D，所以我们需要最小化所有维数和所有点上的误差矩阵的Frobenius范数：<br>$$D^*=arg\min_D\sqrt{\sum</em>{i,j}(x_j^{(i)}-r(x^{(i)})_j)^2}\text{ subject to } D^TD=I_l$$<br>先考虑l=1的情况，此时D是一个单一向量d<br>$$d^*=arg\min_d\sum\left|x^{(i)}-dd^Tx^{(i)}\right|_2^2\text{ subject to } \left|d\right|<em>2=1$$<br>将表示个点的向量堆叠成一个矩阵，记为X∈$\mathbb{R}^{m×n}$，其中$X</em>{i,:}=x^{(i)^T}$<br>原问题可以重新表示为：<br>$$d^*=arg\min_d\left|X-Xdd^T\right|_F^2\text{ subject to }d^Td=1$$<br>暂不考虑约束，可以把Frobenius范数简化成<br>$$arg\min_d\left|X-Xdd^T\right|_F^2$$<br>$$=arg\min_dTr((X-Xdd^T)^T(X-Xdd^T))$$<br>$$=arg\min_d-2Tr(X^TXdd^T)+Tr(X^TXdd^Tdd^T)$$<br>再考虑约束条件<br>$$arg\min_d-2Tr(X^TXdd^T)+Tr(X^TXdd^Tdd^T)\text{ subject to } d^td=1$$<br>$$=arg\max_dTr(d^TX^TXd)\text{ subject to }d^Td=1$$<br>即，最优的d是X^TX最大特征值对应的特征向量</p><blockquote><p>PCA算法两种实现方法：<br>（1）基于特征值分解协方差矩阵实现<br>输入数据集X={x1,x2,x3,…,xn}，需要降到k维。<br>1）去平均值（去中心化），即每一位特征减去各自的平均值<br>2）计算协方差矩阵$\frac{1}{n}XX^T$<br>3）用特征值分解方法求协方差矩阵的特征值和特征向量<br>4）对特征值从大到小排序，选择其中最大的k个，然后将其对应的k个特征向量分别作为行向量组成特征向量矩阵P<br>5）将数据转换到k个特征向量构建的新空间中，即Y=PX<br>(2) 基于SVD分解（奇异值分解）协方差矩阵实现PCA算法</p></blockquote><h3 id="第三章-概率与信息论"><a href="#第三章-概率与信息论" class="headerlink" title="第三章 概率与信息论"></a>第三章 概率与信息论</h3><p>概率论是用于表示不确定性申明的数学框架</p><h4 id="3-1-为什么要使用概率"><a href="#3-1-为什么要使用概率" class="headerlink" title="3.1 为什么要使用概率"></a>3.1 为什么要使用概率</h4><p>几乎所有的活动都需要能够再不确定性存在时进行推理<br>不确定性的三种来源：被建模系统内在的随机性，不完全观测，不完全建模</p><h4 id="3-2-随机变量"><a href="#3-2-随机变量" class="headerlink" title="3.2 随机变量"></a>3.2 随机变量</h4><p>随机变量时可以随机地取不同值的变量。</p><h4 id="3-3-概率分布"><a href="#3-3-概率分布" class="headerlink" title="3.3 概率分布"></a>3.3 概率分布</h4><p>概率分布用来描述随机变量或一簇随机变量在每一个可能取到的状态的可能性大小</p><h5 id="3-3-1-离散型变量和概率质量函数"><a href="#3-3-1-离散型变量和概率质量函数" class="headerlink" title="3.3.1 离散型变量和概率质量函数"></a>3.3.1 离散型变量和概率质量函数</h5><ol><li>离散型变量的概率分布可以用概率质量函数(PMF，probability mass function)来描述</li><li>联合概率分布是多个变量的概率分布</li><li>如果用P表示概率质量函数，则满足以下条件</li></ol><ul><li>P的定义域必须是随机变量x所有可能状态的集合</li><li>$\forall x \in X, 0 \leq P(x) \leq 1$，不可能发生的事件概率为0，并且不存在比这概率更低的状态。类似的，一定发生的事件概率为1，且不存在比这概率更高的事件。</li><li>$\sum_i P(x)=1$</li></ul><h5 id="3-3-2-连续型变量和概率密度函数"><a href="#3-3-2-连续型变量和概率密度函数" class="headerlink" title="3.3.2 连续型变量和概率密度函数"></a>3.3.2 连续型变量和概率密度函数</h5><p>对于连续型随机变量，用概率密度函数（PDF，probability density function），如果用p表示概率密度函数，则满足以下条件</p><ul><li>p的定义域必须是x所有可能状态的集合</li><li>$\forall x \in X,p(x) \geq 0$.并不要求p(x)≤1</li><li>$\int p(x) \text{d}x = 1$</li></ul><h4 id="3-4-边缘概率"><a href="#3-4-边缘概率" class="headerlink" title="3.4 边缘概率"></a>3.4 边缘概率</h4><p>边缘概率分布，定义在其中一个子集上的概率分布</p><p>$\forall x \in X,P(X=x)=\sum_yP(X=x,Y=y)$<br>$p(x)=\int p(x,y) \text{d}y$</p><h4 id="3-5-条件概率"><a href="#3-5-条件概率" class="headerlink" title="3.5 条件概率"></a>3.5 条件概率</h4><p>条件概率，在给定其他事件发生时出现的概率，我们将给定X=x，Y=y发生的条件概率记为P(Y=y|X=x)<br>计算公式如下<br>$$P(Y=y|X=x)=\frac{P(Y=y,X=x)}{P(X=x)}$$</p><p>条件概率只在P(X=x)＞0时有定义</p><h4 id="3-6-条件概率的链式法则"><a href="#3-6-条件概率的链式法则" class="headerlink" title="3.6 条件概率的链式法则"></a>3.6 条件概率的链式法则</h4><p>任何多维随机变量的联合概率分布，都可以分解成只有一个变量的条件概率相乘的形式<br>$$P(x^{(1)},\cdots,x^{(n)})=P(x^{(1)})\prod_{i=2}^{n}P(x^{(i)}|x^{(1)},\cdots,x^{(i-1)})$$</p><p>例如：<br>P(a,b,c)=P(a|b,c)P(b,c)<br>P(b,c)=P(b|c)P(c)<br>P(a,b,c)=P(a|b,c)P(b|c)P(c)</p><h4 id="3-7-独立性和条件独立性"><a href="#3-7-独立性和条件独立性" class="headerlink" title="3.7 独立性和条件独立性"></a>3.7 独立性和条件独立性</h4><p><strong>相互独立</strong>：两个随机变量x，y的概率分布可以表示为两个因子的乘积形式，并且一个因子只包含x，另一个只包含y，那么这两个随机变量是相互独立的<br>$$\forall x \in X,y\in Y,p(X=x,Y=y)=p(X=x)p(Y=y)$$</p><p><strong>条件独立</strong>：如果关于x和y的条件概率分布对于z的每一个值都可以写成乘积的形式，那么这两个随机变量x和y在给定随机变量z时是条件独立的</p><p>$$\forall x \in X,y\in Y,z\in Z,p(X=x,Y=y|Z=z)=p(X=x|Z=z)p(Y=y|Z=z)$$</p><h4 id="3-8-期望，方差，协方差"><a href="#3-8-期望，方差，协方差" class="headerlink" title="3.8 期望，方差，协方差"></a>3.8 期望，方差，协方差</h4><p>（1）期望(expected value)<br>函数f(x)关于某分布P(x)的期望或者期望值是指，当x由P产生，f作用到x时，f(x)的平均值</p><p>离散型随机变量的期望<br>$$\mathbb{E} _ {x\sim P}[f(x)]=\sum_x P(x)f(x)$$<br>连续型随机变量的期望<br>$$\mathbb{E} _ {x\sim P}[f(x)]=\int p(x)f(x)\text{d}x$$</p><p>期望是线性的，假设α和β不依赖于x<br>$$\mathbb{E}_ x[\alpha f(x)+ \beta g(x) ]=\alpha \mathbb{E}_ x[f(x)] +\beta \mathbb{E}_ x[g(x)]$$</p><p>（2）方差(variance)<br>$$Var(f(x))=\mathbb{E}[(f(x)-\mathbb{E}[f(x)])^2]$$</p><p>方差的平方根被称为标准差</p><p>（3）协方差<br>$$Cov(f(x),g(y))=\mathbb{E}[(f(x)-\mathbb{E}[f(x)])(g(y)-\mathbb{E}[g(y)])]$$</p><p>两个随机变量独立→协方差为0→没有线性关系<br>但是协方差为0，两个变量不一定独立</p><p>协方差矩阵：<br>$$Cov(x)_ {i,j}=Cov(x_i,x_j)$$<br>协方差矩阵的对角元是方差：<br>$$Cov(x_i,x_i)=Var(x_i)$$</p><h4 id="3-9-常用概率分布"><a href="#3-9-常用概率分布" class="headerlink" title="3.9 常用概率分布"></a>3.9 常用概率分布</h4><h5 id="3-9-1-Bernoulli分布（伯努利分布）"><a href="#3-9-1-Bernoulli分布（伯努利分布）" class="headerlink" title="3.9.1 Bernoulli分布（伯努利分布）"></a>3.9.1 Bernoulli分布（伯努利分布）</h5><p>单个二值随机变量的分布，又名两点分布，0-1分布</p><ul><li><p>对于单次随机试验，对于一个随机变量X而言：<br>$$P(X=1)=p$$<br>$$P(X=0)=1-p$$<br>$$P(X=x)=p^x(1-p)^{1-x}$$<br>$$E(X)=p$$<br>$$Var(X)=p(1-p)$$</p></li><li><p>进行一次伯努利试验，成功(X=1)概率为p(0≤p≤1)，失败(X=0)概率为1-p，则称随机变量X服从伯努利分布。伯努利分布是离散型概率分布，其概率质量函数为：</p></li></ul><p>$$f(x)=p^x(1-p)^{1-x}=\left{\begin{matrix}<br>p \text{ ,if x=1}\<br>1-p \text{ ,if x=0}\<br>0 \text{ ,otherwise}<br>\end{matrix}\right.$$</p><h5 id="3-9-2-Multinoulli分布"><a href="#3-9-2-Multinoulli分布" class="headerlink" title="3.9.2 Multinoulli分布"></a>3.9.2 Multinoulli分布</h5><p>Multinoulli分布，或称范畴分布，是指在具有k个不同状态的单个离散型随机变量上的分布，其中k是一个有限值</p><blockquote><p>Multinoulli分布是多项式分布（Multinomial distribution）的一个特例。多项式分布是${0,…,n}^k$中的向量的分布，用于表示当对Multinoulli分布采样n次时k个类中的每一个被访问的次数。即n=1的多项式分布是Multinoulli分布。</p></blockquote><p>Multinoulli分布由向量$p∈[0,1]^{k−1}$参数化，其中每一个分量$ p_i $表示第 i 个状态的概率。最后的第k个状态的概率可以通过$1−1^Tp$给出。注意我们必须限制$1^⊤p≤1$。Multinoulli分布经常用来表示对象分类的分布，所以我们很少假设状态 1 具有数值 1 之类的。因此，我们通常不需要去计算 Multinoulli 分布的随机变量的期望和方差。</p><p>Bernoulli 分布和 Multinoulli 分布足够用来描述在它们领域内的任意分布。它们能够描述这些分布，不是因为它们特别强大，而是因为它们的领域很简单。它们可以对那些能够将所有的状态进行枚举的离散型随机变量进行建模。当处理的是连续型随机变量时，会有不可数无限多的状态，所以任何通过少量参数描述的概率分布都必须在分布上加以严格的限制。</p><h5 id="3-9-3-高斯分布"><a href="#3-9-3-高斯分布" class="headerlink" title="3.9.3 高斯分布"></a>3.9.3 高斯分布</h5><p>高斯分布，也叫正态分布</p><p>$$N(x;\mu ,\sigma^2 )=\sqrt{\frac{1}{2\pi \sigma^2}}exp(-\frac{1}{2\sigma^2}(x-\mu)^2)$$<br>$$N(x;\mu ,\sigma^2 )=\sqrt{\frac{1}{2\pi \sigma^2}}e^{(-\frac{1}{2\sigma^2}(x-\mu)^2)}$$</p><p>正态分布的概率密度函数的图象的中心峰的x坐标由μ给出，峰的宽度受σ控制<br>标准正态分布：μ=0，σ=1</p><p>$$\mathbb{E}(x)=\mu$$<br>$$Var(x)=\sigma^2$$</p><p>令β为方差的倒数，来控制分布的精度<br>$$N(x;\mu ,\beta^{-1} )=\sqrt{\frac{\beta}{2\pi}}e^{(-\frac{1}{2}\beta(x-\mu)^2)}$$</p><p>正态分布可以推广到$\mathbb{R}^n$空间，这种情况下被称为多维正态分布<br>$$N(x;\mu ,\sum )=\sqrt{\frac{1}{(2\pi)^ndet(\sum)}}e^{(-\frac{1}{2}(x-\mu)^T\sum^{-1}(x-\mu))}$$</p><p>参数μ仍然表示分布的均值，只不过现在是向量值。参数∑给出了分布的协方差矩阵，但并不是一个很高效的参数化分布的方式，因为要对∑求逆，因此可以使用一个精度矩阵<strong>β</strong>替换<br>$$N(x;\mu ,\beta^{-1} )=\sqrt{\frac{det(\beta)}{(2\pi)^n}}e^{(-\frac{1}{2}(x-\mu)^T\beta(x-\mu))}$$</p><h5 id="3-9-4-指数分布和Laplace分布"><a href="#3-9-4-指数分布和Laplace分布" class="headerlink" title="3.9.4 指数分布和Laplace分布"></a>3.9.4 指数分布和Laplace分布</h5><p>指数分布，可以用来表示事件的时间间隔的概率，可以由泊松分布推导出来</p><p>其概率密度函数<br>$$f(x)=\left{\begin{matrix}<br>\lambda e^{-\lambda x} , x &gt;0 \<br>0 , otherwise<br>\end{matrix}\right.$$<br>分布函数<br>$$F(x)=\left{\begin{matrix}<br>1-\lambda e^{-\lambda x} , x \geqslant 0 \<br>0 , x＜0<br>\end{matrix}\right.（\lambda&gt;0）$$</p><p>Laplace分布，允许我们在任意一点μ处设置概率质量的峰值<br>$$Laplace(x;\mu , \gamma ) = \frac{1}{2\gamma}exp(-\frac{|x-\mu|}{\gamma})$$</p><h5 id="3-9-5-Dirac分布和经验分布"><a href="#3-9-5-Dirac分布和经验分布" class="headerlink" title="3.9.5 Dirac分布和经验分布"></a>3.9.5 Dirac分布和经验分布</h5><p>概率密度函数<br>$$p(x)=\delta(x-\mu)$$</p><p>Dirac分布经常作为经验分布的组成部分出现</p><h5 id="3-9-6-分布的混合"><a href="#3-9-6-分布的混合" class="headerlink" title="3.9.6 分布的混合"></a>3.9.6 分布的混合</h5><p>潜变量：不能直接观测到的随机变量</p><p>高斯混合模型，概率密度的万能近似器</p><p>先验概率：在观测到x之前计算的<br>后验概率：在观测到x之后计算的</p><h4 id="3-10-常用函数的有用性质"><a href="#3-10-常用函数的有用性质" class="headerlink" title="3.10 常用函数的有用性质"></a>3.10 常用函数的有用性质</h4><p>logistic sigmoid函数,在变量取绝对值很大的正值或负值时出现饱和现象</p><p>$$\sigma(x)=\frac{1}{1+e^{-x}}$$</p><p>softplus函数,范围（0,∞）</p><p>$$\zeta (x)=log(1+e^x)$$</p><p>ReLU函数，人工神经网络常用的激活函数</p><p>$$f(x)=max(0,x)$$</p><h4 id="3-11-贝叶斯规则"><a href="#3-11-贝叶斯规则" class="headerlink" title="3.11 贝叶斯规则"></a>3.11 贝叶斯规则</h4><p>$$P(x|y)=\frac{P(x)P(y|x)}{P(y)}$$</p><h4 id="3-12-连续型变量的技术细节"><a href="#3-12-连续型变量的技术细节" class="headerlink" title="3.12 连续型变量的技术细节"></a>3.12 连续型变量的技术细节</h4><p>测度论<br>零测度<br>几乎处处<br>Jacobin矩阵</p><h4 id="3-13-信息论"><a href="#3-13-信息论" class="headerlink" title="3.13 信息论"></a>3.13 信息论</h4><p>量化信息</p><ul><li>非常可能发生的事件信息量要比较少，并且极端情况下，确保能够发生的事件没有信息量</li><li>较不可能发生的事件具有更高的信息量</li><li>独立事件应具有增量的信息。例如，投掷的硬币两次正面朝上传递的信息量，应该是投掷一次硬币正面朝上信息量的两倍</li></ul><p>定义一个事件X=x的自信息为<br>$$I(x)=-logP(x)$$<br>log为自然对数，底为e，I(x)单位是奈特，一奈特是以1/e概率观测到一个事件时获得的信息量<br>若以2为底数，单位是比特或者香农</p><p>香农熵，一个分布的香农熵时指遵循这个分布的事件所产生的期望信息总量<br>$$H(x)=\mathbb{E}_ {x\sim P}[I(x)]=-\mathbb{E} _ {x \sim P}[logP(x)]$$</p><p>当x连续，香农熵被称为微分熵</p><p>KL散度，用域衡量两个分布的差异<br>$$D_{KL}(P||Q)=\mathbb{E} _ {x \sim P}[log\frac{P(x)}{Q(x)}]=\mathbb{E} _ {x \sim P}[logP(x)-logQ(x)]$$</p><p>交叉熵<br>$$H(P,Q)=-\mathbb{E}_ {x \sim P} logQ(x)$$</p><h4 id="3-14-结构化概率模型"><a href="#3-14-结构化概率模型" class="headerlink" title="3.14 结构化概率模型"></a>3.14 结构化概率模型</h4><p>结构化概率模型，又叫图模型</p><p>有向<br>无向</p><h3 id="第四章-数值计算"><a href="#第四章-数值计算" class="headerlink" title="第四章 数值计算"></a>第四章 数值计算</h3><h4 id="4-1-上溢和下溢"><a href="#4-1-上溢和下溢" class="headerlink" title="4.1 上溢和下溢"></a>4.1 上溢和下溢</h4><p>通过有限数量的位表示无限多的实数，总会引入舍入误差，包括了上溢和下溢<br>对上溢和下溢进行数值稳定的一个例子是softmax函数<br>$$softmax(x) _ i=\frac{exp(x_i)}{\sum_{j=1}^nexp(x_j)}$$</p><h4 id="4-2-病态条件"><a href="#4-2-病态条件" class="headerlink" title="4.2 病态条件"></a>4.2 病态条件</h4><p>条件数表征函数相对于输入的微小变化而变化的快慢程度<br>考虑函数$f(x)=A^{-1}x$，当A∈$\mathbb{R}^{n×n}$具有特征分解时，其条件数为<br>$$\max_{i,j}|\frac{\lambda_i}{\lambda_j}|$$</p><h4 id="4-3-基于梯度的优化方法"><a href="#4-3-基于梯度的优化方法" class="headerlink" title="4.3 基于梯度的优化方法"></a>4.3 基于梯度的优化方法</h4><p>（1）优化<br>大多数深度学习算法涉及某种形式的优化，包括改变x以最小化或最大化某个函数f(x)。通常以最小化指代大多数最优化问题，最大化可以经由-f(x)来实现</p><p>目标函数（准则）：要最小化或最大化的函数<br>代价函数（损失函数/误差函数）：对其进行最小化时也称之为代价函数</p><p>梯度下降：将x往导数反方向移动来减小f(x)</p><p>$\frac{df(x)}{x}=0$的点称为临界点，驻点<br>有些临界点既不是最大点也不是最小点，被称为鞍点</p><p>（2）偏导，梯度，方向导数<br>对于多维输入函数，提出了<strong>偏导数</strong>。偏导数为函数在每个位置处沿着自变量坐标轴方向上的导数（切线斜率）</p><p><strong>梯度</strong>，写作$\nabla_xf(x)$，当前位置的梯度方向，为函数在该位置处方向导数最大的方向，也是函数值上升最快的方向，反方向为下降最快的方向。当前位置的梯度长度（模），为最大方向导数的值</p><p><strong>方向导数</strong>，如果是方向不是沿着坐标轴方向，而是任意方向，则为方向导数</p><h5 id="4-3-1-梯度之上：Jacobin和Hessian矩阵"><a href="#4-3-1-梯度之上：Jacobin和Hessian矩阵" class="headerlink" title="4.3.1 梯度之上：Jacobin和Hessian矩阵"></a>4.3.1 梯度之上：Jacobin和Hessian矩阵</h5><p>1.Jacobin<br>在向量分析中, 雅可比矩阵是一阶偏导数以一定方式排列成的矩阵, 其行列式称为雅可比行列式. 还有, 在代数几何中, 代数曲线的雅可比量表示雅可比簇：伴随该曲线的一个代数群, 曲线可以嵌入其中. </p><p>雅可比矩阵的重要性在于它体现了一个可微方程与给出点的最优线性逼近. 因此, 雅可比矩阵类似于多元函数的导数.<br>$$\begin{bmatrix}<br>\frac{\partial y_1}{\partial x_1} &amp;\cdots&amp; \frac{\partial y_1}{\partial x_n} \<br>\vdots &amp; \ddots &amp; \vdots \<br>\frac{\partial y_m}{\partial x_1} &amp;\cdots&amp; \frac{\partial y_m}{\partial x_n}<br>\end{bmatrix}$$</p><p>2.Hessian<br>在数学中, 海森矩阵(Hessian matrix或Hessian)是一个自变量为向量的实值函数的二阶偏导数组成的方块矩阵, 此函数如下：<br>$f(x_1,x_2,\cdots,x_n)$，如果f的所有二阶导数都存在，那么<br>$$\begin{bmatrix}<br>\frac{\partial^2 f}{\partial x_1^2} &amp; \frac{\partial^2 f}{\partial x_1 \partial x_2} &amp;\cdots&amp; \frac{\partial^2 f}{\partial x_1 \partial x_n} \<br>\frac{\partial^2 f}{\partial x_2 \partial x_1} &amp; \frac{\partial^2 f}{\partial x_2^2} &amp;\cdots&amp; \frac{\partial^2 f}{\partial x_2 \partial x_n} \<br>\vdots &amp; \vdots &amp; \ddots &amp;\vdots\<br>\frac{\partial^2 f}{\partial x_n \partial x_1} &amp; \frac{\partial^2 f}{\partial x_n \partial x_2} &amp;\cdots&amp; \frac{\partial^2 f}{\partial x_n^2}<br>\end{bmatrix}$$</p><p>Hessian矩阵等价于梯度的Jacobian矩阵<br>海森矩阵在牛顿法中的应用，牛顿法时一个基于二阶泰勒展开来近似x附近的f(x)的方法</p><p>例如：在x0处展开<br>$$f(x)=f(x_0)+(x-x_0)^T\nabla_xf(x_0)+\frac{1}{2}(x-x_0)^TH(f)(x_0)(x-x_0)$$<br>临界点为<br>$$X’=X_0-H(f)(x_0)^{-1}\nabla_xf(x_0)$$</p><h4 id="4-4-约束优化"><a href="#4-4-约束优化" class="headerlink" title="4.4 约束优化"></a>4.4 约束优化</h4><p>在x的某些集合S中找f(x)的最大值或最小值，称为约束优化</p><p>Karush-Kuhn-Tucker（KKT）方法</p><p>广义Lagrangian（广义Lagrange函数）,通过m个函数g和n个函数h描述S，那么S可以表示为$\mathbb{S}={x|\forall i,g^{(i)}(x)=0 and \forall j ,h^{(j)}(x)≤0}$，其中涉及g的等式称为等式约束，涉及h的不等式称为不等式约束，定义如下：<br>$$L(x,\lambda , \alpha)=f(x)+\sum_i\lambda_ig^{(i)}(x)+\sum_j\alpha_jh^{(j)}(x)$$</p><h4 id="4-5-实例：线性最小二乘"><a href="#4-5-实例：线性最小二乘" class="headerlink" title="4.5 实例：线性最小二乘"></a>4.5 实例：线性最小二乘</h4><p>假设我们希望最小化下式中的x值：</p><p>$$f(x)=\frac{1}{2}\left|Ax-b\right| _ 2 ^ 2$$</p><p>首先，计算梯度<br>$$\nabla_xf(x)=A^T(Ax-b)=A^TAx-A^Tb$$</p><p>假设希望最小化同样的函数，但受$x^Tx≤1$的约束<br>$$L(x,\lambda)=f(x)+\lambda(x^Tx-1)$$</p><p>现在，我们解决以下问题<br>$$\min_x\max_{\lambda , \lambda ≥ 0}L(x,\lambda)$$</p><p>我们可以用Moore-Penrose伪逆。<br>关于x对Lagrangian微分，得到<br>$$A^TAx-A^Tb+2\lambda x=0$$</p><p>解为：<br>$$x=(A^TA+2\lambda I)^{-1}A^Tb$$</p><p>观察<br>$$\frac{\partial}{\partial \lambda}L(x,\lambda)=x^Tx-1$$</p><p>当x的范数超过1时，该导数是正的，所以为了跟随导数上坡并相对λ增加Lagrangian，我们需要增加λ。因为$x^Tx$的惩罚系数增加，秋节关于x的线性方程现在将得到具有较小范数的解</p><h3 id="第五章-机器学习基础"><a href="#第五章-机器学习基础" class="headerlink" title="第五章 机器学习基础"></a>第五章 机器学习基础</h3><h4 id="5-1-学习算法"><a href="#5-1-学习算法" class="headerlink" title="5.1 学习算法"></a>5.1 学习算法</h4><p>能够从数据中学习的算法</p><h5 id="5-1-1-任务T"><a href="#5-1-1-任务T" class="headerlink" title="5.1.1 任务T"></a>5.1.1 任务T</h5><p>通常机器学习任务定义为机器学习系统应该如何处理样本。<br>样本是我们从希望机器学习系统处理的对象或事件中收集到的已经量化的特征的集合</p><p>常见任务：分类，输入缺失分类，回归，转录，机器翻译，结构化输出，异常检测，合成和采样，缺失值填补，去噪，密度估计或概率质量函数估计</p><h5 id="5-1-2-性能度量P"><a href="#5-1-2-性能度量P" class="headerlink" title="5.1.2 性能度量P"></a>5.1.2 性能度量P</h5><p>为了评估机器学习算法的能力，提出准确率，错误率</p><p>使用测试集数据来评估系统性能，将其与训练机器学习系统的训练集数据分开</p><h5 id="5-1-3-经验E"><a href="#5-1-3-经验E" class="headerlink" title="5.1.3 经验E"></a>5.1.3 经验E</h5><p>机器学习算法分为无监督算法和监督算法</p><h5 id="5-1-4-示例：线性回归"><a href="#5-1-4-示例：线性回归" class="headerlink" title="5.1.4 示例：线性回归"></a>5.1.4 示例：线性回归</h5><h4 id="5-2-容量，过拟合和欠拟合"><a href="#5-2-容量，过拟合和欠拟合" class="headerlink" title="5.2 容量，过拟合和欠拟合"></a>5.2 容量，过拟合和欠拟合</h4><h5 id="5-2-1-没有免费午餐定理"><a href="#5-2-1-没有免费午餐定理" class="headerlink" title="5.2.1 没有免费午餐定理"></a>5.2.1 没有免费午餐定理</h5><h5 id="5-2-2-正则化"><a href="#5-2-2-正则化" class="headerlink" title="5.2.2 正则化"></a>5.2.2 正则化</h5><h4 id="5-3-超参数和验证集"><a href="#5-3-超参数和验证集" class="headerlink" title="5.3 超参数和验证集"></a>5.3 超参数和验证集</h4><h5 id="5-3-1-交叉验证"><a href="#5-3-1-交叉验证" class="headerlink" title="5.3.1 交叉验证"></a>5.3.1 交叉验证</h5><h4 id="5-4-估计，偏差和方差"><a href="#5-4-估计，偏差和方差" class="headerlink" title="5.4 估计，偏差和方差"></a>5.4 估计，偏差和方差</h4><h5 id="5-4-1-点估计"><a href="#5-4-1-点估计" class="headerlink" title="5.4.1 点估计"></a>5.4.1 点估计</h5><h5 id="5-4-2-偏差"><a href="#5-4-2-偏差" class="headerlink" title="5.4.2 偏差"></a>5.4.2 偏差</h5><h5 id="5-4-3-方差和标准差"><a href="#5-4-3-方差和标准差" class="headerlink" title="5.4.3 方差和标准差"></a>5.4.3 方差和标准差</h5><h5 id="5-4-4-权衡偏差和方差以最小化均方误差"><a href="#5-4-4-权衡偏差和方差以最小化均方误差" class="headerlink" title="5.4.4 权衡偏差和方差以最小化均方误差"></a>5.4.4 权衡偏差和方差以最小化均方误差</h5><h5 id="5-4-5-一致性"><a href="#5-4-5-一致性" class="headerlink" title="5.4.5 一致性"></a>5.4.5 一致性</h5><h4 id="5-5-最大似然估计"><a href="#5-5-最大似然估计" class="headerlink" title="5.5 最大似然估计"></a>5.5 最大似然估计</h4>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;深度学习&quot;&gt;&lt;a href=&quot;#深度学习&quot; class=&quot;headerlink&quot; title=&quot;深度学习&quot;&gt;&lt;/a&gt;深度学习&lt;/h1&gt;&lt;h3 id=&quot;第一章-前言&quot;&gt;&lt;a href=&quot;#第一章-前言&quot; class=&quot;headerlink&quot; title=&quot;第一章 前</summary>
      
    
    
    
    <category term="人工智能" scheme="https://blog.justlovesmile.top/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    
    <category term="深度学习" scheme="https://blog.justlovesmile.top/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>大学课程 | 计算机系统结构精简知识点</title>
    <link href="https://blog.justlovesmile.top/posts/651e6a0b.html"/>
    <id>https://blog.justlovesmile.top/posts/651e6a0b.html</id>
    <published>2020-09-08T06:31:25.000Z</published>
    <updated>2020-09-08T06:31:25.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="/posts/50754.html">计算机系统结构笔记传送门</a></p><h1 id="计算机系统结构知识点"><a href="#计算机系统结构知识点" class="headerlink" title="计算机系统结构知识点"></a>计算机系统结构知识点</h1><ol><li>多级层次结构：<br>（1）按功能划分成多层机器级组成的层次结构，从上到下依次为，应用语言机器级，高级语言机器级，汇编语言机器级，操作系统机器级，传统机器语言机器级，微程序机器级。<br>（2）机器，被定义为能存储和执行相应语言程序的算法和数据结构的集合体<br>（3）微程序机器级用硬件实现，传统机器语言机器级用固件实现<br>（4）固件：具有软件功能的硬件<br>（5）以软件为主实现的机器称为虚拟机器，由硬件或固件实现的称为实际机器</li><li>透明性：<br>（1）客观存在的事物或属性从某个角度看不见</li><li>翻译与解释<br>（1）翻译：先用转换程序将高一级机器级上的程序<code>整个</code>变换成低一级机器级上等效的程序，然后再在低一级机器级上实现的技术<br>（2）解释：在低级机器级上用它的一串语句或指令来仿真高级机器级上的一条语句或指令的功能，是通过对高级的机器级语言程序中的每条语句或指令<code>逐条</code>解释来实现的技术</li><li>软硬件逻辑功能等效<br>（1）概念：软硬件逻辑功能等效是指计算机系统的某功能可以由硬件实现也可以由软件实现，在逻辑功能上是等价的。由硬件实现功能的特点是速度快、增加硬件成本，灵活性低。由软件实现功能的特点是灵活性好、但速度较慢，增加软件设计费用等</li><li>计算机系统结构、组成与实现的定义及三者之间的关系，以乘法指令为例说明上述三者各自的研究内容<br>（1）计算机系统结构的定义：对计算机系统中各级界面的定义及其上下的功能分配<br>（2）计算机组成的定义：计算机系统结构的逻辑实现，包括机器级内部的数据流和控制流的组成以及逻辑设计等<br>（3）计算机实现：是指计算机组成的物理实现（具体电路，器件的设计，装配技术等等）<br>（4）三者的关系：三者互不相同，但又相互影响。组成向上决定于结构，向下受限于实现技术。<br>（5）对于乘法指令，计算机系统结构主要考虑是否要设置乘法指令；而计算机组成主要考虑乘法指令是用专门的高速乘法器还是用加法器和移位器实现；计算机实现主要考虑乘法器，加法器的物理显示，如器件的类型，继承父，数量的确定和选择</li><li>计算机系统结构的设计思路<br>（1）<code>“由上往下”设计</code>，由顶向底。先考虑应用要求，再逐级向下，下一级是对上一级的优化。是一种穿行设计方法，设计周期较长<br>（2）<code>“由下往上”设计</code>，由底向顶。先设计底层，再加配操作系统和编译系统，以及设施的系统软件和算法等等。软硬件容易脱节，串行设计，周期长，很少采用<br>（3）<code>“从中间开始”向两边设计</code>。一般方法。软硬件并行设计，较好的设计方法</li><li>软件移植及三种移植技术<br>（1）软件的可移植性：软件不修改或只经少量修改就可以由一台机器移到另一台机器上允许，同一软件可应用于不同的环境<br>（2）移植技术：<code>统一高级语言</code>；<code>采用系列机</code>；<code>模拟和仿真</code></li><li>软件兼容及分类<br>（1）软件兼容：机器语言程序以及编译程序能不加修改地通用于各档机器<br>（2）分类：<br>a. 向上兼容/向下兼容：向上（下）兼容是指，按某党机器编制的软件，不加修改就能运行于比他高（低）档的机器上。<br>b. 向前兼容/向后兼容：向前（后）兼容是指，按某个时期投入市场的该型号机器上编制的软件，不加修改就能运行于在它之前（后）的投入市场的机器上。<br>（3）系列机软件必须保证向后兼容，力争向上兼容</li><li>系列机与兼容机，模拟与仿真<br>（1）系列机：是具有相同体系结构，但组成和实现不同的一系列不同型号的计算机系统。<br>兼容机：不同厂家生产的具有相同体系结构的计算机。<br>（2）模拟：用机器语言程序（在主存）解释实现软件移植的方法；运行速度低，实时性差，模拟程序复杂<br>仿真：用微程序（在控制寄存器）直接解释另一种机器指令系统的方法；两种系统结构差别较大时，难以仿真<br>两者的主要区别在于解释用的语言，其次有解释程序的所存位置不同</li><li>应用与器件对系统结构的影响<br>（1）应用的发展对结构设计提出范围广泛的要求<br>（2）器件的发展改变了逻辑设计的传统方法；推动结构和组成前进的关键因素；加速了结构“下移”；促进了算法，语言和软件的发展</li><li>并行性概念及发展并行性的三种技术途径<br>（1）并行性：把解题中具有可以同时进行运算或操作的特性称为并行性，并行性包括同时性和并发性<br>（2）并行性等级：<br>①按计算机系统执行程序的角度，从低到高：指令内部，指令之间，任务或进程之间，作业或程序之间<br>②从计算机系统中处理数据的角度来看，从低到高：位串字串，位并字串，位片串字并，全并行<br>（3）三种技术途径：时间重叠，资源重叠，资源共享<br>时间重叠：多个处理过程在时间上错开<br>资源重叠：重复设置硬件资源来提高可靠性和性能<br>资源共享：多个用户按时间顺序轮流使用同一套资源</li><li>耦合度概念<br>（1）耦合度概念：反映多机系统中各机器之间物理连接的紧密度和交叉作用能力<br>（2）分类：<br>最低耦合：除存储介质，无物理连接，脱机<br>松散耦合：通过通道或通信线路互连，磁带，磁盘…<br>紧密耦合：通过总线或高速开关互连，主存…</li><li>弗林分类法<br>（1）单指令流单数据流（SISD），传统单处理器计算机<br>（2）单指令流多数据流（SIMD），阵列处理机和相联处理机<br>（3）多指令流单数据流（MISD），很少见<br>（4）多指令流多数据流（MIMD），多级系统</li><li>数据表示与数据结构<br>（1）数据表示：能由机器硬件识别和引用的数据类型（数据类型指一类值的集合和可作用于其上的操作集）<br>（2）数据结构：结构数据类型的组织方式，反映了应用中要用到的各种数据元素或信息单元之间的结构关系<br>（3）数据结构和数据表示是软硬件的交界面，数据结构是软，数据表示是硬</li><li>高级数据表示<br>（1）自定义数据表示：<br>a). 标识符数据表示<br>①每个数据带了类型标志位，标识符主要用于指明数据类型，但也可以用域指明所用信息类型。标识符由编译程序建立，对高级语言程序透明。<br>②优点：简化了指令系统和程序设计；简化了编译程序；便于一致性校验；能由硬件自动变换数据类型；支持数据库系统的实现与数据类型无关的要求；为软件调试和应用软件开发提供了支持，便于程序的跟踪和调试<br>③缺点：增加程序所占的主存空间；降低指令的执行速度<br>b). 数据描述符<br>①描述符和数据分开存放，用于描述所访问的数据是整块的还是单个的，访问该数据块或数据元素所要的地址以及其它信息<br>②优点：进一步减少标识符所占存储空间<br>（2）向量，数组数据表示<br>①有向量数据表示的处理机是向量处理机<br>②优点：加快形成元素地址，便于实现把向量各元素成块预取到中央处理机，用一条向量，数组指令流水或同事对整个向量，数组进行高速处理<br>（3）堆栈数据表示<br>①有堆栈数据表示的处理机是堆栈机器<br>②通常用于保存子程序调用时的返回地址<br>③堆栈机器特点：有丰富的堆栈操作指令且功能强大；有力地支持了高级语言程序的编译；有力的支持了子程序的嵌套和递归调用</li><li>引入数据表示的原则<br>（1）原则1：看系统的效率是否显著提高，包括实现时间和存储空间是否显著减少。实现时间是否减少又主要看主存和处理机之间传送的信息量是否减少<br>（2）原则2：看引入这种数据表示后，其通用性和利用率是否提高<br>（3）原则3：基本的数据表示，也有可挖掘的细节问题<br>（4）原则4：基本的数据类型必须设</li><li>浮点数尾数基值的选择与下溢处理方法<br>（1）浮点数尾数基值的选择<br>如果小数点的位置事先已有约定，不再改变，此类数称为“定点数”。<br>如果小数点的位置可变，则称为“浮点数”。<br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200826171758.png"><br>rm ：尾数的基<br>re ：阶码的基（re =2）<br>m： 尾数长度 (注意其含义)<br>p： 阶码长度<br>【p表示数的范围大小；尾数的位数m主要影响表示值的精度】<br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200415135441.png"></li></ol><table><thead><tr><th align="center">条件：非负阶，规格化，正尾数</th><th align="center">阶值：二进制p位，尾数：rm进制m’位</th><th align="center">若p=2，m=4，当rm=2（即m’=4）时</th><th align="center">若p=2，m=4，当rm=16（即m’=1）时</th></tr></thead><tbody><tr><td align="center">可表示最小尾数值</td><td align="center">rm^(-1)</td><td align="center">1/2</td><td align="center">1/16</td></tr><tr><td align="center">可表示最大尾数值</td><td align="center">1-1×rm^(-m’)</td><td align="center">15/16</td><td align="center">15/16</td></tr><tr><td align="center">最大阶值</td><td align="center">2^p-1</td><td align="center">3</td><td align="center">3</td></tr><tr><td align="center">可表示最小值</td><td align="center">rm^(-1)</td><td align="center">1/2</td><td align="center">1/16</td></tr><tr><td align="center">可表示最大值</td><td align="center">rm^(2^p-1)×(1-rm^(-m’))</td><td align="center">7.5</td><td align="center">3840</td></tr><tr><td align="center">可表示的尾数个数</td><td align="center">rm^(m’)×(rm-1)/rm</td><td align="center">8</td><td align="center">15</td></tr><tr><td align="center">可表示阶的个数</td><td align="center">2^p</td><td align="center">4</td><td align="center">4</td></tr><tr><td align="center">可表示数的个数</td><td align="center">2^p×rm^(m’)×(rm-1)/rm</td><td align="center">32</td><td align="center">60</td></tr></tbody></table><p>（2）下溢处理方法（对应用程序员，系统程序员透明）<br>减少运算中的精度损失关键是要处理好运算中尾数超出字长的部分，使精度损失最小<br>a) <strong>截断法</strong><br>①方法：<strong>将尾数超出机器字长的部分去掉</strong><br>②以rm=2，m=2为例讨论最大误差：在整数时接近于1(“11:111…1”截断成“11:”)；在分数时接近于2^(-m) (“.01:111…1”截断成“.01:”)<br>③对于正数，如有误差总是负误差<br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200415140626.png"><br>④优点：实现简单，不增加硬件，不需要处理时间<br>缺点：最大误差较大，且平均误差为负且较大，无法调节，因而已很少使用<br>b) <strong>舍入法</strong><br>①方法：在机器运算的规定字长之外增设一位附加位，存放溢出部分的最高位，每当进行尾数下溢处理时，将附加位加1，[整数加0.5，分数加2^(-(m+1))]<br>②例如：整数：“10:10…0”舍入成“11:” 正误差；分数：“.10:01…0”舍入成“.10:” 负误差<br>③优点：实现简单，增加的硬件开销少，最大误差小，平均误差接近于零，略偏正<br>缺点：处理速度慢，需要花费在数的附加位加1以及因此产生进位的时间，最坏情况下，需要从尾数最低位进制<br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/Y5E%7B1ZHXC%25%5B77%7BW_5Z2%5BY%5DO.png"><br>c) <strong>恒置“1”法</strong><br>①方法：将机器运算的规定字长之最低位恒置“1”<br>②最大误差：整数为1（如“10:00…0”处理成“11:”）；分数为2-m（如“.00:00…0”处理成“.01:”）<br>③误差有正负：负误差（如“.11:10…1”处理成“.11:”）；正误差（如“.00:00…0”处理成“.01:”)<br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/%25%7DMG%25YPJF2L0QNXJ%7DK8FC%402.png"><br>④优点：实现简单，不需要增加硬件和处里时间，平均误差趋于0<br>缺点：最大误差最大，比截断法还大（接近于1）<br>⑤多用于中、高速机器中，由于尾数位数比微、小型机器长<br>d) <strong>查表舍入法</strong><br>①方法：取尾数p位的最后k-1位和准备舍弃的最高1位，共k位。通过ROM或PLA查表得到k-1位，作为新的尾数p位的最后k-1位<br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/%29%7DUAT%7DO7%7D%7B%25%28WNSFI%40I%25MOP.png"><br>②下溢处理表的内容：当尾数最低k-1位为全”1“时以<code>截断法</code>设置处理结果；其余情况采用<code>舍入法</code><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200415142649.png"><br>③优点：ROM法速度较快，平均误差可调到0；避免再次右规操作<br>缺点：需要硬件配合，硬件量大<br>（3）上述4种处理方法中：<br><strong>最大误差最大</strong>的是恒置“1”法，<br><strong>最大误差最小</strong>的是舍入法；<br><strong>平均误差最大</strong>的是截断法；<br><strong>平均误差可人为调节</strong>的是查表舍入法；<br><strong>下溢处理不需要附加时间开销，即速度最快</strong>的是截断法和恒置“1”法，<br><strong>处理速度最慢</strong>的是舍入法；<br><strong>实现上最花费硬件</strong>的是查表舍入法，<br><strong>最省硬件</strong>的是截断法和恒置“1”法。 </p><ol start="18"><li>程序在主存中的定位技术<br>（1）静态再定位：再目的程序装入主存时，由装入程序用软件方法把目的程序的逻辑地址变换成物理地址，程序执行时，物理地址不再改变<br>（2）动态再定位：在执行每条指令时才形成访存物理地址的方法<br>①基址寻址：设置基址寄存器和地址加法器硬件，实现逻辑地址到物理地址空间变换的支持<br>②优越性：地址加法器形成物理地址的速度快于装入程序形成的物理地址速度；具有越界保护措施，如设置上、下界寄存器，判断是否出现地址越界错误<br>（3）虚实地址映像表：用虚拟存储器增加映像表硬件后，程序空间可以超过实际主存空间，采用基地址寄存器加位移量的方法</li><li>基址寻址与变址寻址的区别<br>（1）基址寻址：基址寄存器里的值加上指令格式内的逻辑地址形成物理地址；对逻辑地址空间到物理地址空间变换的支持；通常基址寄存器的内容不变，逻辑地址可变；<br>（2）变址寻址：变址寄存器的值和指令地址码部分给出的地址之和作为操作数地址；对诸如向量，数组等数据块运算的支持；通常逻辑地址不变，变址寄存器里面的值由用户定义</li><li>信息按整数边界存储<br>（1）为了使任何时候所需的信息都只用一个存储周期访问到，要求信息在主存中存放的地址必须是该信息宽度（字节数）的整数倍，防止信息跨主存边界存放<br>（2）优点：访问周期短；缺点：存储空间浪费</li><li>哈夫曼压缩思想与指令格式的优化<br>（1）指令包含操作码和地址码两部分，为了优化指令格式，要使指令的平均字长最短，减少程序总位数以及增加指令字能表示的操作信息和地址信息<br>（2）哈夫曼压缩思想：当各种时间发生的概率不均等时，对发生概率最高的事件用最短的位数（时间）来表示（处理），而对出现概率较低的事件允许用较长的位数（时间）来表示（处理），就会使表示（处理）的平均位数（时间）缩短<br>（3）扩展操作码编码</li><li>CISC与RISC的常用技术<br>（1）CISC（复杂指令系统计算机）：进一步增强原有指令的功能以及设置更为复杂的新指令，取代原先由软件子程序完成的功能，实现软件功能的硬化；<br>a)面向目标程序的优化实现改进<br>①途径1：对大量已有机器的机器语言程序及其执行情况进行统计各种指令和指令串的使用频度来加以分析和改进。<br>使用频度分为<code>静态使用频度</code>（程序中统计出的指令及指令串的使用频度称为静态使用频度。着眼于减少目标程序所占用的存储空间）和<code>动态使用频度</code>（目标程序执行过程中对指令和指令串统计出的频度称为动态使用频度。着眼于减少目标程序的执行时间）<br>②途径2：增设强功能复合指令来取代原先由常用宏指令或子程序实现的功能，由微程序解释实现，不仅大大提高了运算速度，减少了程序调用的额外开销，也减少了子程序所占的主存空间。<br>b)面向高级语言的优化实现改进<br>①目的：缩短高级语言和机器语言的语义差距，支持高级语言编译，缩短编译程序长度和时间<br>②途径1：通过对源程序中各种高级语言语句的使用频度进行统计来分析改进<br>③途径2：如何面向编译，优化代码生成来改进<br>④途径3：改进指令系统，使它与各种语言间的语义差距都有同等的缩小<br>⑤途径4：让机器具有分别面向各种高级语言的多种指令系统，多种系统结构的面向问题动态自寻优的计算机系统<br>⑥途径5：发展高级语言计算机（或称高级语言机器）<br>c)面向操作系统的优化实现改进<br>①目的：通过缩短操作系统与计算机系统结构之间的语义差距，来进一步减少运行操作系统的时间和节省操作系统软件所占用的存储空间<br>②途径1：通过对操作系统中常用指令和指令串的使用频度进行统计分析来改进<br>③途径2：考虑如何增设专用于操作系统的新指令<br>④途径3：把操作系统中频繁使用的，对速度影响大的机构型软件子程序硬化或固化，直接用硬件或微程序解释实现<br>⑤途径4：发展让操作系统由专门的处理机来执行的功能分布处理系统结构<br>d) CISC的问题：指令系统庞大；许多质量操作繁杂，执行速度很低；编译程序太长，太复杂；部分指令利用率很低<br>（2）RISC（精简指令系统计算机）：通过减少指令种数和简化指令功能来降低硬件设计的复杂度，提高指令的执行速度<br>a) RISC的基本技术<br>①按照设计RISC一般原则来设计<br>②逻辑实现采用硬联和微程序相结合<br>③在CPU中设置大量工作寄存器并采用重叠寄存器窗口<br>④指令用流水和延迟转移<br>⑤采用高速缓冲寄存器cache，设置指令cache和数据cache分别存放指令和数据<br>⑥优化设计编译系统<br>b) RISC的问题<br>①指令少，加重汇编语言程序设计的负担，增加了机器语言程序的长度，占用存储空间多，加大了指令的信息流量<br>②对浮点运算和虚拟存储器支持不足<br>③RISC机器的编译程序比CISC的难写</li><li>总线的分类<br>（1）按在系统中的位置分为<code>芯片级</code>，<code>板级</code>，<code>系统级</code><br>（2）按允许信息传送方向分为<code>单向传输</code>，<code>双向传输（半双向和全双向）</code><br>（3）按用法分为<code>专用</code>和<code>非专用</code></li><li>总线的控制技术及通讯技术<br>（1）控制技术<br>a） 集中式控制<br>①优先次序的确定方法：串行链接，定时查询，独立请求<br>②串行链接获得使用总线权优先次序由“总线可用”线所接不见的物理位置决定，离总线控制器越近，优先级越高；三根总线，总线忙，总线可用，总线请求<br>③定时查询：查询线上计数值与发出请求的部件号是否一致；总线忙+总线请求+「logn「个定时查询计数线=2+「logn「<br>④独立请求：1根总线已被分配线+每个部件各有一对总线请求和总线准许线=2×N+1<br>b） 分布式控制<br>（2）通讯技术<br>a) 同步通信：定宽，定距的系统时标同步<br>b) 异步通信：<br>分为单向控制（分为单向源控和单向目控）和请求/回答双向控制（分为源控式（互锁和非互锁）和目控）</li><li>中断响应优先级与中断处理程序优先级及分析过程<br>（1）基本概念:<br>中断：CPU中止正在执行的程序，转去处理随机提出的请求，待处理完后，再回到预先被打断的程序继续恢复执行的过程<br>中断系统：相应和处理各种中断的软硬件总体称为中断系统<br>中断分为内部中断（CPU内的异常引起），外部中断（由中断信号引起）和软件中断（由自陷指令引起）；外部中断又分为可屏蔽中断和不可屏蔽中断<br>中断源：引起中断的各种事件<br>中断请求：中断源向中断系统发出请求中断的申请<br>中断响应：允许中断CPU现行程序的运行，转去对该请求进行预处理，包括保存好断电及其现场，调出有关处理该中断的中断服务程序，准备运行（交换新旧程序状态字PSW）<br>中断现行程序细分为中断（可屏蔽）和异常（不可屏蔽，如自陷，故障，失败）<br>（2）中断分级<br>机器校验（第1级），程序性中断和管理程序调用（第2级），外部中断（第3级），输入/输出（第4级），重新启动（最低级）<br>（3）中断的响应次序与处理次序<br>中断级屏蔽位寄存器<br>本级对本级屏蔽<br>中断响应次序，中断处理完成次序，中断处理次序<br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200909153438.png"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200909154306.png"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200909160431.png"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200909160611.png"><br>（4）通道程序结束引起的中断属于I/O中断；<br>访管中断属于第2级；</li><li>通道的工作原理及三类通道的流量计算<br>（1）通道的工作原理：用户只能再目态程序中安排要求输入输出的广义指令，然后进入相应管理程序执行这些输入输出管态指令<br>a) 目态和管态：<br>管态又叫特权态，系统态或核心态。CPU在管态下可以执行指令系统的全集。通常，操作系统在管态下运行。<br>目态又叫常态或用户态。机器处于目态时，程序只能执行非特权指令。<br>从目态转换为管态的唯一途径是中断。<br>从管态到目态可以通过修改程序状态字来实现，这将伴随这由操作系统程序到用户程序的转换。<br>启动I/O指令属于管态指令<br>（2）三类通道：<code>字节多路</code>，<code>数组多路</code>和<code>选择通道</code><br>通道流量和通道工作方式，数据传送期内选择一次设备的时间Ts，传送一个字节的时间Td有关<br>通道的极限流量：<br>fmaxbyte=1/(Ts+Td)<br>fmaxblock=K/(Ts+K×Td)<br>fmaxselect=N/(Ts+N×Td)<br>设备要求通道的实际最大流量只有小于等于通道所能达到的极限<br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200909165849.png"><br>工作周期小的不能挂载<br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200909171038.png"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200909171326.png"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200909171903.png"></li><li>存储器的性能指标<br>（1）容量Sm：存储器的位数或总字节数<br>Sm=W×L×m （W:存储体的字长，L每个存储体的字数，m并行工作存储体的个数）<br>（2）速度<br>访问时间Tn，存储周期Tm，频宽Bm<br>（3）价格<br>总价格C，每位价格c<br>同等容量下，存储器的访问速度由高到低：双极型→MOS→电荷耦合型→磁泡→定位磁盘→动头磁盘→磁带</li><li>单体多字、多体单字与多体多字<br>（1）并行主存系统的三种模式：单体多字，多体单字，多体多字</li><li>存储器系统、并行存储体系与存储层次<br>（1）存储系统：存储系统是指计算机中由存放程序和数据的各种存储设备、控制部件及管理信息调度的设备（硬件）和算法（软件）所组成的系统。<br>（2）存储体系（存储层次）：构成存储系统的几种不同的存储器之间，配上辅助软硬件或辅助硬件，使之从应用程序员来看，在逻辑上是一个整体<br>基本的二级存储体系：虚拟存储器和Cache存储器（主存-辅存存储层次）<br>Cache存储器对于应用程序员和系统程序员都是透明的<br>（3）并行主存系统：可以并行读出多个CPU字的单体多字，多体单字，，多体多字的交叉存储主存系统</li><li>虚拟存储器与高速缓冲存储器<br>（1）在具有层次结构存储器的计算机系统中，增设地址映像表机构来实现程序在主存中的定位，自动实现部分装入和部分替换功能，能从逻辑上为用户提供一个比物理贮存容量大得多，可寻址的“主存储器”。虚拟存储区的容量与物理主存大小无关，而受限于计算机的地址结构和可用磁盘容量。<br>（2）存储管理方式：段式，页式，段页式</li><li>段式存储管理与页式存储管理技术<br>（1）段式管理：将主存按段分配的存储管理方式<br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200909201223.png"><br>首先分配算法<br>最佳分配算法<br>（2）页式管理：将主存空间和程序空间都机械等分成固定大小的页<br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200909201854.png"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200909202427.png"><br>（3）段页式管理：将主存机械等分成固定大小的页，程序按模块分段，每个段又分为和主存页面大小相同的页</li><li>地址的映像与变换<br>（1）地址的映像：将每个虚存单元按什么规则（算法）装入（定位于）主存，建立起多用户虚地址N和贮存地址n之间的对应关系<br>（2）地址变换：是指程序按照映像关系装入实存后，在执行中，如何将多用户虚地址N变换成对应的实地址</li><li>全相联映像、直接相联映像与组相联映像<br>（1）Cache的全相联映像：主存中任意一块都可映像装入到Cache中任意一块位置<br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200909212811.png"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200909212838.png"><br>（2）Cache的直接相联映像：把主存空间按Cache大小等分成区，每区内的各块只能映像到Cache中唯一一个特定块位置<br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200909212844.png"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200909212845.png"><br>（3）Cache的组相联映像：将Cache和贮存空间先分成若干个组，共有2^n个组。Cache中多有的组构成Cache的唯一一个区。而主存则分成与Cache同样大小的2^(nd)个区<br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200909212846.png"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200909212847.png"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200910083257.png"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200910212801.png"></li><li>常用替换算法<br>（1）随机算法：RAND，随机产生页号，命中率低，不采用<br>（2）先进先出算法：FIFO，选择最早装如主存的页作为被替换的页<br>（3）近期最少使用算法：LRU，选择近期最少访问的页作为被替换的页<br>（4）优化替换算法：OPT，将未来的近期内不用的页替换出去的算法，有较高的主存命中率，但是不太现实<br>（5）页面失效频率法（动态算法）：PFF，根据各道程序运行中的主存页面失效率的高低，由操作系统来动态操控</li><li>堆栈型替换算法<br>（1）堆栈型替换算法：任何时刻t，在n个实页中的虚页集合总是被包含在给其增加一个实页，即n+1个实页时，在实存中的虚页集合之内的<br>（2）堆栈型替换算法，命中率H随着主存页数n的增减单调上升，至少不下降。<br>LRU替换算法属于堆栈型替换算法，操作：将刚访问过的页号置于栈顶，最久未被访问过的页号置于栈底<br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200909212445.png"></li><li>重叠与流水的区别<br>（1）重叠解释方式：在解释第K条指令的操作完成之前，就开始解释第K+1条指令<br>（2）重叠和流水的区别：依次重叠时把一条指令的解释分为两个子程序，而流水是分为更多个子过程</li><li>流水线中的各种相关及解决方案<br>（1）局部性相关：指的是在机器同时解释的多条指令之间出现了对同一主存单元或寄存器要求“先写后读”；包括<code>指令相关</code>，<code>访存操作数相关</code>和<code>通用寄存器组相关</code>等;<br>a）指令相关：包含寄存器相关（包括数据相关和名字相关）和控制相关<br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200910101227.png"><br>b）主存空间数相关：相邻两条指令之间出现要求对同一主存单元先写入而后再读出的关联<br>解决方法：推后读<br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200910101451.png"><br>（2）全局性相关：已进入流水线的转移指令（尤其时条件转移指令）和后续指令之间的相关</li><li>流水线的分类及三个性能指标的计算方法<br>（1）流水线分类：<br>按处理级别：部件级，处理机级，系统级<br>按具有的功能多少：单功能流水线，多功能流水线<br>按多功能流水线的各段能否允许同时用于多种不同功能连接流水：静态流水线，动态流水线<br>按机器所具有的数据表示：标量流水机和向量流水机<br>按各功能段间是否有反馈电路分为：线性流水和非线性流水<br>按信息流动控制方式：顺序流动流水线，异步流动流水线<br>（2）三个性能指标<br>a) 吞吐率Tp ：流水线单位时间里能流出的任务数或结果数<br>Tpmax = 1/max{各个子过程的时间}<br>Tp = n/(m×▲t+(n-1)×▲t) ：m段流水线，各段时间为▲t，完成n个任务的解释共需要时间m×▲t+(n-1)×▲t<br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200910102823.png"><br>b) 加速比Sp ：流水线方式相对于非流水线顺序方式速度提高的比值<br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200910103021.png"><br>c) 效率η：流水线设备的时间利用率，设备实际使用时间占整个设备运行时间的比值<br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200910103134.png"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200910103907.png"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200910103939.png"><br>（3）消除瓶颈：<br>①瓶颈子过程再细分<br>②重复设置多套瓶颈段并联</li><li>单功能非线性流水线的调度技术<br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200910105055.png"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200910104959.png"></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;/posts/50754.html&quot;&gt;计算机系统结构笔记传送门&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;计算机系统结构知识点&quot;&gt;&lt;a href=&quot;#计算机系统结构知识点&quot; class=&quot;headerlink&quot; title=&quot;计算机系统结构知识点&quot;&gt;&lt;/a&gt;计算机系</summary>
      
    
    
    
    <category term="学习笔记" scheme="https://blog.justlovesmile.top/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="学习笔记" scheme="https://blog.justlovesmile.top/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    <category term="大学课程" scheme="https://blog.justlovesmile.top/tags/%E5%A4%A7%E5%AD%A6%E8%AF%BE%E7%A8%8B/"/>
    
    <category term="计算机系统结构" scheme="https://blog.justlovesmile.top/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E7%BB%93%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>大学课程 | 《微机原理与接口技术》笔记</title>
    <link href="https://blog.justlovesmile.top/posts/43666.html"/>
    <id>https://blog.justlovesmile.top/posts/43666.html</id>
    <published>2020-09-05T10:05:39.000Z</published>
    <updated>2020-09-05T10:05:39.000Z</updated>
    
    <content type="html"><![CDATA[<p>大学课程《微机原理与接口技术》学习笔记整理</p><span id="more"></span><h1 id="第一章-微型计算机基础概论"><a href="#第一章-微型计算机基础概论" class="headerlink" title="第一章 微型计算机基础概论"></a>第一章 微型计算机基础概论</h1><h2 id="第一讲-关于"><a href="#第一讲-关于" class="headerlink" title="第一讲 关于"></a>第一讲 关于</h2><ul><li>计算机的主要应用：数值计算，信息处理，过程控制</li><li>微机原理与接口技术包括：数值信息表示，微型机基本原理，汇编程序设计，半导体存储器及其接口设计，输入输出技术</li></ul><h2 id="第二讲-微型计算机系统组成"><a href="#第二讲-微型计算机系统组成" class="headerlink" title="第二讲 微型计算机系统组成"></a>第二讲 微型计算机系统组成</h2><ul><li>计算机系统：<ul><li>硬件系统<ul><li>主机系统：CPU，存储器，输入输出接口，总线</li><li>外部设备</li></ul></li><li>软件系统</li></ul></li><li>能够与CPU直接进行信息交换的部件属于主机系统，不能够与CPU直接进行信息交换的部件属于外部设备</li><li>CPU<ul><li>微处理器简称CPU，是计算机的核心</li><li>主要包括：运算器，控制器，寄存器组</li></ul></li><li>存储器：<ul><li>计算机中的记忆装置。用于存放计算机工作过程中需要操作的数据和程序</li><li>内存储器 ：<ul><li>存取速度较快，容量相对较小</li><li>内存按单元组织，每单元都对应一个惟一的地址 </li><li>每个内存单元中存放1Byte数据【每8位0或1称 为1字节（Byte）】</li><li>内存单元个数称为内存容量</li><li>按工作方式分类：随机存取存储器（RAM），只读存储器（ROM） </li></ul></li><li>外存储器 <ul><li>联机外存：硬磁盘 </li><li>脱机外存：各种移动存储设备</li></ul></li></ul></li><li>输入/输出接口 <ul><li>接口是CPU与外部设备间的桥梁</li><li>主要功能：<ul><li>数据缓冲寄存；</li><li>信号电平或类型的转换；</li><li>实现主机与外设间的运行匹配。 </li></ul></li></ul></li><li>总线 <ul><li>是一组导线和相关的控制、驱动电路的集合。</li><li>是计算机系统各部件之间传输地址、数据和控制信息的通道 </li><li>地址总线（AB） 数据总线（DB） 控制总线（CB） </li></ul></li></ul><h2 id="第三讲-微机工作过程"><a href="#第三讲-微机工作过程" class="headerlink" title="第三讲 微机工作过程"></a>第三讲 微机工作过程</h2><ul><li>计算机的工作就是按照一定的顺序，一条条地执行指令</li><li>指令： 由人向计算机发出的、能够为计算机所识别的命令 </li><li>过程：<strong>取指令</strong>-&gt;<strong>分析指令</strong>-&gt;读取操作数-&gt;<strong>执行指令</strong>-&gt;存放结果 </li><li>顺序执行： 一条指令执行完了再执行下一条指令。 <ul><li>执行时间=取指令+分析指令+执行指令 </li><li>设：三个部分的执行时间均为Δt，则：执行n条指令时间T0为：  </li><li>T0=3nΔt </li></ul></li><li>并行执行： 同时执行两条或多条指令。 <ul><li>仅第1条指令需要3 Δt时间，之后每经过1 Δt，就有一条指令执行结束</li><li>执行时间： T =3Δt +（ n-1）Δt </li></ul></li><li>并行： 更高的效率，更高的复杂度 </li><li>相对于顺序执行方式，指令并行执行的优势用加速比S表示： <ul><li>S=顺序执行花费的时间/并行执行花费的时间 </li><li>例： 3n Δt /（3Δt +（ n-1）Δt) =3n/（2+n） </li></ul></li><li>冯 • 诺依曼计算机的工作原理: 存储程序工作原理,结构特点:运算器为核心 </li><li>冯 • 诺依曼机的工作过程 <ul><li>取一条指令的工作过程： <ul><li>① 将指令所在地址赋给程序计数器PC； </li><li>② PC内容送到地址寄存器AR，PC自动加1； </li><li>③ 把AR的内容通过地址总线送至内存储器，经地址译码器译码，选中相应单元。 </li><li>④ CPU的控制器发出读命令。 </li><li>⑤ 在读命令控制下，把所选中单元的内容（即指令操作码）读到数据总线 DB。 </li><li>⑥ 把读出的内容经数据总线送到数据寄存器DR。 </li><li>⑦ 指令译码:数据寄存器DR将它送到指令寄存器IR，然后再送到指令译码器ID </li></ul></li><li>特点： <ul><li>程序存储，共享数据，顺序执行 </li><li>属于顺序处理机，适合于确定的算法和数值数据的处理。</li></ul></li><li>不足： <ul><li>与存储器间有大量数据交互，对总线要求很高；</li><li>执行顺序由程序决定，对大型复杂任务较困难；</li><li>以运算器为核心，处理效率较低；</li><li>由PC控制执行顺序，难以进行真正的并行处理。 </li></ul></li></ul></li><li>哈佛结构 <ul><li>指令和数据分别存放在两个独立的存储器模块中； </li><li>CPU与存储器间指令和数据的传送分别采用两组独立的总线；</li><li>可以在一个机器周期内同时获得指令操作码和操作数。 </li></ul></li></ul><h2 id="第四讲-常用数制"><a href="#第四讲-常用数制" class="headerlink" title="第四讲 常用数制"></a>第四讲 常用数制</h2><ul><li>计算机中的常用计数制：十进制 ，二进制数 ，十六进制数 ，八进制数 </li></ul><h2 id="第五讲-编码"><a href="#第五讲-编码" class="headerlink" title="第五讲 编码"></a>第五讲 编码</h2><ul><li>编码：<ul><li>信息从一种形式或格式转换为另一种形式的过程</li><li>用代码来表示各种信息，以便于计算机处理。 </li></ul></li><li>需要编码的信息种类：数值，字符，声音，图形，图像 </li><li>所有需要由计算机处理的信息，都需要编码，使所有信息都以二进制码形式表示</li><li>计算机中的编码 <ul><li>数值编码：<ul><li>二进制码</li><li>BCD码 </li></ul></li><li>西文字符编码<ul><li>ASCII码 </li></ul></li></ul></li><li>BCD（Binary Coded Decimal）码 <ul><li>用二进制表示的十进制数</li><li>特点： <ul><li>保留十进制的权，数字用0和1表示。</li></ul></li></ul></li><li>8421BCD编码： <ul><li>用4位二进制码表示1位十进制数，每4位之间有一个空格 </li><li>1010—1111是非法BCD码</li><li>（0001 0001 .0010 0101）BCD    =11 .25    =（1011 .01）B </li></ul></li><li>BCD码在计算机中的存储方式 <ul><li>以压缩BCD码形式存放：<ul><li>用4位二进制码表示1位BCD码</li><li>一个存储单元中存放2位BCD数</li></ul></li><li>以扩展BCD码形式存放 <ul><li>用8位二进制码表示1位BCD码.即高4位为0，低4位为有效位 </li><li>每个存储单元存放1位BCD </li></ul></li></ul></li><li>ASCII码 <ul><li>西文字符编码：将每个字母、数字、标点、控制符用1Byte二进制码表示 </li><li>标准ASCII的有效位：7bit，最高位默认为0 </li></ul></li><li>ASCII码的奇偶校验 <ul><li>奇校验：加上校验位后编码中“1”的个数为奇数。</li><li>偶校验：加上校验位后编码中“1”的个数为偶数。 </li></ul></li></ul><h2 id="第六讲-数及其运算"><a href="#第六讲-数及其运算" class="headerlink" title="第六讲 数及其运算"></a>第六讲 数及其运算</h2><ul><li>定点数</li><li>浮点数 <ul><li>小数点的位置可以左右移动的数</li><li>规格化浮点数：尾数部分用纯小数表示，即小数点右边第1位不为0 </li></ul></li><li>无符号数</li><li>有符号数：用最高位表示符号，其余是数值，0正，1负<ul><li>原码：最高位为符号位，其余为真值部分<ul><li>[X]原=符号位+|绝对值| </li><li>有[+0]和[-0]之分</li></ul></li><li>反码：<ul><li>若X&gt;0 ，则 [X]反 = [X]原 </li><li>若X&lt;0， 则 [X]反 = 对应原码的符号位不变，数值部分按位求反。</li><li>有[+0]和[-0]之分</li></ul></li><li>补码：<ul><li>若X&gt;0， 则 [X]补 = [X]反= [X]原</li><li>若X&lt;0， 则 [X]补 = [X]反+1 </li><li>没有[+0]和[-0]之分</li></ul></li></ul></li><li>无符号整数的表示范围(n表示字长)： 0 ≤ X ≤ 2^n - 1</li><li>有符号整数的表示范围： <ul><li>原码和反码： -（2^(n-1) -1） ≤ X ≤ 2^(n-1) -1 </li><li>补码： -2^(n-1) ≤ X ≤ 2^(n-1) -1 </li><li>对8位二进制数：<ul><li>原码： -127 ～+127</li><li>反码： -127 ～+127</li><li>补码： -128 ～+127 </li></ul></li></ul></li></ul><h2 id="第七讲-基本逻辑运算和逻辑门"><a href="#第七讲-基本逻辑运算和逻辑门" class="headerlink" title="第七讲 基本逻辑运算和逻辑门"></a>第七讲 基本逻辑运算和逻辑门</h2><ul><li>逻辑，命题，推理</li><li>基本逻辑运算：与或非</li><li>逻辑运算是按位进行的运算，低位运算结果对高位运算不产生影响 </li><li>算术运算是两个数之间的运算，低位运算结果将对高位运算产生影响</li></ul><h2 id="第八讲-基本逻辑运算及其门电路"><a href="#第八讲-基本逻辑运算及其门电路" class="headerlink" title="第八讲 基本逻辑运算及其门电路"></a>第八讲 基本逻辑运算及其门电路</h2><ul><li>与非，或非，异或，同或</li></ul><h1 id="第二章-微处理器与总线"><a href="#第二章-微处理器与总线" class="headerlink" title="第二章 微处理器与总线"></a>第二章 微处理器与总线</h1><h2 id="第九讲-8088-8086微处理器"><a href="#第九讲-8088-8086微处理器" class="headerlink" title="第九讲 8088/8086微处理器"></a>第九讲 8088/8086微处理器</h2><ul><li>8088/8086 CPU的特点 <ul><li>采用并行流水线工作方式<ul><li>通过设置指令预取队列实现</li></ul></li><li>对内存空间实行分段管理<ul><li>将内存分为4个段并设置地址段寄存器，以实现对1MB空间的寻址</li></ul></li><li>支持协处理器</li></ul></li><li>8088/8086可工作于两种模式下 <ul><li>最小模式：单处理器模式，所有控制信号由微处理器产生<ul><li><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200605180344.png"></li></ul></li><li>最大模式：最大模式为多处理器模式，部分控制信号由外部总线控制器产生 <ul><li><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200605180420.png"></li></ul></li></ul></li></ul><h2 id="第十讲-8088的主要引线及其内部结构"><a href="#第十讲-8088的主要引线及其内部结构" class="headerlink" title="第十讲 8088的主要引线及其内部结构"></a>第十讲 8088的主要引线及其内部结构</h2><ul><li>8088最小模式下的主要引脚信号<ul><li>完成一次访问内存或接口所需要的主要信号</li><li>与外部同步控制信号</li><li>中断请求和响应信号</li><li>总线保持和响应信号<br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200605180737.png"></li></ul></li><li>主要控制信号<ul><li>WR：  写信号；</li><li>RD：  读信号；</li><li>IO/M：为“0”表示访问内存，为“1”表示访问接口；</li><li>DEN： 低电平有效时，允许进行读/写操作；</li><li>DT/R：数据收发器的传送方向控制；</li><li>ALE：  地址锁存信号；</li><li>RESET：复位信号。 </li></ul></li><li>中断请求和响应信号<ul><li>INTR：可屏蔽中断请求输入端</li><li>NMI： 非屏蔽中断请求输入端 </li><li>INTA：中断响应输出端 </li></ul></li><li>总线保持信号<ul><li>HOLD：总线保持请求信号输入端。当CPU以外的其他设备要求占用总线时，通过该引脚向CPU发出请求。</li><li>HLDA：总线保持响应信号输出端。CPU对HOLD信号的响应信号。 </li></ul></li><li>微处理器读取一条指令的控制过程<ul><li>1.发出读取数据所在的目标地址<ul><li>内存储器单元地址</li><li>I/O接口地址 </li></ul></li><li>2.发出读控制信号</li><li>3.送出传输的数据 </li></ul></li><li>地址线和数据线：<ul><li>20位地址信号(20根地址线)–》可产生2^20=1M个编码 </li><li>8位数据信号(8位数据线)–》可同时传输8bit二进制码 </li></ul></li><li>8088内部结构：<ul><li>执行单元EU<ul><li>构成：运算器，8个通用寄存器，1个标志寄存器，EU部分控制电路</li><li>功能：指令译码，指令执行，暂存中间运算结果，保存运算结果特征 </li></ul></li><li>总线接口单元BIU<ul><li>功能：</li><li>从内存中取指令到指令预取队列，指令预取队列是并行流水线工作的基础</li><li>负责与内存或输入/输出接口之间的数据传送</li><li>在执行转移程序时，BIU使指令预取队列复位，从指定的新地址取指令，并立即传给执行单元执行。 </li></ul></li></ul></li><li>指令预取队列的存在使EU和BIU两个部分可同时进行工作 </li><li>8088和8086CPU引线功能比较 <ul><li>数据总线宽度不同：8088的外部总线宽度是8位，8086为16位。 </li><li>访问存储器和输入输出控制信号含义不同：8088——IO/M=0表示访问内存；8086——IO/M=1表示访问内存。 </li><li>其他部分引线功能的区别 </li></ul></li></ul><h2 id="第十一讲-8088CPU内部寄存器"><a href="#第十一讲-8088CPU内部寄存器" class="headerlink" title="第十一讲 8088CPU内部寄存器"></a>第十一讲 8088CPU内部寄存器</h2><ul><li>含14个16位寄存器，按功能可分为三类： <ul><li>8个通用寄存器<ul><li>数据寄存器（AX，BX，CX，DX）</li><li>地址指针寄存器（SP，BP）</li><li>变址寄存器（SI，DI） </li></ul></li><li>4个段寄存器</li><li>2个控制寄存器  </li></ul></li><li>通用寄存器：<ul><li>数据寄存器：8088/8086含4个16位数据寄存器，它们又可分为8个8位寄存器，即： <ul><li>AX——-AH，AL：累加器，所有I/O指令都通过AX与接口传送信息，中间运算结果也多放于AX中； </li><li>BX——-BH，BL：基址寄存器,在间接寻址中用于存放基地址</li><li>CX——-CH，CL：计数寄存器,用于在循环或串操作指令中存放计数值</li><li>DX——-DH，DL：数据寄存器,在间接寻址的I/O指令中存放I/O端口地址；在32位乘除法运算时，存放高16位数。 </li></ul></li><li>地址指针寄存器:<ul><li>SP：堆栈指针寄存器，其内容为栈顶的偏移地址 </li><li>BP：基址指针寄存器，常用于在访问内存时存放内存单元的偏移地址。 </li></ul></li><li>变址寄存器 <ul><li>SI：源变址寄存器</li><li>DI：目标变址寄存器</li><li>变址寄存器在指令中常用于存放数据在内存中的地址。 </li></ul></li></ul></li><li>BX与BP在应用上的区别 <ul><li>作为通用寄存器，二者均可用于存放数据；</li><li>作为基址寄存器，用BX表示所寻找的数据在数据段；用BP 则表示数据在堆栈段。 </li></ul></li><li>段寄存器:<ul><li>作用:用于存放相应逻辑段的段基地址</li><li>8086/8088内存中逻辑段的类型<ul><li>代码段: 存放指令代码</li><li>数据段: 存放操作的数据</li><li>附加段: 存放附加的操作的数据</li><li>堆栈段: 存放暂时不用但需保存的数据。 </li></ul></li><li>CS:代码段寄存器，存放代码段的段基地址。</li><li>DS:数据段寄存器，存放数据段的段基地址。</li><li>ES:附加段寄存器，存放附加段的段基地址。</li><li>SS:堆栈段寄存器，存放堆栈段的段基地址 </li><li>段寄存器的值表明相应逻辑段在内存中的位置 </li></ul></li><li>控制寄存器：<ul><li>指令指针控制寄存器IP</li><li>状态标志寄存器FLAGS<ul><li>状态标志位：<ul><li>CF:进位标志位。加(减)法运算时，若最高位有进(借)位则CF=1</li><li>OF:溢出标志位。当算术运算的结果超出了有符号数的可表达范围时，OF=l</li><li>ZF:零标志位。当运算结果为零时ZF=1</li><li>SF:符号标志位。当运算结果的最高位为1时，SF=l</li><li>PF:奇偶标志位。运算结果的低8位中“1”的个数为偶数时PF=l</li><li>AF:辅助进位标志位。加(减)操作中，若Bit3(D3)向Bit4(D4)有进位(借位)， AF=1</li></ul></li><li>控制标志位：<ul><li>TF:单步陷阱标志位，也叫跟踪标志位。TF=1时，使CPU处于单步执行指令的工作方式。</li><li>IF:中断允许标志位。IF=1时，CPU可以响应中断请求。</li><li>DF:方向标志位。在数据串操作时确定操作的方向。 </li></ul></li></ul></li></ul></li></ul><h2 id="第十二讲-实模式下的存储器寻址"><a href="#第十二讲-实模式下的存储器寻址" class="headerlink" title="第十二讲 实模式下的存储器寻址"></a>第十二讲 实模式下的存储器寻址</h2><ul><li>存储单位地址及其内容表示<ul><li>若X表示某个单元地址，则[X]表示X单元的内容</li><li>例如：[0004H]=34H代表34存放在4号单元，而[0004H]=1234H，代表34存放在4号单元，12存放在5号单元</li></ul></li><li>字的存储<ul><li>占连续两个字节（16位）</li><li>低对低，高对高</li><li>用低位地址来表示字的地址</li></ul></li><li>规则存放，非规则存放<ul><li>8088：数据总线8位，每次传送1个字节</li><li>8086：数据总线16位<ul><li>字：16位，规则字，以偶地址开始存放</li><li>字节： 高8位传送奇地址，低8位传送偶地址</li></ul></li></ul></li><li>内存储器管理<ul><li>8088CPU是16位体系结构的微处理器</li><li>可以同时处理16位二进制码</li><li>8088CPU需要管理1MB内存</li></ul></li><li>分段技术<ul><li>分为若干个逻辑段，取内地址，用16位表示，每段最大64KB</li><li>对段首地址（物理地址）规定，段首地址低4位为0，例如：00000H,00010H,FFFF0H</li><li>段地址：段的起始地址的高16位</li><li>偏移地址：段内相对于段的起始地址的偏移量（字节数）</li></ul></li><li>实地址模式下的存储器地址变换<ul><li>内存物理地址由段基地址和偏移地址组成</li><li>物理地址=段基地址×16+偏移地址</li></ul></li><li>内存地址变换<ul><li>内存单元编址<ul><li>段（基）地址</li><li>段内地址（相对地址/偏移地址）</li></ul></li><li>存储器的编址<ul><li>段（基）地址</li><li>相对地址（偏移地址）</li><li>逻辑段的起始地址称为段首，段首的偏移地址0000H</li></ul></li></ul></li><li>段寄存器：<ul><li>作用：用于存放相应逻辑段的段基地址</li><li>8086/8088内存中逻辑段的类型<ul><li>代码段==&gt;CS（代码段寄存器）<ul><li>CS×16+IP</li></ul></li><li>数据段==&gt;DS（数据段寄存器）<ul><li>DS×16+偏移地址</li></ul></li><li>附加段==&gt;ES（附加段寄存器）<ul><li>ES×16+偏移地址</li></ul></li><li>堆栈段==&gt;SS（堆栈段寄存器）<ul><li>SS×16+SP</li></ul></li></ul></li><li>8086/8088内存中每类逻辑段的数量最多64K个</li></ul></li><li>逻辑段与逻辑地址<ul><li>内存的分段式逻辑分段，不是物理段</li><li>两个逻辑段可以完全重合或部分重合</li></ul></li><li>堆栈及堆栈段的使用<ul><li>堆栈： <ul><li>内存中一个特殊区域，用于存放暂时不用或需要保护的数据。</li><li>常用于响应中断或子程序调用</li></ul></li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200626171551.png"></p><h2 id="第十三讲-8088-系统总线"><a href="#第十三讲-8088-系统总线" class="headerlink" title="第十三讲 8088 系统总线"></a>第十三讲 8088 系统总线</h2><ul><li>总线时序<ul><li>CPU工作时序<ul><li>CPU各引脚信号在时间上的关系</li></ul></li><li>总线周期<ul><li>CPU完成一次访问内存（或接口）操作所需要的时间</li><li>8086的基本总线周期为4个时钟周期，每个时钟周期间隔称为一个T状态（8086/8088：5MHz时钟信号，时钟周期T=200ns）<br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200626181818.png"></li><li>T1 状态：BIU将RAM或I/O地址放在地址/数据复用             总线（AD）上。</li><li>T2 状态：   <ul><li>读总线周期： A/D总线为接收数据做准备。改变线路的方向。</li><li>写总线周期： A/D总线上形成待写的数据，且保持到总线周期的结束(T4)。</li></ul></li><li>T3, T4:对于读或写总线周期，AD总线上均为数据。</li><li>Tw: 当RAM或I/O接口速度不够时，T3与 T4 之间可插入等待状态 Tw 。</li><li>Ti : 当BIU无访问操作数和取指令的任务时，8086不执行总线操作，总线周期处于空闲状态 Ti 。</li></ul></li></ul></li><li>总线：<ul><li>按层次结构分类：<ul><li>CPU总线</li><li>系统总线</li><li>外部总线</li></ul></li><li>按传送信息的类别分类：<ul><li>地址总线</li><li>数据总线</li><li>控制总线</li></ul></li><li>按总线在微机系统的位置分类：<ul><li>片内总线</li><li>片间总线</li><li>系统总线</li><li>通信总线</li></ul></li><li>总线的基本功能<ul><li>数据传送</li><li>仲裁控制</li><li>出错处理</li><li>总线驱动</li></ul></li><li>总线的主要性能指标<ul><li>总线带宽（B/S）<ul><li>单位时间内总线上可传送的数据量</li><li>总线带宽=位宽×工作频率</li></ul></li><li>总线位宽（bit）<ul><li>能同时传送的数据位数</li></ul></li><li>总线的工作频率（MHz）<ul><li>总线带宽=（位宽/8）×（工作频率/每个存储周期的时钟数）</li></ul></li></ul></li></ul></li><li>引脚信号设计特点<ul><li>分时复用，如引脚AD0-AD15<ul><li>如何实现：增加地址锁存器</li><li>8282三位锁存器</li><li>8286八位数据收发器</li></ul></li><li>两种工作模式复用<ul><li>最大模式</li><li>最小模式</li></ul></li></ul></li></ul><h1 id="第三章-指令系统概述"><a href="#第三章-指令系统概述" class="headerlink" title="第三章 指令系统概述"></a>第三章 指令系统概述</h1><h2 id="第十四讲-8088-8086指令系统"><a href="#第十四讲-8088-8086指令系统" class="headerlink" title="第十四讲 8088/8086指令系统"></a>第十四讲 8088/8086指令系统</h2><ul><li>指令：控制计算机完成某种操作的命令</li><li>指令系统：处理器所能识别的所有指令的集合</li><li>指令的兼容性：同一系列机的指令都是兼容的</li><li>一条指令应包含的信息：<ul><li>运算数据的来源</li><li>运算结果的去向</li><li>执行的操作</li></ul></li><li>指令格式<br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200626182906.png"></li><li>指令中的操作数<ul><li>立即数：参加操作的数据本身，可以是8位或16位，只能作为源操作数，无法作为目标操作数<ul><li><code>MOV AX, 1234H</code></li></ul></li><li>寄存器：数据存放地址<ul><li><code>MOV AX, BX</code></li></ul></li><li>存储器：数据存放地址<ul><li>参加运算的数存放在存储器的某一个或两个单元中</li><li>表现形式： [操作数在内存中的偏移地址]</li><li><code>MOV AL, [1200H]</code></li></ul></li></ul></li></ul><h2 id="第十五讲-指令的寻址方式"><a href="#第十五讲-指令的寻址方式" class="headerlink" title="第十五讲 指令的寻址方式"></a>第十五讲 指令的寻址方式</h2><ul><li><p>操作数可能的来源或运算结果可能的去处：</p><ul><li>由指令直接给出</li><li>寄存器</li><li>内存单元</li></ul></li><li><p>寻找操作数所在地址的方法可以有三种大类型：</p><ul><li>指令直接给出的方式</li><li>存放于寄存器中的寻址方式</li><li>存放于存储器中的寻址方式</li></ul></li><li><p>1.直接寻址：</p><ul><li>指令中直接给出操作数的偏移地址</li><li>直接寻址方式下，操作数默认为在数据段，但允许段重设，即由指令给出所在逻辑段。 </li><li><code>MOV AX，ES：[1200H]</code>  ES：段重设符</li></ul></li><li><p>2.寄存器间接寻址</p><ul><li>操作数存放在内存中，数据在内存中的偏移地址为方括号中通用寄存器的内容</li><li>仅有4个通用寄存器可用于存放数据的偏移地址，<code>BX</code>，<code>BP</code>，<code>SI</code>，<code>DI</code><ul><li>若使用<code>BX</code>,<code>SI</code>,<code>DI</code>，则操作数在数据段<code>DS</code>中<ul><li>物理地址=DS×16+{BX/SI/DI}</li></ul></li><li>若使用<code>BP</code>，则操作数在堆栈段<code>SS</code>中<ul><li>物理地址=SS×16+BP</li></ul></li></ul></li><li>间接寻址的一般格式：[ 间址寄存器 ] </li><li>例： <code>MOV AX，[BX]</code></li><li>可以段重设</li></ul></li><li><p>3.寄存器相对寻址</p><ul><li>操作数的偏移地址为寄存器的内容加上一个位移量</li><li>相对寻址主要用于一维数组的操作</li><li><code>MOV AX，[BX+DATA]</code></li></ul></li><li><p>4.基址、变址寻址</p><ul><li>操作数的偏移地址为<ul><li>一个基址寄存器的内容 + 一个变址寄存器的内容；</li></ul></li><li>操作数的<strong>段地址由选择的基址寄存器决定</strong><ul><li>基址寄存器为<code>BX</code>，默认在数据段<code>DS</code></li><li>基址寄存器为<code>BP</code>，默认在堆栈段<code>SS</code></li></ul></li><li>基址变址寻址方式与相对寻址方式一样，主要用于一维数组操作。 </li></ul></li><li><p>5.基址、变址、相对寻址</p><ul><li>操作数的偏移地址为：<ul><li>基址寄存器内容+变址寄存器内容+位移量</li></ul></li><li>操作数的段地址由选择的基址寄存器决定。</li><li>基址变址相对寻址方式主要用于二维表格操作。</li><li>例如：<code>MOV AL, [BP][DI]5</code>==&gt;也可以表示为<code>[BP+DI+5]</code></li></ul></li><li><p>6.隐含寻址</p><ul><li>指令中隐含了一个或两个操作数的地址，即操作数在默认的地址中。</li><li>例：<ul><li><code>MUL BL</code></li></ul></li><li>指令执行：<ul><li><code>AL×BL--&gt;AX</code></li></ul></li></ul></li><li><p>I/O端口寻址方式</p><ul><li>直接端口寻址<ul><li>由指令提供一个8位端数（0-255）</li></ul></li><li>间接端口寻址<ul><li>由DX寄存器给出，寻址64KB</li></ul></li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200626201755.png"></p><h2 id="第十六讲-数据传送指令"><a href="#第十六讲-数据传送指令" class="headerlink" title="第十六讲 数据传送指令"></a>第十六讲 数据传送指令</h2><ul><li>8086指令系统从功能上包括六大类：<ul><li>数据传送</li><li>算术运算</li><li>逻辑运算和移位</li><li>串操作</li><li>程序控制</li><li>处理器控制</li></ul></li><li>数据传送类指令<ul><li>1.通用数据传送指令<ul><li>一般数据传送指令<ul><li><code>MOV</code></li><li>格式：<code>MOV dest,src</code></li><li>操作：<code>src-&gt;dest</code></li><li>例子：<code>MOV AL, BL</code></li><li>注意点：两操作数字长必须相同；两操作数不允许同时为存储器操作数；两操作数不允许同时为段寄存器；在源操作数是立即数时，目标操作数不能是段寄存器；IP和CS不作为目标操作数，FLAGS一般也不作为操作数在指令中出现。 </li><li><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200626203308.png"></li></ul></li><li>堆栈操作指令<ul><li>先进后出，以字为单位</li><li>压栈：<code>PUSH OPRD</code> 16位寄存器或存储器两单元</li><li><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200626204137.png"></li><li>出栈：<code>POP OPRD</code></li><li><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200626204453.png"></li><li>指令的操作数必须是16位；操作数可以是寄存器或存储器两单元，但不能是立即数；不能从栈顶弹出一个字给CS；PUSH和POP指令在程序中一般成对出现；PUSH指令的操作方向是从<code>高地址向低地址</code>，而POP指令的操作正好相反。 </li><li>堆栈指针寄存器SP指向栈顶位置</li></ul></li><li>交换指令<ul><li>格式：<code>XCHG REG，MEM/REG</code></li><li>注：两操作数必须有一个是寄存器操作数;不允许使用段寄存器。</li><li>例：<code>XCHG AX, BX</code>,<code>XCHG [2000], CL</code></li></ul></li><li>查表转换指令<ul><li>格式：<code>XLAT</code></li><li>说明：用BX的内容代表表格首地址，AL内容为表内位移量，BX+AL得到要查找元素的偏移地址</li><li>操作：将BX+AL所指单元的内容送AL（将BX为首地址的,偏移地址为AL的内容送给AL。）</li></ul></li><li>字位扩展指令 <ul><li>将符号数的符号位扩展到高位；</li><li>指令为零操作数指令，采用隐含寻址，隐含的操作数为AX及AX，DX</li><li>无符号数的扩展规则为在高位补0</li><li>字节到字：<code>CBW</code>，将AL内容扩展到AX ，若AL最高位=1，则执行后AH=FFH，若AL最高位=0，则执行后AH=00H 。AL不变（即将AL的符号位移至AH）<ul><li>CBW属符号扩展指令，它可以把8位扩展到16位，扩展前后两数的真值不变，主要用于数据类型不同时用符号扩展指令可以使得数据类型相同。</li></ul></li><li>字到双字：<code>CWD</code>，将AX内容扩展到DX AX ，若AX最高位=1，则执行后DX=FFFFH，若AX最高位=0，则执行后DX=0000H<ul><li>CWD的作用是将带符号的16位整数（AX）转为32位的带符号位的整数(DX:AX),例如：AX=0xFFFE, 转为32位带符号位的整数时，DX=0xFFFF,AX=0XFFFE.又例如：AX=0x0002,转为带符号位的整数时DX=0x0000,AX=0x0002.</li></ul></li></ul></li></ul></li><li>2.输入输出指令<ul><li>从端口地址读入数据到累加器/将累加器的值输出到端口中 </li><li>指令格式：<ul><li>输入指令： <code>IN acc，PORT</code></li><li>输出指令 ：<code>OUT PORT，acc</code></li></ul></li><li>根据端口地址码的长度，指令具有两种不同的端口地址表现形式：直接寻址，间接寻址</li></ul></li><li>3.地址传送指令<ul><li><code>LEA</code>取偏移地址指令<ul><li>将变量的16位偏移地址写入到目标寄存器</li><li><code>LEA REG,SRC</code></li></ul></li><li><code>LDS</code>指令<ul><li><code>LDS</code>（Load pointer using DS）的一般格式：</li><li><code>LDS 通用寄存器，存储器操作数(32位)</code></li></ul></li><li><code>LES</code>指令<ul><li><code>LDS</code>和<code>LES</code>均用于将一个32位的远地址指针写入到目标寄存器。</li><li><code>LES</code>（Load pointer using ES）的一般格式：</li><li><code>LES 通用寄存器，存储器操作数(32位)</code></li></ul></li></ul></li><li>4.标志传送指令<ul><li>隐含操作数AH,将FLAGS的低8位装入AH <ul><li><code>LAHF</code>（Load AH from Flags）</li><li><code>SAHF</code>（Store AH into Flags）</li></ul></li><li>隐含操作数FLAGS<ul><li><code>PUSHF</code>（Push flags onto stack）</li><li><code>POPF</code>（Pop flags off stack）</li></ul></li><li>除标志传送指令外，其它指令的执行对标志位不产生影响</li></ul></li></ul></li></ul><h1 id="第四章-算术运算，逻辑运算与移位操作指令"><a href="#第四章-算术运算，逻辑运算与移位操作指令" class="headerlink" title="第四章 算术运算，逻辑运算与移位操作指令"></a>第四章 算术运算，逻辑运算与移位操作指令</h1><h2 id="第十七讲-算术运算类指令"><a href="#第十七讲-算术运算类指令" class="headerlink" title="第十七讲 算术运算类指令"></a>第十七讲 算术运算类指令</h2><ul><li>加法运算指令<ul><li>1.<code>ADD</code>加法指令<ul><li>格式：<code>ADD OPRD1，OPRD2</code></li><li>操作：<code>OPRD1+OPRD2--&gt;OPRD1</code></li><li>ADD指令的执行对全部6个状态标志位都产生影响</li></ul></li><li>2.<code>ADC</code>带进位的加法指令<ul><li><code> OPRD1+OPRD2+CF--&gt;OPRD1</code></li></ul></li><li>3.<code>INC</code>加1指令<ul><li>格式：<code>INC OPRD</code></li><li>操作：<code>OPRD+1--&gt;OPRD</code></li><li>常用于在程序中修改地址指针,OPRD不能是段寄存器,不能是立即数，除CF外，影响其他标志位</li></ul></li></ul></li><li>减法运算指令<ul><li>1.普通减法指令<code>SUB</code><ul><li>格式：<code>SUB OPRD1，OPRD2</code></li><li>操作：<code>OPRD1- OPRD2--&gt;OPRD1</code></li><li>对标志位的影响与ADD指令同</li></ul></li><li>2.考虑借位的减法指令<code>SBB</code><ul><li>操作：<code>OPRD1- OPRD2- CF--&gt;OPRD1</code></li></ul></li><li>3.减1指令<code>DEC</code><ul><li>格式：<code>DEC OPRD</code></li><li>操作：<code>OPRD - 1--&gt;OPRD</code></li><li>除了不影响CF外，影响其他标志位</li></ul></li><li>4.比较指令<code>CMP</code><ul><li>格式：    <code>CMP OPRD1，OPRD2</code></li><li>操作：<code>OPRD1- OPRD2</code></li><li>指令执行的结果不影响目标操作数，仅影响标志位！</li><li><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200626210751.png"></li></ul></li><li>5.求补指令<code>NEG</code><ul><li><code>NEG OPRD</code></li><li>操作：<code>0-OPRD--&gt;OPRD</code></li></ul></li></ul></li><li>乘法指令<ul><li>乘法指令采用隐含寻址，隐含的是存放被乘数的累加器AL或AX及存放结果的AX，DX； </li><li>1.无符号的乘法指令MUL<ul><li><code>MUL OPRD</code>不能是立即数和段寄存器</li><li>操作：<ul><li>OPRD为字节数<code>AL×OPRD--&gt;AX</code></li><li>OPRD为16位数<code>AX×OPRD--&gt;DX,AX</code></li></ul></li></ul></li><li>2.带符号的乘法指令IMUL <ul><li>格式：<code>IMUL OPRD</code></li><li>指令格式及对操作数的要求与MUL指令相同。</li><li>指令执行原理：<ul><li>① 将两个操作数取补码（对负数按位取反加1，正数不变）；</li><li>② 做乘法运算；</li><li>③ 将乘积按位取反加1。</li></ul></li></ul></li></ul></li><li>除法指令<ul><li>1.无符号除法指令<ul><li>格式： <code>DIV OPRD</code></li><li>操作：<ul><li>操作数是字节(8位)：<code>AX/OPRD</code>,商–&gt;AL,余数–&gt;AH</li><li>操作数是字(16位)：<code>DX,AX/OPRD</code>,商–&gt;AX,余数–&gt;DX</li></ul></li></ul></li><li>2.有符号除法指令<ul><li>格式： <code>IDIV OPRD</code></li><li>指令格式及对操作数的要求与DIV指令相同。</li></ul></li><li>注：<ul><li>余数符号与被除数相同</li><li>范围<ul><li>双字/字：商范围 -32768到+32767</li><li>字/字节：商范围 -128到+127</li><li>超过范围按除数为0处理，产生0号中断<br>算术运算指令的执行大多对状态标志位会产生影响</li></ul></li></ul></li></ul></li></ul><h2 id="第十八讲-逻辑运算指令"><a href="#第十八讲-逻辑运算指令" class="headerlink" title="第十八讲 逻辑运算指令"></a>第十八讲 逻辑运算指令</h2><ul><li>逻辑运算指令<ul><li>对操作数的要求：<ul><li>大多与MOV指令相同。</li><li>“非”运算指令要求操作数不能是立即数；</li></ul></li><li>对标志位的影响<ul><li>除“非”运算指令，其余指令的执行都会影响除<code>AF</code>外的5个状态标志；</li><li>无论执行结果如何，都会使标志位<code>OF=CF=0</code>。</li><li>“非”运算指令的执行不影响标志位。 </li></ul></li></ul></li><li>1.”与”指令<ul><li>格式：<code>AND OPRD1，OPRD2</code></li><li>操作：两操作数相“与”，结果送目标地址。<code>(OPRD1)∧(OPRD2)--&gt;(OPRD1)</code></li><li>CF=0,OF=0,SF,ZF,PF有影响，对AF无影响</li></ul></li><li>2.”或”指令<ul><li>格式：<code>OR OPRD1，OPRD2</code></li><li>操作：两操作数相“或”，结果送目标地址 </li></ul></li><li>3.”非”指令<ul><li>格式：<code>NOT OPRD</code></li><li>操作：操作数按位取反再送回原地址</li></ul></li><li>4.”异或”指令<ul><li>格式：<code>XOR OPRD1，OPRD2</code></li><li>操作：两操作数相“异或”，结果送目标地址 </li></ul></li><li>5.”测试”指令<ul><li>格式：<code>TEST OPRD1，OPRD2</code></li><li>操作：执行“与”运算，但运算的结果不送回目标地址。</li><li>应用：常用于测试某些位的状态 </li></ul></li></ul><h2 id="第十九讲-移位操作指令"><a href="#第十九讲-移位操作指令" class="headerlink" title="第十九讲 移位操作指令"></a>第十九讲 移位操作指令</h2><ul><li>移位操作指令<ul><li>控制二进制位向左或向右移动的指令<ul><li>非循环移位指令</li><li>循环移位指令</li></ul></li><li>移动移动1位时由指令直接给出；移动两位及以上时，移位次数必须由CL指定</li></ul></li><li>1.非循环移位指令<ul><li>逻辑左移<code>SHL</code><ul><li>格式： <code>SHL OPR,CNT</code></li><li>注：<ul><li>OPR不能是立即数和段寄存器操作数</li><li>CNT移位次数，若为1，直接写在指令中，若为几，必须先写入CL中</li><li>对CF，OP，PF,ZF，SF有影响，对AF无意义</li></ul></li></ul></li><li>算术左移<code>SAL</code><ul><li>格式：<code>SAL OPR,CNT</code></li><li>操作同<code>SHL</code></li></ul></li><li>逻辑右移<code>SHR</code><ul><li>格式：<code>SHR OPR,CNT</code></li></ul></li><li>算术右移<code>SAR</code><ul><li>格式：<code>SAR OPR,CNT</code></li><li>操作：左边补上符号位，和之前的符号一样</li></ul></li></ul></li><li>2.循环移位指令<ul><li>不带进位位的循环移位<ul><li>左移 <code>ROL</code><ul><li>格式： <code>ROL OPR,CNT</code></li></ul></li><li>右移 <code>ROR</code><ul><li>格式： <code>ROR OPR,CNT</code></li></ul></li></ul></li><li>带进位位的循环移位<ul><li>左移 <code>RCL</code><ul><li>格式： <code>RCL OPR,CNT</code></li></ul></li><li>右移 <code>RCR</code><ul><li>格式： <code>RCR OPR,CNT</code></li></ul></li></ul></li></ul></li></ul><h1 id="第五章-串操作指令"><a href="#第五章-串操作指令" class="headerlink" title="第五章 串操作指令"></a>第五章 串操作指令</h1><h2 id="第二十讲-串操作指令"><a href="#第二十讲-串操作指令" class="headerlink" title="第二十讲 串操作指令"></a>第二十讲 串操作指令</h2><ul><li>针对数据块或字符串的操作 </li><li>可实现存储器到存储器的数据传送； </li><li>待操作的数据串称为源串，目标地址称为目标串。 </li><li>串操作指令的操作对象是多个字节数（一串字符或数据），因此，指令的执行需要确定：<ul><li>串所在的区域<ul><li>源串一般存放在数据段，偏移地址由SI指定。允许段重设。</li><li>目标串必须在附加段，偏移地址由DI指定</li></ul></li><li>串的首地址（原串、目标串起始地址）</li><li>串长度（大小）<ul><li>串长度值由CX指定 </li></ul></li><li>串的操作方向<ul><li>由DF标志位决定。指令根据DF状态自动修改地址指针<ul><li>DF=0 增地址方向 </li><li>DF=1 减地址方向 </li></ul></li></ul></li></ul></li><li>通过增加重复前缀， 可以实现对CX值的自动修改 <ul><li> 无条件重复</li><li>REP<ul><li>当CX≠0时，REP后的指令将继续重复执行</li><li>常用于传送类指令前–》未传完则继续传送 </li></ul></li><li>条件重复<ul><li>相等（为零）重复：REPE（REPZ）</li><li>CX≠0  ∩  ZF=1，则前缀后的指令将继续重复执行</li><li>不相等（不为零）重复：REPNE（ REPNZ）</li><li>CX≠0  ∩  ZF=0，则前缀后的指令将继续重复执行 </li><li>条件前缀常用于运算类指令前，当：<ul><li>1）操作未结束  AND  结果=0</li><li>2）操作未结束  AND  结果≠0 使其后的指令继续重复执行。 </li></ul></li></ul></li></ul></li><li>串操作指令 <ul><li>串传送 <code>MOVS</code> </li><li>串比较 <code>CMPS</code> </li><li>串扫描 <code>SCAS</code> </li><li>串装入 <code>LODS</code> </li><li>串送存 <code>STOS</code> </li></ul></li></ul><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200627102608.png"></p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200627102624.png"></p><h2 id="第二十一讲-串传送与串比较指令"><a href="#第二十一讲-串传送与串比较指令" class="headerlink" title="第二十一讲 串传送与串比较指令"></a>第二十一讲 串传送与串比较指令</h2><p>1.串传送指令</p><ul><li>功能：将原数据串传送到目标地址</li><li>格式：<ul><li><code>MOVS OPRD1，OPRD2</code></li><li><code>MOVSB</code>,按字节传送</li><li><code>MOVSW</code>，按字传送</li></ul></li><li>串传送指令常与无条件重复前缀连用</li></ul><p>2.串比较指令</p><ul><li>功能：用于实现两个数据串的比较</li><li>操作：<ul><li>目标串-源串，结果不写回目标地址</li><li>常与条件重复前缀连用</li></ul></li><li>格式：<ul><li><code>CMPS OPRD1，OPRD2</code></li><li><code>CMPSB</code></li><li><code>CMPSW</code></li></ul></li><li>前缀的操作对标志位不影响</li></ul><h2 id="第二十二讲-串扫描指令"><a href="#第二十二讲-串扫描指令" class="headerlink" title="第二十二讲 串扫描指令"></a>第二十二讲 串扫描指令</h2><ul><li><p>格式：</p><ul><li><code>SCAS OPRD</code></li><li><code>SCASB</code></li><li><code>SCASW</code></li></ul></li><li><p>执行与CMPS指令相似的操作，区别是：这里的源操作数是AX或AL</p></li><li><p>串扫描指令应用例：</p><ul><li>在ES段中从2000H单元开始存放了10个字符，寻找其中有无字符“A”。若有则记下搜索次数，将搜索次数写入到DATA1单元，并将存放“A”的地址写入DATA2单元。</li><li><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200627103549.png"></li></ul></li></ul><h2 id="第二十三讲-串装入与串存储指令"><a href="#第二十三讲-串装入与串存储指令" class="headerlink" title="第二十三讲 串装入与串存储指令"></a>第二十三讲 串装入与串存储指令</h2><p>1.串装入指令</p><ul><li>格式：<ul><li><code>LODS OPRD</code></li><li><code>LODSB</code></li><li><code>LODSW</code></li></ul></li><li>操作：<ul><li>对字节：AL  [DS:SI]</li><li>对  字：AX  [DS:SI]</li></ul></li></ul><p>2.串存储指令</p><ul><li><p>格式：</p><ul><li><code>STOS OPRD</code></li><li><code>STOSB</code></li><li><code>STOSW</code></li></ul></li><li><p>操作：</p><ul><li>对字节： AL  [ES:DI]</li><li>对  字： AX  [ES:DI]</li></ul></li><li><p>串操作指令应用注意事项:</p><ul><li>需要定义附加段<ul><li>目标操作数必须在附加段</li></ul></li><li>需要设置数据的操作方向<ul><li>确定DF的状态</li></ul></li><li>源串和目标串指针分别为SI和DI</li><li>串长度值必须由CX给出</li><li>注意重复前缀的使用方法<ul><li>传送类指令前加无条件重复前缀</li><li>串比较类指令前加条件重复前缀，但前缀不影响ZF状态 </li></ul></li></ul></li></ul><h1 id="第六章-程序与处理器控制指令"><a href="#第六章-程序与处理器控制指令" class="headerlink" title="第六章 程序与处理器控制指令"></a>第六章 程序与处理器控制指令</h1><h2 id="第二十四讲-程序控制指令"><a href="#第二十四讲-程序控制指令" class="headerlink" title="第二十四讲 程序控制指令"></a>第二十四讲 程序控制指令</h2><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200627104125.png"></p><ul><li>程序控制类指令<ul><li>转移指令<ul><li>通过修改指令的偏移地址或段地址及偏移地址实现程序的转移</li><li>无条件转移指令–&gt;无条件转移到目标地址<ul><li><code>JMP OPRD</code></li><li>段内直接转移<ul><li>当偏移量为8位时，称为段内直接短跳转<ul><li>格式：<code>JMP (SHORT) 标号</code></li><li>操作：<code>(IP)&lt;--disp8+(IP)</code></li></ul></li><li>当偏移量为8位时，称为段内直接近跳转<ul><li>格式：<code>JMP (NEAR PTR) 标号</code></li><li>操作：<code>(IP)&lt;--disp16+(IP)</code></li></ul></li></ul></li><li>段内间接转移<ul><li><code>JMP BP</code><ul><li>转向(SS):(BP)</li></ul></li><li><code>JMP BX</code><ul><li>转向(CS):(BX)</li></ul></li><li><code>JMP (WORD PTR) [BX][DI]</code><ul><li>转向(CS):(BX)+(DI)</li></ul></li></ul></li><li>段间直接转移<ul><li><code>JMP (FAR PTR) 标号</code></li><li>执行该指令时，将把标号所在的段的值送CS，将标号在所属段内的偏移量送IP，从而形成新的转移地址CS:IP</li></ul></li><li>段间间接转移<ul><li><code>JMP DWORD PTR [BX]</code></li><li>中间的<code>DWORD PTR</code>不能省略，表示存储器双字操作数</li></ul></li></ul></li><li>条件转移指令–&gt;当具备一定条件时转移到目标地址<ul><li><code>JC/JNC</code><ul><li>判断CF的状态。常用于两个无符号数大小比较</li></ul></li><li><code>JZ/JNZ</code><ul><li>判断ZF的状态。常用于循环体的结束判断</li></ul></li><li><code>JO/JNO</code><ul><li>判断OF的状态。常用于有符号数溢出的判断</li></ul></li><li><code>JP/JNP</code><ul><li>判断PF的状态。用于判断运算结果低8位中1的个数是否为偶数</li></ul></li><li><code>JS /JNS</code><ul><li>判断SF的状态。常用于判断数的性质 </li></ul></li><li><code>JA/JAE/JB/JBE</code><ul><li>判断CF或CF+ZF的状态。常用于无符号数大小的比较</li></ul></li><li><code>JG/JGE/JL/JLE</code><ul><li>判断SF+OF或SF+OF+ZF的状态。常用于有符号数大小的比较</li></ul></li><li><code>JCXZ</code><ul><li>可根据指令执行后CX的结果实现转移<br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200829201509.png"></li></ul></li></ul></li></ul></li><li>循环控制<ul><li><code>LOOP</code><ul><li>条件：CX≠0</li></ul></li><li><code>LOOPZ</code><ul><li>条件：CX≠0，且ZF=1</li></ul></li><li><code>LOOPNZ</code><ul><li>条件：CX≠0，且ZF=0</li></ul></li></ul></li><li>过程调用<ul><li>用于调用一个子过程，必须保护返回地址 </li><li>调用指令的执行过程<ul><li>① 保护断点：将调用指令的下一条指令的地址（断点）压入堆栈</li><li>② 获取子过程的入口地址：子过程第1条指令的偏移地址</li><li>③ 执行子过程：功能实现，参数的保存及恢复</li><li>④ 恢复断点，返回原程序：将断点偏移地址由堆栈弹出 </li></ul></li><li>段内调用：被调用程序与调用程序在同一代码段<ul><li><code>CALL NEAR PROCC</code></li></ul></li><li>段间调用:子过程与原调用程序不在同一代码段</li><li>返回指令:<ul><li>功能：从堆栈中弹出断点地址，返回原程序</li><li>格式：<code>RET</code></li><li>子程序的最后一条指令必须是RET</li></ul></li></ul></li><li>中断控制<ul><li>中断的概念:某种异常或随机事件使处理器暂时停止正在运行的程序，转去执行一段特殊处理程序，并在处理结束后返回原程序被中断处继续执行的过程。</li><li>中断指令：引起CPU产生一次中断的指令 <ul><li>格式：<code>INT n</code></li><li>说明： nх4</li></ul></li><li>中断指令的执行过程<ul><li>① 将FLAGS压入堆栈；</li><li>② 将INT指令的下一条指令的CS、IP压栈；</li><li>③ 由n×4得到存放中断向量的地址；</li><li>④ 将中断向量（中断服务程序入口地址）送CS和IP寄存器；</li><li>⑤ 转入中断服务程序</li></ul></li><li>中断返回指令:<ul><li>格式：<code>IRET</code></li><li>中断服务程序的最后一条指令，负责：恢复断点;恢复标志寄存器内容</li></ul></li></ul></li></ul></li></ul><h2 id="第二十五讲-处理器控制指令"><a href="#第二十五讲-处理器控制指令" class="headerlink" title="第二十五讲 处理器控制指令"></a>第二十五讲 处理器控制指令</h2><ul><li>这类指令用来对CPU进行控制，如修改标志寄存器，使CPU暂停，使CPU与外部设备同步等。<br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200627113916.png"></li></ul><h1 id="第七章-汇编语言"><a href="#第七章-汇编语言" class="headerlink" title="第七章 汇编语言"></a>第七章 汇编语言</h1><h2 id="第二十六讲-汇编语言程序设计"><a href="#第二十六讲-汇编语言程序设计" class="headerlink" title="第二十六讲 汇编语言程序设计"></a>第二十六讲 汇编语言程序设计</h2><ol><li>汇编语言源程序与汇编程序<br>（1）汇编语言源程序：用助记符编写<br>（2）汇编程序：源程序的编译程序</li><li>汇编语言程序设计与执行过程<br>（1）输入汇编语言源程序：源文件.ASM<br>（2）汇编MASM：目标文件.OBJ<br>（3）链接LINK：可执行文件.EXE<br>（4）调试TD：最终程序</li><li>汇编语言语句类型和格式<br>（1）语句类型：指令性语句，指示性语句<br>（2）语句格式：<br>指令性语句：<code>[标号：] [前缀] 助记符 [操作数]，[操作数] [ ；注释] </code><br>指示性语句格式： <code>[名字] 伪指令助记符 操作数 [，操作数，…] [ ；注释] </code></li><li>汇编语言语句中的操作数<br>(1)寄存器<br>(2)存储器单元<br>(3)常量:（数字/字符串）<br>(4)变量或标号<br>(5)表达式 ：算术运算；逻辑运算；关系运算；取值运算（<code>OFFSET</code>,<code>SEG</code>）和属性运算(<code>PTR</code>)；其它运算 </li></ol><h2 id="第二十七讲-数据定义伪代码"><a href="#第二十七讲-数据定义伪代码" class="headerlink" title="第二十七讲 数据定义伪代码"></a>第二十七讲 数据定义伪代码</h2><ol><li>数据定义伪指令<br>（1）用于定义数据区中变量的类型及其所占内存空间大小<br>（2）DB（Define Byte）:定义的变量为字节型<br>（3）DW （Define Word） :定义的变量为字类型<br>（4）DD （Define Double Word） :定义的变量为双字型<br>（5）DQ （Define Quadword） :定义的变量为4字型<br>（6）DT （Define Tenbytes） :定义的变量为10字节型</li><li>重复操作符<br>（1）当同样的操作数重复多次时，可以使用重复操作符<br>（2）作用：为一个数据区的各单元设置相同的初值<br>（3）格式：[变量名] 伪指令助记符 n DUP（初值 [,初值,… ] ）<br>（4）例：<code>M1 DB 10 DUP（0）</code></li><li>“？”的作用<br>（1）表示随机值，用于预留存储空间<br>（2）例：<code>MEM1 DB 34H，’A’，？</code>，例：<code>DW 20 DUP（？）</code></li><li>调整偏移量伪指令<br>（1）规定程序或变量在逻辑段中的起始地址<br>（2）格式：<code>ORG 表达式</code><br>（3）例：<figure class="highlight x86asm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DATA <span class="meta">SEGMENT</span></span><br><span class="line">ORG <span class="number">1200H</span></span><br><span class="line">BUFF <span class="built_in">DB</span> <span class="number">1</span>,<span class="number">2</span></span><br><span class="line">DATA ENDS </span><br></pre></td></tr></table></figure></li></ol><h2 id="第二十八讲-符号与段定义相关伪指令"><a href="#第二十八讲-符号与段定义相关伪指令" class="headerlink" title="第二十八讲 符号与段定义相关伪指令"></a>第二十八讲 符号与段定义相关伪指令</h2><ol><li>符号定义伪指令<br>（1）将表达式的值赋给一个名字。当源程序中需多次引用某一表达式时，可以利用EQU伪指令，用一个符号代替表达式，以便于程序维护。<br>（2）格式：<code>符号名 EQU 表达式</code><br>（3）操作：用符号名取代后边的表达式，不可重新定义<br>（4）例：<code>CONSTANT EQU 100 </code></li><li>段定义伪指令<br>（1）在汇编语言源程序中定义逻辑段<br>说明逻辑段的起始和结束<br>说明不同程序模块中同类逻辑段之间的联系形态<br>（2）格式：<code>段名 SEGMENT [定位类型] [组合类型] [’类别’] </code></li><li>设定段寄存器伪指令<br>（1）说明所定义逻辑段的性质<br>（2）格式：<code>ASSUME 段寄存器名:段名[，段寄存器名:段名，…] </code></li><li>结束伪指令<br>（1）表示源程序结束<br>（2）格式：<code>END [标号] </code></li></ol><h2 id="第二十九讲-其他伪指令"><a href="#第二十九讲-其他伪指令" class="headerlink" title="第二十九讲 其他伪指令"></a>第二十九讲 其他伪指令</h2><ol><li>过程定义伪指令<br>（1）用于定义一个过程体<br>（2）格式：<figure class="highlight x86asm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">过程名 PROC [ <span class="built_in">NEAR</span> / <span class="built_in">FAR</span> ]</span><br><span class="line">┇</span><br><span class="line"><span class="keyword">RET</span></span><br><span class="line">过程名 ENDP</span><br></pre></td></tr></table></figure>（3）<img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200627120345.png"></li><li>宏命令伪指令<br>（1）宏：源程序中由汇编程序识别的具有独立功能的一段程序代码<br>（2）当源程序中需要多次使用同一个程序段时，可以将该程序段定义为一个宏<br>（3）格式：<code>宏命令名 MACRO &lt;形式参数&gt;</code><br>（4）<img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200627120517.png"></li></ol><h2 id="第三十讲-系统功能调用"><a href="#第三十讲-系统功能调用" class="headerlink" title="第三十讲 系统功能调用"></a>第三十讲 系统功能调用</h2><ol><li>BIOS、DOS功能调用<br>（1）BIOS：驻留在ROM中的基本输入/输出系统<br>加电自检，装入引导，主要I/O设备处理程序及接口控制<br>（2）DOS：磁盘操作系统<br>DOS功能/BIOS功能调用是调用系统内核子程序<br>（3）BIOS、DOS功能调用：DOS功能与BIOS功能均通过中断方式调用，DOS和BIOS中断均可能影响AX</li><li>DOS软中断<br>（1）DOS中断包括：设备管理，目录管理，文件管理，其它用中断类型码区分<br>（2）DOS软中断：类型码为21H </li><li>单字符输入<br>（1）调用方法：<figure class="highlight x86asm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">MOV</span> <span class="number">AH</span>，<span class="number">01</span></span><br><span class="line"><span class="keyword">INT</span> <span class="number">21H</span></span><br></pre></td></tr></table></figure>（2）输入的字符在AL中 </li><li>字符串输入<br>（1）接收由键盘输入一串字符<br>（2）输入的字符串存储在内存指导区域中<br>（3）用户自定义缓冲区格式：<br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200627121105.png"></li><li>单字符显示输出<br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200627121240.png"></li><li>字符串显示输出<br>AH 功能号09H<br>DS：DX 待输出字符串的偏移地址<br>INT 21H</li><li>返回操作系统（DOS）功能<br>（1）功能号：4CH<br>（2）调用格式：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MOV AH，4CH</span><br><span class="line">INT 21H</span><br></pre></td></tr></table></figure>（3）功能：程序执行完该2条语句后能正常返回OS;常位于程序结尾处</li></ol><h1 id="第八章-半导体存储器"><a href="#第八章-半导体存储器" class="headerlink" title="第八章 半导体存储器"></a>第八章 半导体存储器</h1><h2 id="第三十一讲-半导体存储器概述"><a href="#第三十一讲-半导体存储器概述" class="headerlink" title="第三十一讲 半导体存储器概述"></a>第三十一讲 半导体存储器概述</h2><ol><li>半导体存储器<br>（1）由能够表示二进制数“0”和“1”的，具有记忆功能的半导体器件组成<br>（2）能存放一位二进制数的半导体器件称为一个存储元<br>（3）若干存储元构成一个存储单元</li><li>半导体存储器的分类</li></ol><ul><li>内存储器：<ul><li>随机存取存储器（RAM）<ul><li>静态存储器（SRAM）</li><li>动态存储器（DRAM）</li></ul></li><li>只读存储器（ROM）<ul><li>掩模ROM</li><li>一次性可写ROM</li><li>可读写ROM<ul><li>EPROM</li><li>EEPROM</li><li>PROM</li></ul></li></ul></li></ul></li></ul><ol start="3"><li>半导体存储器的主要技术指标<br>（1）存储容量：存储单元个数×每单元的二进制数位数<br>存储容量=2^m×N<br>（m：芯片地址线根数）<br>（N：芯片数据线根数）<br>（2）存取时间：实现一次读/写所需要的时间<br>（3）存取周期：连续启动两次独立的存储器操作所需间隔的最小时间<br>（4）可靠性，功耗</li></ol><h2 id="第三十二讲-微机中的存储器"><a href="#第三十二讲-微机中的存储器" class="headerlink" title="第三十二讲 微机中的存储器"></a>第三十二讲 微机中的存储器</h2><ol><li>微机中的存储器<br>（1）内存储器<br>主内存<br>高速缓冲存储器<br>（2）外存储器<br>联机外存<br>脱机外存<br>（3）虚拟存储器<br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200627143039.png"></li></ol><table><thead><tr><th align="center"></th><th align="center">内存</th><th align="center">外存</th></tr></thead><tbody><tr><td align="center">速度</td><td align="center">快</td><td align="center">慢</td></tr><tr><td align="center">容量</td><td align="center">小</td><td align="center">大</td></tr><tr><td align="center">单位容量价格</td><td align="center">高</td><td align="center">低</td></tr><tr><td align="center">制造材料</td><td align="center">半导体</td><td align="center">磁性材料</td></tr></tbody></table><ol start="2"><li><p>微机中的存储系统主要有：<br>（1）Cache存储器系统<br>（2）虚拟存储器系统</p></li><li><p>随机存取存储器<br>（1）特点：可以随机读或写操作；掉电后存储内容即丢失<br>（2）类型：静态随机存取存储器（SRAM）；动态随机存取存储器（DRAM）</p></li></ol><h2 id="第三十三讲-存储单元的编址"><a href="#第三十三讲-存储单元的编址" class="headerlink" title="第三十三讲 存储单元的编址"></a>第三十三讲 存储单元的编址</h2><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200627143427.png"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200627143454.png"></p><ol><li>地址译码电路<br>（1）单译码结构<br>（2）双译码结构<br>（3）3-8译码器（741S138）</li></ol><h2 id="第三十四讲-存储器扩展技术"><a href="#第三十四讲-存储器扩展技术" class="headerlink" title="第三十四讲 存储器扩展技术"></a>第三十四讲 存储器扩展技术</h2><ol><li>存储器扩展：用已有的存储器芯片构造一个需要的存储空间<br>（1）用多片存储芯片构成一个需要的内存空间；<br>（2）各存储器芯片在整个内存中占据不同的地址范围；<br>（3）任一时刻仅有一片（或一组）被选中。<br>（4）存储器芯片的存储容量等于：单元数×每单元的位数 </li><li>存储器扩展方法<br>（1）位扩展–》扩展字长<br>（2）字扩展–》扩展单元数<br>（3）字位扩展–》既扩展字长也扩展单元数</li></ol><h1 id="第九章-输入输出与中断技术"><a href="#第九章-输入输出与中断技术" class="headerlink" title="第九章 输入输出与中断技术"></a>第九章 输入输出与中断技术</h1><h2 id="第三十五讲-输入输出技术概述"><a href="#第三十五讲-输入输出技术概述" class="headerlink" title="第三十五讲 输入输出技术概述"></a>第三十五讲 输入输出技术概述</h2><ol><li>I/O接口<br>（1）接口要解决的问题<br>速度匹配👉数据的缓冲与暂存<br>信号的驱动能力👉信号驱动<br>信号形式和电平的匹配👉信号类型转换<br>信息格式👉信号格式转换<br>时序匹配（定时关系）<br>总线隔离👉三态门 </li><li>I/O端口及其编址<br>（1）端口：接口电路中用于缓存数据及控制信息的部件<br>（2）分类：数据端口，控制端口，状态端口<br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200627144642.png"><br>（3）I/0端口编址：为确保CPU能够访问到每个不同的端口<br>（4）寻址端口的方法：</li></ol><ul><li>先找到端口所在的接口电路芯片</li><li>再在该芯片上找具体访问的端口<ul><li>若接口中仅有一个端口，则找到芯片即找到端口</li><li>若接口中有多个端口，则找到芯片后需再找端口<br>（5）每个端口地址=片选地址（高位地址）+片内地址 </li></ul></li></ul><ol start="3"><li> I/O地址译码<br>（1）目的：确定端口的地址<br>（2）参加译码的信号：<br><code>#IOR，#IOW，高位地址信号</code><br>（3）对端口读/写信号的产生条件</li></ol><ul><li><code>IO/#M=1</code></li><li><code>#RD=0 #IOR=0</code></li><li><code>#WR=0 #IOW=0</code><br>（4）当接口只有一个端口时：无片内地址，全部地址信号均为高位地址（可全部参与译码），译码输出直接选择该端口；<br>（5）当接口具有多个端口时：则16位地址线的高位参与译码（决定接口的基地址），而低位则用于确定要访问哪一个端口</li></ul><h2 id="第三十六讲-简单接口芯片"><a href="#第三十六讲-简单接口芯片" class="headerlink" title="第三十六讲 简单接口芯片"></a>第三十六讲 简单接口芯片</h2><ol><li> 接口的分类及特点<br>（1）按传输信息的方向分类：<br>输入接口<br>输出接口<br>（2）按传输信息的类型分类：<br>数字接口<br>模拟接口<br>（3）按传输信息的方式分类：<br>并行接口<br>串行接口</li><li>接口特点<br>（1）输入接口：<br>要求对数据具有控制能力<br>常用三态门实现<br>（2）输出接口：<br>要求对数据具有锁存能力<br>常用锁存器实现</li></ol><h2 id="第三十七讲-基本输入输出方法"><a href="#第三十七讲-基本输入输出方法" class="headerlink" title="第三十七讲 基本输入输出方法"></a>第三十七讲 基本输入输出方法</h2><ol><li>基本输入/输出方法<br>（1）无条件传送：要求外设总是处于准备好状态<br>优点：软件及接口硬件简单<br>缺点：只适用于简单外设，适应范围较窄<br>（2）查询式传送：仅当条件满足时才能进行数据传送；每满足一次条件只能进行一次数据传送。<br>适用场合：外设并不总是准备好；对传送速率和效率要求不高<br>工作条件：外设应提供设备状态信息；接口应具备状态端口<br>（3）中断方式传送<br>特点：外设在需要时向CPU提出请求，CPU再去为它服务。服务结束后或在外设不需要时，CPU可执行自己的程序。<br>优点：CPU效率高，实时性好，速度快。<br>缺点：程序编制相对较为复杂。<br>（4）直接存储器存取(DMA) ：<br>特点：<br>①外设直接与存储器进行数据交换 ，CPU不再担当数据传输的中介者；<br>②总线由DMA控制器（DMAC）进行控制（CPU要放弃总线控制权），内存/外设的地址和读写控制信号均由DMAC提供。<br>③DMA传送方式有单元传送方式，快传送方式，请求传送方式<br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200627145513.png"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200627145541.png"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200627145557.png"></li></ol><h3 id="DMA控制器8237A"><a href="#DMA控制器8237A" class="headerlink" title="DMA控制器8237A"></a>DMA控制器8237A</h3><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200907143459.png"></p><h2 id="第三十八讲-中断技术"><a href="#第三十八讲-中断技术" class="headerlink" title="第三十八讲 中断技术"></a>第三十八讲 中断技术</h2><ol><li>中断的基本概念<br>（1）CPU执行程序时，由于发生了某种随机的事件(外部或内部)，<br>引起CPU暂时中断正在运行的程序，转去执行一段特殊的服务<br>程序，以处理该事件，该事件处理完后又返回被中断的程序<br>继续执行，这一过程称为中断。<br>（2）引入中断的原因<br>提高对外设请求的响应实时性。<br>提高了CPU的利用率<br>避免了CPU不断检测外设状态的过程<br>（3）中断类型<br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200627145857.png"></li></ol><h1 id="第十章-可编程数字接口电路"><a href="#第十章-可编程数字接口电路" class="headerlink" title="第十章 可编程数字接口电路"></a>第十章 可编程数字接口电路</h1><h2 id="可编程定时计数器8253"><a href="#可编程定时计数器8253" class="headerlink" title="可编程定时计数器8253"></a>可编程定时计数器8253</h2><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/8253-1.PNG"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/8253-2.PNG"></p><h2 id="可编程并行接口8255"><a href="#可编程并行接口8255" class="headerlink" title="可编程并行接口8255"></a>可编程并行接口8255</h2><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/8255-1.PNG"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/8255-2.PNG"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/8255-3.PNG"></p><h2 id="可编程中断控制器8259"><a href="#可编程中断控制器8259" class="headerlink" title="可编程中断控制器8259"></a>可编程中断控制器8259</h2><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/8259-1.PNG"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/8259-2.PNG"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/8259-3.PNG"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/8259-4.PNG"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/8259-5.PNG"></p><h1 id="第十一章-模拟接口电路"><a href="#第十一章-模拟接口电路" class="headerlink" title="第十一章 模拟接口电路"></a>第十一章 模拟接口电路</h1><h2 id="模拟量的输入输出"><a href="#模拟量的输入输出" class="headerlink" title="模拟量的输入输出"></a>模拟量的输入输出</h2><p>D： （Digital） 数字量<br>A： （Analog） 模拟量<br>采样和量化</p><h2 id="D-A转换器和A-D转换器"><a href="#D-A转换器和A-D转换器" class="headerlink" title="D/A转换器和A/D转换器"></a>D/A转换器和A/D转换器</h2><ol><li>主要参数<br>（1）分辨率<br>输入的二进制数每+1/-1个最低有效位LSB，使输出变化的程度 , 1LSB = 1/(2^n-1)<br>[n：D/A转换器的字长]<br>（2）转换时间<br>（3）精度<br>（4）线性度</li><li>D/A转换器与微处理器的接口方法<br>（1）接口任务:解决数据锁存，缓冲问题<br>（2）特点：控制信号，无专门数据传送间隔时间，调节数据宽度<br>（3）接口电路结构：通用并行接口或直连<br>（4）D/A：数字量转换为模拟量</li></ol><h2 id="D-A转换器（DAC0832）NS"><a href="#D-A转换器（DAC0832）NS" class="headerlink" title="D/A转换器（DAC0832）NS"></a>D/A转换器（DAC0832）NS</h2><ol><li>三种工作方式：直通方式，单缓冲方式，双缓冲方式</li><li>8位寄存器，T型电阻网络，电流型输出，不可编程</li><li>主要引脚功能：D7-D0，ILE，CS，WR1，WR2，XFER（低电平有效）</li><li>内部结构：<br>（1）8位输入寄存器<br>（2）8位DAC寄存器<br>（3）8位D/A转换器<br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200907141224.png"></li></ol><h2 id="A-D转换器（ADC0809）"><a href="#A-D转换器（ADC0809）" class="headerlink" title="A/D转换器（ADC0809）"></a>A/D转换器（ADC0809）</h2><p>1.特点</p><ul><li>8通道（8路）输入</li><li>8位字长</li><li>逐位逼近型</li><li>转换时间100us</li><li>内置三态输出缓冲器</li></ul><p>2.主要引脚功能</p><ul><li>D7-D0：输出数据线，三态</li><li>IN0-IN7：8通道模拟输入</li><li>ADDC,ADDB,ADDA通道地址选择</li><li>Start：启动变换</li><li>ALE：通道地址锁存</li><li>EOC：转换结束状态输出</li><li>OE：输出允许</li><li>CLK：工作时钟</li></ul><h1 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h1><ol><li>控制信号<br>（1）M/IO=1,CPU对存储器操作，M/IO=0,CPU对I/O操作<br>（2）DT/R=1,CPU→【内存/（I/O）】，DT/R=0，外部→CPU<br>（3）8086，RD，WR低电平有效</li><li>ADC0809<br>（1）EOC发出中断请求<br>（2）CPU查询EOC状态</li><li>D/A和A/D<br>（1）主要参数：分辨率，转换时间，精度</li><li>数据传输方式（输入输出控制方式）<br>（1）程序控制方式<br> (1.1)无条件传送<br> (1.2)有条件传送（查询）<br>（2）中断控制方式<br>（3）DMA<br> （3.1）DMA传送方式：单元传送，块传送，请求传送<br> （3.2）DMAC，8237A<br> （3.3）DMA方式写，外设到存储器<br> （3.4）8237占用8个输入输出端口<br> （3.5）DMA控制方式中需要用到的一对联络信号是HLDA/HRQ</li><li>串行接口<br>（1）串行接口中，并行数据和串行数据的转换通过移位寄存器实现<br>（2）RS-232是串行通信标准</li><li>片选控制方式，全译码，部分译码，线译码</li><li>I/O接口有独立编址和统一编址方式</li><li>复位后段寄存器的初值为：CS=FFFFH，DS=0000H，SS=0000H，ES=0000H,其他寄存器的初值都是0，特别是CS=FFFFH，IP=0000H，因此复位后CPU从FFFF0H开始执行程序</li><li>奇地址存储体和系统数据总线高8位相连，用BHE=0作为选通信号；偶地址存储体和系统数据总线低8位相连，用A0=0作为连通信号</li><li>对准字，从偶地址开始存放字数据的存放方式（传一次，A0和BHE都有效），非对准字，从奇地址开始存放字数据的存放方式（传两次，先奇BHE后偶A0）</li><li>寻址隐含约定：<br>（1）直接寻址，DS<br>（2）寄存器寻址：DS←BX/SI/DI；SS←BP<br>（3）基址变址寻址：DS←BX+SI/DI ； SS：BP+SI/DI<br>（4）堆栈：SS←SP<br>（5）取指令：CS←IP</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;大学课程《微机原理与接口技术》学习笔记整理&lt;/p&gt;</summary>
    
    
    
    <category term="学习笔记" scheme="https://blog.justlovesmile.top/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="学习笔记" scheme="https://blog.justlovesmile.top/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    <category term="大学课程" scheme="https://blog.justlovesmile.top/tags/%E5%A4%A7%E5%AD%A6%E8%AF%BE%E7%A8%8B/"/>
    
    <category term="微机原理与接口技术" scheme="https://blog.justlovesmile.top/tags/%E5%BE%AE%E6%9C%BA%E5%8E%9F%E7%90%86%E4%B8%8E%E6%8E%A5%E5%8F%A3%E6%8A%80%E6%9C%AF/"/>
    
  </entry>
  
  <entry>
    <title>学习笔记 | CSP201912认证考试部分题解</title>
    <link href="https://blog.justlovesmile.top/posts/7c5e3f37.html"/>
    <id>https://blog.justlovesmile.top/posts/7c5e3f37.html</id>
    <published>2020-07-30T07:36:38.000Z</published>
    <updated>2020-07-30T07:36:38.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CSP练习"><a href="#CSP练习" class="headerlink" title="CSP练习"></a>CSP练习</h1><h2 id="201912-1-报数"><a href="#201912-1-报数" class="headerlink" title="201912-1 报数"></a>201912-1 报数</h2><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/csp201912101.png"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/csp201912102.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">n=<span class="built_in">int</span>(<span class="built_in">input</span>())</span><br><span class="line">a=[<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>)]</span><br><span class="line">i,j=<span class="number">1</span>,<span class="number">1</span></span><br><span class="line"><span class="keyword">while</span>(j&lt;=n):</span><br><span class="line">    <span class="keyword">if</span>(i%<span class="number">7</span>==<span class="number">0</span> <span class="keyword">or</span> (<span class="string">&#x27;7&#x27;</span> <span class="keyword">in</span> <span class="built_in">str</span>(i))):<span class="comment">#7的倍数或者含有7则跳过</span></span><br><span class="line">        a[(i-<span class="number">1</span>)%<span class="number">4</span>]+=<span class="number">1</span><span class="comment">#跳过次数加1</span></span><br><span class="line">        j-=<span class="number">1</span><span class="comment">#不计被跳过的数</span></span><br><span class="line">    i+=<span class="number">1</span><span class="comment">#下一个人</span></span><br><span class="line">    j+=<span class="number">1</span><span class="comment">#下一个数</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> a:</span><br><span class="line">    <span class="built_in">print</span>(i)</span><br></pre></td></tr></table></figure><pre><code>6675115</code></pre><h2 id="201912-2-回收站选址"><a href="#201912-2-回收站选址" class="headerlink" title="201912-2 回收站选址"></a>201912-2 回收站选址</h2><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/csp201912201.png"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/csp201912202.png"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/csp201912203.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Point</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,x,y,w=<span class="number">0</span>,a=<span class="number">0</span>,s=<span class="number">0</span>,d=<span class="number">0</span>,score=<span class="number">0</span></span>):</span></span><br><span class="line">        self.x,self.y,self.w,self.a,self.s,self.d,self.score=x,y,w,a,s,d,score</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">newscore</span>(<span class="params">self,p</span>):</span><span class="comment">#四对角得分</span></span><br><span class="line">        <span class="keyword">if</span>(self.x+<span class="number">1</span>==p.x <span class="keyword">and</span> self.y+<span class="number">1</span>==p.y):</span><br><span class="line">            self.score+=<span class="number">1</span></span><br><span class="line">            p.score+=<span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span>(self.x+<span class="number">1</span>==p.x <span class="keyword">and</span> self.y-<span class="number">1</span>==p.y):</span><br><span class="line">            self.score+=<span class="number">1</span></span><br><span class="line">            p.score+=<span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span>(self.x-<span class="number">1</span>==p.x <span class="keyword">and</span> self.y+<span class="number">1</span>==p.y):</span><br><span class="line">            self.score+=<span class="number">1</span></span><br><span class="line">            p.score+=<span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span>(self.x-<span class="number">1</span>==p.x <span class="keyword">and</span> self.y-<span class="number">1</span>==p.y):</span><br><span class="line">            self.score+=<span class="number">1</span></span><br><span class="line">            p.score+=<span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">begood</span>(<span class="params">self,p</span>):</span><span class="comment">#上下左右</span></span><br><span class="line">        <span class="keyword">if</span>(self.x+<span class="number">1</span>==p.x <span class="keyword">and</span> self.y==p.y):</span><br><span class="line">            self.d+=<span class="number">1</span></span><br><span class="line">            p.a+=<span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span>(self.x-<span class="number">1</span>==p.x <span class="keyword">and</span> self.y==p.y):</span><br><span class="line">            self.a+=<span class="number">1</span></span><br><span class="line">            p.d+=<span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span>(self.x==p.x <span class="keyword">and</span> self.y+<span class="number">1</span>==p.y):</span><br><span class="line">            self.w+=<span class="number">1</span></span><br><span class="line">            p.s+=<span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span>(self.x==p.x <span class="keyword">and</span> self.y-<span class="number">1</span>==p.y):</span><br><span class="line">            self.s+=<span class="number">1</span></span><br><span class="line">            p.w+=<span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">good</span>(<span class="params">self</span>):</span><span class="comment">#是否上下左右满足</span></span><br><span class="line">        <span class="keyword">if</span>(self.w==<span class="number">1</span> <span class="keyword">and</span> self.a==<span class="number">1</span> <span class="keyword">and</span> self.s==<span class="number">1</span> <span class="keyword">and</span> self.d==<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">s=<span class="built_in">int</span>(<span class="built_in">input</span>())</span><br><span class="line">p=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(s):</span><br><span class="line">    s=<span class="built_in">input</span>().split()</span><br><span class="line">    tempP=Point(<span class="built_in">int</span>(s[<span class="number">0</span>]),<span class="built_in">int</span>(s[<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">for</span> eachp <span class="keyword">in</span> p:</span><br><span class="line">        dx=eachp.x-tempP.x</span><br><span class="line">        dy=eachp.y-tempP.y</span><br><span class="line">        <span class="keyword">if</span>(<span class="keyword">not</span> eachp.good() <span class="keyword">and</span> dx*dx+dy*dy==<span class="number">1</span>):</span><br><span class="line">            eachp.begood(tempP)</span><br><span class="line">        <span class="keyword">if</span>(eachp.score!=<span class="number">4</span> <span class="keyword">and</span> dx*dx+dy*dy==<span class="number">2</span>):</span><br><span class="line">            eachp.newscore(tempP)</span><br><span class="line"></span><br><span class="line">    p.append(tempP)</span><br><span class="line"></span><br><span class="line">ans=[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> p:</span><br><span class="line">    <span class="keyword">if</span>(i.good()):</span><br><span class="line">        ans[i.score]+=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> ans:</span><br><span class="line">    <span class="built_in">print</span>(i)</span><br></pre></td></tr></table></figure><pre><code>71 22 10 01 11 02 00 100100</code></pre><h2 id="201912-3-化学方程式"><a href="#201912-3-化学方程式" class="headerlink" title="201912-3 化学方程式"></a>201912-3 化学方程式</h2><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/csp201912301.png"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/csp201912302.png"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/csp201912303.png"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">//样例：</span><br><span class="line">11</span><br><span class="line">H2+O2=H2O</span><br><span class="line">2H2+O2=2H2O</span><br><span class="line">H2+Cl2=2NaCl</span><br><span class="line">H2+Cl2=2HCl</span><br><span class="line">CH4+2O2=CO2+2H2O</span><br><span class="line">CaCl2+2AgNO3=Ca(NO3)2+2AgCl</span><br><span class="line">3Ba(OH)2+2H3PO4=6H2O+Ba3(PO4)2</span><br><span class="line">3Ba(OH)2+2H3PO4=Ba3(PO4)2+6H2O</span><br><span class="line">4Zn+10HNO3=4Zn(NO3)2+NH4NO3+3H2O</span><br><span class="line">4Au+8NaCN+2H2O+O2=4Na(Au(CN)2)+4NaOH</span><br><span class="line">Cu+As=Cs+Au</span><br><span class="line">//结果：</span><br><span class="line">N</span><br><span class="line">Y</span><br><span class="line">N</span><br><span class="line">Y</span><br><span class="line">Y</span><br><span class="line">Y</span><br><span class="line">Y</span><br><span class="line">Y</span><br><span class="line">Y</span><br><span class="line">Y</span><br><span class="line">N</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chemistry</span>(<span class="params">x</span>):</span></span><br><span class="line">    each=x.split(<span class="string">&#x27;+&#x27;</span>) <span class="comment">#拆成每个化学式</span></span><br><span class="line">    res=&#123;&#125;<span class="comment">#化学元素字典</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> each:</span><br><span class="line">        i=<span class="number">0</span></span><br><span class="line">        num=<span class="string">&quot;&quot;</span><span class="comment">#存储每个化学式前面的数字</span></span><br><span class="line">        <span class="keyword">if</span>(<span class="number">48</span>&lt;=<span class="built_in">ord</span>(item[i])&lt;=<span class="number">57</span>):<span class="comment">#如果开始是数字</span></span><br><span class="line">            num+=item[i]</span><br><span class="line">            i+=<span class="number">1</span></span><br><span class="line">            <span class="keyword">while</span>(i&lt;<span class="built_in">len</span>(item)-<span class="number">1</span> <span class="keyword">and</span> <span class="number">48</span>&lt;=<span class="built_in">ord</span>(item[i])&lt;=<span class="number">57</span>):</span><br><span class="line">                num+=item[i]</span><br><span class="line">                i+=<span class="number">1</span></span><br><span class="line">        n=<span class="number">1</span> <span class="keyword">if</span> num==<span class="string">&quot;&quot;</span> <span class="keyword">else</span> <span class="built_in">int</span>(num)</span><br><span class="line">        <span class="comment">#化学式前面没有数字，代表为1</span></span><br><span class="line">        re=single(item[i:],n)</span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> re:</span><br><span class="line">            res[key]=res.get(key,<span class="number">0</span>)+re[key]</span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">single</span>(<span class="params">item,m</span>):</span></span><br><span class="line">    res=&#123;&#125;<span class="comment">#化学元素字典</span></span><br><span class="line">    i=<span class="number">0</span></span><br><span class="line">    s=<span class="string">&quot;&quot;</span></span><br><span class="line">    num=<span class="string">&quot;&quot;</span></span><br><span class="line">    <span class="keyword">while</span>(i&lt;<span class="built_in">len</span>(item)):</span><br><span class="line">        <span class="keyword">if</span>(item[i]==<span class="string">&quot;(&quot;</span>):</span><br><span class="line">            p=<span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i+<span class="number">1</span>,<span class="built_in">len</span>(item)):</span><br><span class="line">                <span class="keyword">if</span>(item[j]==<span class="string">&quot;(&quot;</span>):</span><br><span class="line">                    p+=<span class="number">1</span></span><br><span class="line">                <span class="keyword">elif</span> (item[j]==<span class="string">&quot;)&quot;</span>):</span><br><span class="line">                    p-=<span class="number">1</span></span><br><span class="line">                    index=j</span><br><span class="line">                <span class="keyword">if</span>(p==<span class="number">0</span>):</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            k=i+<span class="number">1</span><span class="comment">#第一个左括号之后的位置</span></span><br><span class="line">            i=index<span class="comment">#括号结束的位置</span></span><br><span class="line">            <span class="keyword">if</span>(i&lt;<span class="built_in">len</span>(item)-<span class="number">1</span> <span class="keyword">and</span> <span class="number">48</span>&lt;=<span class="built_in">ord</span>(item[i+<span class="number">1</span>])&lt;=<span class="number">57</span>):<span class="comment">#如果是数字</span></span><br><span class="line">                num+=item[i+<span class="number">1</span>]</span><br><span class="line">                i+=<span class="number">1</span></span><br><span class="line">                <span class="keyword">while</span>(i&lt;<span class="built_in">len</span>(item)-<span class="number">1</span> <span class="keyword">and</span> <span class="number">48</span>&lt;=<span class="built_in">ord</span>(item[i+<span class="number">1</span>])&lt;=<span class="number">57</span>):</span><br><span class="line">                    num+=item[i+<span class="number">1</span>]</span><br><span class="line">                    i+=<span class="number">1</span></span><br><span class="line">            n=<span class="number">1</span> <span class="keyword">if</span> num==<span class="string">&quot;&quot;</span> <span class="keyword">else</span> <span class="built_in">int</span>(num)</span><br><span class="line">            re=single(item[k:index],n*m)</span><br><span class="line">            <span class="keyword">for</span> key <span class="keyword">in</span> re:</span><br><span class="line">                res[key]=res.get(key,<span class="number">0</span>)+re[key]</span><br><span class="line">        <span class="keyword">elif</span>(<span class="number">65</span>&lt;=<span class="built_in">ord</span>(item[i])&lt;=<span class="number">90</span>):<span class="comment">#如果是大写字母</span></span><br><span class="line">            s+=item[i]</span><br><span class="line">            <span class="keyword">if</span>(i&lt;<span class="built_in">len</span>(item)-<span class="number">1</span> <span class="keyword">and</span> <span class="number">97</span>&lt;=<span class="built_in">ord</span>(item[i+<span class="number">1</span>])&lt;=<span class="number">122</span>):<span class="comment">#如果是小写字母</span></span><br><span class="line">                s+=item[i+<span class="number">1</span>]</span><br><span class="line">                i+=<span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span>(i&lt;<span class="built_in">len</span>(item)-<span class="number">1</span> <span class="keyword">and</span> <span class="number">48</span>&lt;=<span class="built_in">ord</span>(item[i+<span class="number">1</span>])&lt;=<span class="number">57</span>):<span class="comment">#如果是数字</span></span><br><span class="line">                num+=item[i+<span class="number">1</span>]</span><br><span class="line">                i+=<span class="number">1</span></span><br><span class="line">                <span class="keyword">while</span>(i&lt;<span class="built_in">len</span>(item)-<span class="number">1</span> <span class="keyword">and</span> <span class="number">48</span>&lt;=<span class="built_in">ord</span>(item[i+<span class="number">1</span>])&lt;=<span class="number">57</span>):</span><br><span class="line">                    num+=item[i+<span class="number">1</span>]</span><br><span class="line">                    i+=<span class="number">1</span></span><br><span class="line">            n=<span class="number">1</span> <span class="keyword">if</span> num==<span class="string">&quot;&quot;</span> <span class="keyword">else</span> <span class="built_in">int</span>(num)</span><br><span class="line">            res[s]=res.get(s,<span class="number">0</span>)+m*n<span class="comment">#这个化学元素个数，初始为0</span></span><br><span class="line">        num=<span class="string">&quot;&quot;</span></span><br><span class="line">        s=<span class="string">&quot;&quot;</span></span><br><span class="line">        i+=<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    n=<span class="built_in">int</span>(<span class="built_in">input</span>())</span><br><span class="line">    expressions=[]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        expressions.append(<span class="built_in">input</span>())</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> expressions:</span><br><span class="line">        express=item.split(<span class="string">&quot;=&quot;</span>)<span class="comment">#每个表达式从等号分开</span></span><br><span class="line">        left,right=express[<span class="number">0</span>],express[<span class="number">1</span>]</span><br><span class="line">        ans1=chemistry(left)</span><br><span class="line">        ans2=chemistry(right)</span><br><span class="line">        <span class="keyword">if</span>(ans1==ans2):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Y&#x27;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;N&#x27;</span>)</span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure><pre><code>11H2+O2=H2O2H2+O2=2H2OH2+Cl2=2NaClH2+Cl2=2HClCH4+2O2=CO2+2H2OCaCl2+2AgNO3=Ca(NO3)2+2AgCl3Ba(OH)2+2H3PO4=6H2O+Ba3(PO4)23Ba(OH)2+2H3PO4=Ba3(PO4)2+6H2O4Zn+10HNO3=4Zn(NO3)2+NH4NO3+3H2O4Au+8NaCN+2H2O+O2=4Na(Au(CN)2)+4NaOHCu+As=Cs+AuNYNYYYYYYYN</code></pre><h2 id="201912-4-区块链"><a href="#201912-4-区块链" class="headerlink" title="201912-4 区块链"></a>201912-4 区块链</h2><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/csp201912401.png"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/csp201912402.png"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/csp201912403.png"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/csp201912404.png"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/csp201912405.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure><h2 id="201912-5-魔数"><a href="#201912-5-魔数" class="headerlink" title="201912-5 魔数"></a>201912-5 魔数</h2><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/201912501.png"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/201912502.png"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/201912503.png"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/201912504.png"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/201912505.png"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/201912506.png"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/201912507.png"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/201912508.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">CSP2019年12月题组，python解法，目前只研究了前三题,也不知道对错...先记录一下，有时间再研究...</summary>
    
    
    
    <category term="学习笔记" scheme="https://blog.justlovesmile.top/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="CSP" scheme="https://blog.justlovesmile.top/tags/CSP/"/>
    
    <category term="python" scheme="https://blog.justlovesmile.top/tags/python/"/>
    
    <category term="算法" scheme="https://blog.justlovesmile.top/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>随笔记录 | 漫威DC观影指南</title>
    <link href="https://blog.justlovesmile.top/posts/604a50ec.html"/>
    <id>https://blog.justlovesmile.top/posts/604a50ec.html</id>
    <published>2020-07-22T12:17:25.000Z</published>
    <updated>2020-07-22T12:17:25.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Marvel"><a href="#Marvel" class="headerlink" title="Marvel"></a>Marvel</h2><h3 id="MCU-漫威电影宇宙"><a href="#MCU-漫威电影宇宙" class="headerlink" title="MCU(漫威电影宇宙)"></a>MCU(漫威电影宇宙)</h3><p>十年漫威，一战终局，英雄已去，传奇依旧</p><p><big>对</big>于MCU（漫威电影宇宙）的<strong>电影</strong>观看顺序，上映顺序就是最好的指南</p><table><thead><tr><th>阶段</th><th>观影顺序（↓）</th><th>上映时间</th></tr></thead><tbody><tr><td>《无限传奇》第一阶段</td><td>钢铁侠</td><td>2008</td></tr><tr><td></td><td>无敌浩克</td><td>2008</td></tr><tr><td></td><td>钢铁侠2</td><td>2010</td></tr><tr><td></td><td>雷神</td><td>2011</td></tr><tr><td></td><td>美国队长：复仇者先锋</td><td>2011</td></tr><tr><td></td><td>复仇者联盟</td><td>2012</td></tr><tr><td>《无限传奇》第二阶段</td><td>钢铁侠3</td><td>2013</td></tr><tr><td></td><td>雷神2：黑暗世界</td><td>2013</td></tr><tr><td></td><td>美国队长2：冬日战士</td><td>2014</td></tr><tr><td></td><td>银河护卫队</td><td>2014</td></tr><tr><td></td><td>复仇者联盟2：奥创纪元</td><td>2015</td></tr><tr><td></td><td>蚁人</td><td>2015</td></tr><tr><td>《无限传奇》第三阶段</td><td>美国队长3：内战</td><td>2016</td></tr><tr><td></td><td>奇异博士</td><td>2016</td></tr><tr><td></td><td>银河护卫队2</td><td>2017</td></tr><tr><td></td><td>蜘蛛侠：英雄归来</td><td>2017</td></tr><tr><td></td><td>雷神3：诸神黄昏</td><td>2017</td></tr><tr><td></td><td>黑豹</td><td>2018</td></tr><tr><td></td><td>复仇者联盟3：无限战争</td><td>2018</td></tr><tr><td></td><td>蚁人2：黄蜂女现身</td><td>2018</td></tr><tr><td></td><td>惊奇队长</td><td>2019</td></tr><tr><td></td><td>复仇者联盟4：终局之战</td><td>2019</td></tr><tr><td></td><td>蜘蛛侠2：英雄远征（2019）</td><td>2019</td></tr></tbody></table><ul><li>下面是完整的电影，电视剧以及短片的观影顺序图：</li></ul><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/d01373f082025aafc11ca80500690662024f1a59.jpg"></p><h3 id="X战警宇宙"><a href="#X战警宇宙" class="headerlink" title="X战警宇宙"></a>X战警宇宙</h3><table><thead><tr><th>系列名称</th><th>观影顺序（↓）</th><th>上映时间</th></tr></thead><tbody><tr><td>X战警系列作品</td><td>《金刚狼》X-Men Origins:Wolverine</td><td>2009</td></tr><tr><td></td><td>《X战警：第一战》X-Men:First Class</td><td>2011</td></tr><tr><td></td><td>《X战警》X-Men</td><td>2000</td></tr><tr><td></td><td>《X战警2》X2</td><td>2003</td></tr><tr><td></td><td>《X战警：背水一战》X-Men:The Last Stand</td><td>2006</td></tr><tr><td></td><td>《金刚狼2》The Wolverine</td><td>2013</td></tr><tr><td></td><td>《X战警：逆转未来》X-Men:Days of Future Past</td><td>2014</td></tr><tr><td></td><td>《死侍》Dead pool</td><td>2016</td></tr><tr><td></td><td>《X战警：天启》X-Men:Apocalypse</td><td>2016</td></tr><tr><td></td><td>《金刚狼3：殊死一战》Logan</td><td>2017</td></tr><tr><td></td><td>《死侍2：我爱我家》Deadpool2</td><td>2018</td></tr><tr><td></td><td>《X战警：黑凤凰》Dark Phoenix</td><td>2019</td></tr><tr><td></td><td>《新变种人》The New Mutants</td><td>2020</td></tr></tbody></table><h3 id="索尼电影宇宙"><a href="#索尼电影宇宙" class="headerlink" title="索尼电影宇宙"></a>索尼电影宇宙</h3><table><thead><tr><th>系列名称</th><th>观影顺序（↓）</th><th>上映时间</th></tr></thead><tbody><tr><td>索尼漫威宇宙</td><td>《蜘蛛侠》</td><td>2002</td></tr><tr><td></td><td>《蜘蛛侠2》</td><td>2004</td></tr><tr><td></td><td>《蜘蛛侠3》</td><td>2007</td></tr><tr><td></td><td>《超凡蜘蛛侠》</td><td>2012</td></tr><tr><td></td><td>《超凡蜘蛛侠2》</td><td>2014</td></tr><tr><td></td><td>《毒液：致命守护者》</td><td>2018</td></tr><tr><td></td><td>《蜘蛛侠：平行宇宙》（动画电影）</td><td>2018</td></tr></tbody></table><h3 id="其他漫威电影"><a href="#其他漫威电影" class="headerlink" title="其他漫威电影"></a>其他漫威电影</h3><table><thead><tr><th>电影名</th><th>上映时间</th></tr></thead><tbody><tr><td>《新神奇四侠》</td><td>2015</td></tr><tr><td>《超能陆战队》（动画电影）</td><td>2014</td></tr><tr><td>《恶灵骑士2：复仇时刻》</td><td>2011</td></tr><tr><td>《惩罚者2》</td><td>2008</td></tr><tr><td>《神奇四侠2：银影侠来袭》</td><td>2007</td></tr><tr><td>《恶灵骑士》</td><td>2007</td></tr><tr><td>《神奇四侠》</td><td>2005</td></tr><tr><td>《类人体》</td><td>2005</td></tr><tr><td>《艾丽卡》</td><td>2005</td></tr><tr><td>《惩罚者》</td><td>2004</td></tr><tr><td>《刀锋战士3》</td><td>2004</td></tr><tr><td>《夜魔侠》</td><td>2003</td></tr><tr><td>《浩克》</td><td>2003</td></tr><tr><td>《刀锋战士2》</td><td>2002</td></tr><tr><td>《刀锋战士》</td><td>1998</td></tr><tr><td>《神奇四侠》</td><td>1994</td></tr><tr><td>《惩罚者》</td><td>1989</td></tr><tr><td>《霍华德怪鸭》</td><td>1986</td></tr></tbody></table><h2 id="DC"><a href="#DC" class="headerlink" title="DC"></a>DC</h2><h3 id="DCEU（DC扩展宇宙）"><a href="#DCEU（DC扩展宇宙）" class="headerlink" title="DCEU（DC扩展宇宙）"></a>DCEU（DC扩展宇宙）</h3><table><thead><tr><th>电影名</th><th>上映时间</th></tr></thead><tbody><tr><td>《超人：钢铁之躯》</td><td>2013</td></tr><tr><td>《DC电影出品：正义联盟黎明》</td><td>2016</td></tr><tr><td>《蝙蝠侠大战超人：正义黎明》</td><td>2016</td></tr><tr><td>《自杀小队》</td><td>2016</td></tr><tr><td>《神奇女侠》</td><td>2017</td></tr><tr><td>《正义联盟》</td><td>2017</td></tr><tr><td>《海王》</td><td>2018</td></tr><tr><td>《雷霆沙赞》</td><td>2019</td></tr></tbody></table><h3 id="其他DC电影"><a href="#其他DC电影" class="headerlink" title="其他DC电影"></a>其他DC电影</h3><ul><li>太过久远的电影就不罗列了</li></ul><table><thead><tr><th>电影名</th><th>上映时间</th></tr></thead><tbody><tr><td>《猫女》</td><td>2004</td></tr><tr><td>《康斯坦丁》</td><td>2005</td></tr><tr><td>《V字仇杀队》</td><td>2005</td></tr><tr><td>《蝙蝠侠：侠影之谜》</td><td>2005</td></tr><tr><td>《超人归来》</td><td>2006</td></tr><tr><td>《黑暗骑士》</td><td>2008</td></tr><tr><td>《守望者》</td><td>2009</td></tr><tr><td>《失败者》</td><td>2010</td></tr><tr><td>《西部英雄约拿·哈克斯》</td><td>2010</td></tr><tr><td>《绿灯侠》</td><td>2011</td></tr><tr><td>《黑暗骑士崛起》</td><td>2012</td></tr><tr><td>《乐高蝙蝠侠大电影》</td><td>2017</td></tr><tr><td>《小丑》</td><td>2019</td></tr></tbody></table><h2 id="壁纸"><a href="#壁纸" class="headerlink" title="壁纸"></a>壁纸</h2><div class="fj-gallery"><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/444211.jpg"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/344e370b0a0d5fb463724cc556458d22.jpg"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/4c1svpbxcfh.jpg"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/4fcd5y13bws.jpg"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/bi3wrjelcd4.jpg"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/db2984b3a973e7e8cdc6834de4f69704.jpg"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/e7530dec43be8a8c7718752306bbbb54.jpg"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/ijelpz2bnxp.jpg"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/op3pktbhatl.jpg"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/qhoo4nupjp5.jpg"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/zfx1it4hq3i.jpg"><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/zjykzjztrjg.jpg"></p>          </div>]]></content>
    
    
    <summary type="html">对于漫威和DC的电影的观影顺序介绍，十年漫威，一战终局，英雄已去，传奇依旧...</summary>
    
    
    
    <category term="随笔记录" scheme="https://blog.justlovesmile.top/categories/%E9%9A%8F%E7%AC%94%E8%AE%B0%E5%BD%95/"/>
    
    
    <category term="漫威" scheme="https://blog.justlovesmile.top/tags/%E6%BC%AB%E5%A8%81/"/>
    
    <category term="DC" scheme="https://blog.justlovesmile.top/tags/DC/"/>
    
    <category term="电影" scheme="https://blog.justlovesmile.top/tags/%E7%94%B5%E5%BD%B1/"/>
    
  </entry>
  
  <entry>
    <title>必看 | Hexo博客搭建超级指南</title>
    <link href="https://blog.justlovesmile.top/posts/c8972b63.html"/>
    <id>https://blog.justlovesmile.top/posts/c8972b63.html</id>
    <published>2020-07-15T12:04:21.000Z</published>
    <updated>2021-08-14T04:33:26.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h1><p>不知不觉，我的博客已经在风雨飘摇中运行超过一年时间了，回想这一年的博客维护以及魔改经历，我觉得有必要详细记录一下博客搭建的过程，以防我不小心搞崩了博客…</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200715162057.png"></p><h1 id="2-环境部署工作"><a href="#2-环境部署工作" class="headerlink" title="2. 环境部署工作"></a>2. 环境部署工作</h1><h2 id="2-1-安装Node-js"><a href="#2-1-安装Node-js" class="headerlink" title="2.1 安装Node.js"></a>2.1 安装Node.js</h2><p>1.进入官网选择对应的系统下载：<br>官网：<a href="https://nodejs.org/en/download/">https://nodejs.org/en/download/</a><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200715163331.png"><br>2.安装<br>选好路径，完成安装<br>3.检查<br>打开<code>cmd</code>或者<code>powershell</code>,输入:</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">node</span> <span class="title">-v</span></span><br><span class="line">npm -v</span><br></pre></td></tr></table></figure><p>显示版本号，即安装无误</p><blockquote><p>npm为Node.js的包管理工具</p></blockquote><h2 id="2-2-安装Git"><a href="#2-2-安装Git" class="headerlink" title="2.2 安装Git"></a>2.2 安装Git</h2><p>1.进入官网下载<br>官网：<a href="https://git-scm.com/downloads">https://git-scm.com/downloads</a><br>2.安装<br>选好路径，完成安装<br>3.检查<br>打开git bash，输入：</p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="comment">--version</span></span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200715170132.png"></p><h2 id="2-3-注册Github账号"><a href="#2-3-注册Github账号" class="headerlink" title="2.3 注册Github账号"></a>2.3 注册Github账号</h2><p>1.Github官网<a href="https://github.com/">https://github.com</a>,注册账号<br>2.新建项目</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200715171105.png"></p><p>项目名字为<code>你的昵称.github.io</code>，例如：</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//我的昵称是Justlovesmile</span></span><br><span class="line">所以我的项目名称为<span class="module-access"><span class="module"><span class="identifier">Justlovesmile</span>.</span></span>github.io</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200715170812.png"></p><p>3.代码库设置</p><p>创建好之后，保存<code>&lt;&gt;code</code>内的SSH，即：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git@github<span class="selector-class">.com</span>:XXXXXXXXX/XXXXXXXXX<span class="selector-class">.github</span><span class="selector-class">.io</span>.git</span><br></pre></td></tr></table></figure><p>点击右侧的<code>Settings</code></p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200715171759.png"></p><p>向下找到<code>Gihub pages</code>,点击<code>Launch automatic page generator</code>，<code>Github</code>将会自动替你创建出一个<code>pages</code>的页面。 如果配置没有问题，大约几分钟之后，<code>yourname.github.io</code>这个网址就可以正常访问了</p><p>5.推荐开启强制使用https</p><h2 id="2-4-安装Hexo"><a href="#2-4-安装Hexo" class="headerlink" title="2.4 安装Hexo"></a>2.4 安装Hexo</h2><p>1.在合适的位置，如<code>E:/hexo</code>，安装<code>hexo-cli</code>,输入：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd <span class="regexp">/e/</span>hexo/</span><br><span class="line">npm install hexo-cli -g</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200715172419.png"></p><p>再安装<code>hexo</code></p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo <span class="comment">--save</span></span><br></pre></td></tr></table></figure><p>安装完成后，检查</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">hexo -v</span></span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200715172614.png"></p><p>2.初始化一个文件夹： </p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd <span class="regexp">/e/</span>hexo/</span><br><span class="line">hexo init</span><br><span class="line">npm install</span><br></pre></td></tr></table></figure><p>3.生成Hexo页面：</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">hexo g</span></span><br></pre></td></tr></table></figure><p>4.启动服务：</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">hexo s</span></span><br></pre></td></tr></table></figure><p>默认是<code>localhost:4000</code>，打开浏览器输入即可</p><h2 id="2-5-推送到Github"><a href="#2-5-推送到Github" class="headerlink" title="2.5 推送到Github"></a>2.5 推送到Github</h2><p>1.配置个人信息</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git <span class="keyword">config</span> --<span class="keyword">global</span> user<span class="variable">.name</span> <span class="string">&quot;XXXX&quot;</span></span><br><span class="line">git <span class="keyword">config</span> --<span class="keyword">global</span> user<span class="variable">.email</span> <span class="string">&quot;XXXXXXXXX@XXX.com&quot;</span></span><br></pre></td></tr></table></figure><p>2.生成密钥</p><figure class="highlight excel"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -<span class="built_in">t</span> rsa -C <span class="string">&quot;XXXXXXXXX@XXX.com&quot;</span></span><br></pre></td></tr></table></figure><p>3.查看<code>id_rsa.pub</code>文件，并整个复制</p><figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat ~<span class="regexp">/.ssh/i</span>d_rsa.pub</span><br></pre></td></tr></table></figure><p>4.然后再在<code>Github</code>中添加<code>ssh key</code></p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200715173551.png"></p><p>5.修改hexo根目录下的文件<code>_config.yml</code>中的deploy，添加之前保存的ssh：</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">deploy:</span></span><br><span class="line"><span class="symbol">  type:</span> git</span><br><span class="line"><span class="symbol">  repository:</span> </span><br><span class="line"><span class="symbol">github:</span> git@github.com:Justlovesmile/Justlovesmile.github.io.git</span><br><span class="line"><span class="symbol">  branch:</span> master</span><br></pre></td></tr></table></figure><p>6.上传到github：</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">hexo d -g</span></span><br></pre></td></tr></table></figure><p>如果没有hexo-deployer-git，安装</p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-deployer-git <span class="comment">--save</span></span><br></pre></td></tr></table></figure><p>7.查看blog,<code>https://username.github.io</code></p><h1 id="3-Hexo基础"><a href="#3-Hexo基础" class="headerlink" title="3. Hexo基础"></a>3. Hexo基础</h1><h2 id="3-1-写博客"><a href="#3-1-写博客" class="headerlink" title="3.1 写博客"></a>3.1 写博客</h2><p>1.新建文章</p><figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo <span class="keyword">new</span> <span class="type">post</span> <span class="string">&#x27;我的第一篇文章&#x27;</span></span><br></pre></td></tr></table></figure><p>2.hexo自动生成一个md文件，修改md内容<br>头部如：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">title:</span> <span class="string">postName</span> <span class="comment">#文章页面上的显示名称</span></span><br><span class="line"><span class="attr">date:</span> <span class="number">2020-03-10 12:30:16</span> <span class="comment">#文章生成时间，一般不改，当然也可以任意修改</span></span><br><span class="line"><span class="attr">categories:</span> <span class="string">默认分类</span> <span class="comment">#分类</span></span><br><span class="line"><span class="attr">tags:</span> [<span class="string">tag1</span>,<span class="string">tag2</span>,<span class="string">tag3</span>] <span class="comment">#文章标签，可空，多标签请用格式，注意冒号:后面有个空格</span></span><br><span class="line"><span class="attr">description:</span> <span class="string">摘要</span></span><br><span class="line"><span class="meta">---</span></span><br></pre></td></tr></table></figure><p>3.在头部下面即可写文章内容</p><blockquote><p>markdown，支持html和其自带的语法。Markdown是一种可以使用普通文本编辑器编写的标记语言，通过简单的标记语法，它可以使普通文本内容具有一定的格式。</p></blockquote><h2 id="3-2-新建页面"><a href="#3-2-新建页面" class="headerlink" title="3.2 新建页面"></a>3.2 新建页面</h2><figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo <span class="keyword">new</span> <span class="type">page</span> <span class="string">&quot;music&quot;</span></span><br></pre></td></tr></table></figure><p>会在source文件夹中生成music文件夹，其内的index.md为页面内容</p><h2 id="3-3-常用基本命令"><a href="#3-3-常用基本命令" class="headerlink" title="3.3 常用基本命令"></a>3.3 常用基本命令</h2><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">hexo <span class="built_in">new</span> <span class="string">&quot;文章&quot;</span></span><br><span class="line">hexo <span class="built_in">new</span> <span class="built_in">post</span> <span class="string">&quot;文章&quot;</span></span><br><span class="line">hexo <span class="built_in">new</span> page <span class="string">&quot;页面&quot;</span></span><br><span class="line"></span><br><span class="line">hexo clean <span class="comment">#清除缓存，每次重新部署时最好执行</span></span><br><span class="line">hexo g <span class="comment">#生成静态页面</span></span><br><span class="line">hexo s <span class="comment">#本地端口，默认4000运行</span></span><br><span class="line">hexo s -p <span class="number">5000</span> <span class="comment"># 端口5000</span></span><br><span class="line">hexo d <span class="comment">#部署</span></span><br><span class="line">hexo deploy <span class="comment">#部署</span></span><br></pre></td></tr></table></figure><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#为了方便，每次准备推送时,可以👇</span></span><br><span class="line">hexo cl <span class="meta">&amp;&amp; hexo g &amp;&amp; hexo d</span></span><br></pre></td></tr></table></figure><h1 id="4-Hexo进阶"><a href="#4-Hexo进阶" class="headerlink" title="4. Hexo进阶"></a>4. Hexo进阶</h1><h2 id="4-1-推荐编辑器"><a href="#4-1-推荐编辑器" class="headerlink" title="4.1 推荐编辑器"></a>4.1 推荐编辑器</h2><p>方便后续魔改内容</p><ol><li>VSCode <a href="https://code.visualstudio.com/">https://code.visualstudio.com/</a></li><li>Sublime Text <a href="http://www.sublimetext.com/">http://www.sublimetext.com/</a></li></ol><h2 id="4-2-更换主题"><a href="#4-2-更换主题" class="headerlink" title="4.2 更换主题"></a>4.2 更换主题</h2><p>1.因为自带的主题并不好看，所以可以更换主题，常见主题的很多，例如butterfly</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd <span class="regexp">/e/</span>hexo/</span><br><span class="line">git clone -b master https:<span class="regexp">//gi</span>thub.com<span class="regexp">/jerryc127/</span>hexo-theme-butterfly.git themes/butterfly</span><br></pre></td></tr></table></figure><p>2.修改hexo根目录下的<code>_config.yml</code>中的    <code>theme: landscape</code>改成<code>theme： butterfly</code> ,(注意冒号：后面有一个空格)</p><h2 id="4-3-注册Coding账号"><a href="#4-3-注册Coding账号" class="headerlink" title="4.3 注册Coding账号"></a>4.3 注册Coding账号</h2><p>1.由于国内访问github的速度较慢，因此可以通过双部署同时部署到Coding<a href="https://coding.net/">https://coding.net/</a>，同样注册账号，新建项目，项目名随意<br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200715175443.png"><br>2.创建好后，同样记住<code>SSH</code><br>3.修改hexo根目录下的文件<code>_config.yml</code>中的deploy，添加之前保存的ssh：<br>例如我的：</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repository: </span><br><span class="line">    github: git@github<span class="selector-class">.com</span>:Justlovesmile/Justlovesmile<span class="selector-class">.github</span><span class="selector-class">.io</span><span class="selector-class">.git</span></span><br><span class="line">    coding: git@e<span class="selector-class">.coding</span><span class="selector-class">.net</span>:justlovesmile/justlovesmile<span class="selector-class">.top</span><span class="selector-class">.git</span></span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure><p>4.在Coding中保存你的密钥，方法同Github</p><figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat ~<span class="regexp">/.ssh/i</span>d_rsa.pub</span><br></pre></td></tr></table></figure><p>5.下次<code>hexo d -g</code>部署后,开启<code>静态网站</code>，然后可以通过其提供的<code>//xxxxxxx.coding-pages.com</code>访问。（第一次记得点，<code>立即部署</code>）</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200715180119.png"></p><p>6.推荐开启强制使用https</p><h2 id="4-4-注册Gitee账号"><a href="#4-4-注册Gitee账号" class="headerlink" title="4.4 注册Gitee账号"></a>4.4 注册Gitee账号</h2><p>1.除了Coding外，网内访问速度较快的还有码云<a href="https://gitee.com/">https://gitee.com/</a>，同样注册账号，新建项目<code>yourname</code><br>2.创建好后，同样记住<code>SSH</code><br>3.修改hexo根目录下的文件<code>_config.yml</code>中的deploy，添加之前保存的ssh：<br>例如我的：</p><figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repository: </span><br><span class="line">    gitee: git@gitee.com:justlovesmile/justlovesmile.git</span><br><span class="line">    coding: git@e.coding.net:justlovesmile/justlovesmile.<span class="attribute">top.git</span></span><br><span class="line"><span class="attribute">    github</span>: git<span class="variable">@github</span>.<span class="attribute">com</span>:Justlovesmile/Justlovesmile.github.io.git</span><br><span class="line">  <span class="attribute">branch</span>: master</span><br></pre></td></tr></table></figure><p>4.在Gitee中保存你的密钥，方法同Github</p><figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat ~<span class="regexp">/.ssh/i</span>d_rsa.pub</span><br></pre></td></tr></table></figure><p>5.开启GiteePages服务，Gitee只能免费使用gitee.io域名,其他的域名要收费，并且免费版每次部署后，需要手动点击更新来更新网站内容<br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200715181243.png"><br>5.推荐开启强制使用https</p><h2 id="4-5-绑定域名"><a href="#4-5-绑定域名" class="headerlink" title="4.5 绑定域名"></a>4.5 绑定域名</h2><p>1.在阿里云<a href="https://wanwang.aliyun.com/">https://wanwang.aliyun.com/</a>购买自己喜欢的域名</p><p>2.在阿里云控制台找到<code>云解析DNS</code></p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200715185711.png"></p><p>找到自己购买的域名,点击<code>解析设置</code></p><p>添加记录</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200715185838.png"></p><p>3.如果多部署了，可以设置多条</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200715190408.png"></p><p>一条给github.io，一条给coding-pages.com等等</p><p>4.再返回到对应的部署页面，修改解析域名</p><ul><li>Github的在仓库的<code>Settings--Github_Pages--Custon_domain</code></li><li>Coding的<code>静态网站-设置-自定义域名</code></li></ul><h2 id="4-6-安装插件"><a href="#4-6-安装插件" class="headerlink" title="4.6 安装插件"></a>4.6 安装插件</h2><p>1.安装hexo插件</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">npm <span class="keyword">install </span>hexo-generator-sitemap --save</span><br><span class="line">npm <span class="keyword">install </span>hexo-generator-<span class="keyword">baidu-sitemap </span>--save</span><br><span class="line">npm <span class="keyword">install </span>hexo-generator-feed --save</span><br></pre></td></tr></table></figure><p>2.在hexo根目录下的文件<code>_config.yml</code>中添加</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">Plugins:</span> <span class="meta"># 在该区域添加两个插件名称</span></span><br><span class="line">  - hexo-generator-sitemap</span><br><span class="line">  - hexo-generator-baidu-sitemap</span><br><span class="line">  - hexo-generator-feed</span><br><span class="line"><span class="meta"># 自动生成sitemap</span></span><br><span class="line"><span class="symbol">sitemap:</span></span><br><span class="line"><span class="symbol">  path:</span> sitemap.xml</span><br><span class="line"><span class="symbol">baidusitemap:</span></span><br><span class="line"><span class="symbol">  path:</span> baidusitemap.xml</span><br></pre></td></tr></table></figure><p>3.更多插件见<a href="/posts/86111745.html">Hexo插件推荐</a></p><h2 id="4-7-添加robots-txt"><a href="#4-7-添加robots-txt" class="headerlink" title="4.7 添加robots.txt"></a>4.7 添加robots.txt</h2><p>1.在hexo根目录下的source文件夹中，创建一个名为robots.txt的文件<br>2.内容为</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">User-agent: * </span><br><span class="line"><span class="symbol">Allow:</span> /</span><br><span class="line"><span class="symbol">Allow:</span> <span class="meta-keyword">/categories/</span></span><br><span class="line"><span class="symbol">Allow:</span> <span class="meta-keyword">/tags/</span></span><br><span class="line"><span class="symbol">Allow:</span> <span class="meta-keyword">/archives/</span></span><br><span class="line"><span class="symbol"></span></span><br><span class="line"><span class="symbol">Disallow:</span> <span class="meta-keyword">/js/</span></span><br><span class="line"><span class="symbol">Disallow:</span> <span class="meta-keyword">/css/</span></span><br><span class="line"><span class="symbol">Disallow:</span> <span class="meta-keyword">/images/</span></span><br><span class="line"><span class="symbol">Disallow:</span> <span class="meta-keyword">/img/</span></span><br><span class="line"><span class="symbol">Disallow:</span> <span class="meta-keyword">/lib/</span></span><br><span class="line"><span class="symbol"></span></span><br><span class="line"><span class="symbol">Sitemap:</span> https:<span class="comment">//XXXXXXXXXXX.XXX/sitemap.xml</span></span><br><span class="line"><span class="symbol">Sitemap:</span> https:<span class="comment">//XXXXXXXXXXX.XXX/baidusitemap.xml</span></span><br></pre></td></tr></table></figure><h2 id="4-8-创建百度站长账号"><a href="#4-8-创建百度站长账号" class="headerlink" title="4.8 创建百度站长账号"></a>4.8 创建百度站长账号</h2><p>1.进入百度站长<a href="https://ziyuan.baidu.com/">https://ziyuan.baidu.com/</a>，注册账号，登录<br>2.点击用户中心-站点管理-添加网站</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/image.png"></p><p>3.验证<br>（1）若选择文件验证，则下载文件到根目录下的source文件夹中，并在文件内容最上面添加三行</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">layout:</span> <span class="literal">false</span></span><br><span class="line"><span class="meta">---</span></span><br></pre></td></tr></table></figure><p>（2）若选择HTML标签验证，需要找到主题下的head文件位置，添加其给的html代码，（一般在<code>/themes/xxxxxx/layout/**/head.pug(ejs)</code>内)</p><p>（3）若选择CNAME验证，操作和绑定域名操作一样，看其给的说明即可</p><p>4.推送你的网址，使之更快收录<br>点击左侧<code>资源提交-普通收录</code>可以选择三种方式提交网址</p><h2 id="4-9-创建百度-谷歌统计账号"><a href="#4-9-创建百度-谷歌统计账号" class="headerlink" title="4.9 创建百度/谷歌统计账号"></a>4.9 创建百度/谷歌统计账号</h2><p>1.现在绝大部分国内主题集成了百度统计<a href="https://tongji.baidu.com/">https://tongji.baidu.com/</a>和<a href="https://search.google.com/">谷歌统计</a>功能，如果没有可以自行在head文件内添加，和上面的html标签验证相似</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200715193338.png"></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="javascript"></span></span><br><span class="line"><span class="javascript"><span class="keyword">var</span> _hmt = _hmt || [];</span></span><br><span class="line"><span class="javascript">(<span class="function"><span class="keyword">function</span>(<span class="params"></span>) </span>&#123;</span></span><br><span class="line"><span class="javascript">  <span class="keyword">var</span> hm = <span class="built_in">document</span>.createElement(<span class="string">&quot;script&quot;</span>);</span></span><br><span class="line"><span class="javascript">  hm.src = <span class="string">&quot;https://hm.baidu.com/hm.js?a2ee893562999ebad688b0d82daa100a&quot;</span>;</span></span><br><span class="line"><span class="javascript">  <span class="keyword">var</span> s = <span class="built_in">document</span>.getElementsByTagName(<span class="string">&quot;script&quot;</span>)[<span class="number">0</span>]; </span></span><br><span class="line"><span class="javascript">  s.parentNode.insertBefore(hm, s);</span></span><br><span class="line"><span class="javascript">&#125;)();</span></span><br><span class="line"><span class="javascript"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure><p>其中问号<code>?</code>之后的一串数字为你的统计id</p><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">hm.src</span> = <span class="string">&quot;https://hm.baidu.com/hm.js?a2ee893562999ebad688b0d82daa100a&quot;</span><span class="comment">;</span></span><br></pre></td></tr></table></figure><p>2.谷歌同理，不过需要翻墙才能进去</p><h2 id="4-10-CDN图床"><a href="#4-10-CDN图床" class="headerlink" title="4.10 CDN图床"></a>4.10 CDN图床</h2><p>1.博客中往往会使用到很多图片，如果全部都保存在博客中，那肯定是不行的，推荐使用<code>Github+Jsdelivr+PicGo</code>搭建免费图床<br>2.在Github中创建一个新仓库<code>CDN</code>，名字随意<br>3.生成Token<br>依次选择<code>Settings</code>-<code>Developer settings</code>-<code>Personal access tokens</code>-<code>Generate new token</code>，勾选<code>repo</code>，然后点击<code>Generate token</code>生成一个<code>Token</code></p><p><strong>注意这个Token只会显示一次，自己先保存下来，或者等后面配置好PicGo后再关闭此网页</strong></p><p>4.配置PicGo，使用jsDelivr的CDN<br>(1)下载<code>PicGo</code> <a href="https://github.com/Molunerfinn/picgo/releases">https://github.com/Molunerfinn/picgo/releases</a><br>(2)设置仓库名<br>(3)设置分支名<br>(4)设置Token<br>(5)指定存储路径<br>(6)设定自定义域名</p><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20200715200322.png"></p><figure class="highlight vala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#每次上传后,生成图片路径</span></span><br><span class="line">自定义域名+/+储存路径+上传的图片名</span><br></pre></td></tr></table></figure><h1 id="5-主题配置总结"><a href="#5-主题配置总结" class="headerlink" title="5. 主题配置总结"></a>5. 主题配置总结</h1><h2 id="5-1-Ayer主题修改–适用ejs类型的主题"><a href="#5-1-Ayer主题修改–适用ejs类型的主题" class="headerlink" title="5.1 Ayer主题修改–适用ejs类型的主题"></a>5.1 Ayer主题修改–适用ejs类型的主题</h2><h3 id="5-1-1-随机博客封面"><a href="#5-1-1-随机博客封面" class="headerlink" title="5.1.1 随机博客封面"></a>5.1.1 随机博客封面</h3><p><a href="/posts/27301.html">Hexo博客美化之随机封面</a></p><h3 id="5-1-2-添加二级菜单"><a href="#5-1-2-添加二级菜单" class="headerlink" title="5.1.2 添加二级菜单"></a>5.1.2 添加二级菜单</h3><p><a href="/posts/14357.html">Hexo博客添加二级菜单</a></p><h3 id="5-1-3-添加公告板"><a href="#5-1-3-添加公告板" class="headerlink" title="5.1.3 添加公告板"></a>5.1.3 添加公告板</h3><p><a href="/posts/57206.html">Hexo博客美化之添加公告板</a></p><h2 id="5-2-Butterfly主题–适用pug类型的主题"><a href="#5-2-Butterfly主题–适用pug类型的主题" class="headerlink" title="5.2 Butterfly主题–适用pug类型的主题"></a>5.2 Butterfly主题–适用pug类型的主题</h2><ul><li>看Butterfly作者的教程<a href="https://butterfly.js.org/">https://butterfly.js.org/</a></li><li>看小康博客<a href="https://www.antmoe.com/posts/a811d614/index.html">https://www.antmoe.com/posts/a811d614/index.html</a></li></ul><h1 id="6-主题魔改"><a href="#6-主题魔改" class="headerlink" title="6. 主题魔改"></a>6. 主题魔改</h1><h2 id="6-1-页脚养鱼🐟"><a href="#6-1-页脚养鱼🐟" class="headerlink" title="6.1 页脚养鱼🐟"></a>6.1 页脚养鱼🐟</h2><ul><li>摘取自<a href="https://xiabor.com/714f.html">木槿：Hexo大结局</a></li></ul><p>1.在<code>\themes\butterfly\layout\includes\footer.pug</code>最后添加这句话</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#jsi-flying-fish-container.container</span><br><span class="line"></span><br><span class="line">style.</span><br><span class="line">  @media only screen and (max-width: 767px)&#123;</span><br><span class="line">    #sidebar_search_box input[type=text]&#123;width:calc(100% - 24px)&#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>2.然后添加js文件，如果是butterfly在主题配置的inject处添加即可</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">data-pjax</span> <span class="attr">src</span>=<span class="string">&quot;https://cdn.jsdelivr.net/gh/Justlovesmile/CDN@latest/js/fish.js&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure><p>3.修改样式，butterfly在<code>themes\butterfly\source\css\_layout\footer.styl</code>，这一部分对应修改</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#footer</span></span><br><span class="line">  <span class="attr">position:</span> <span class="string">relative</span></span><br><span class="line">  <span class="attr">background:</span> <span class="string">$light-blue</span></span><br><span class="line">  <span class="attr">background-attachment:</span> <span class="string">local</span></span><br><span class="line">  <span class="attr">background-position:</span> <span class="string">bottom</span></span><br><span class="line">  <span class="attr">background-size:</span> <span class="string">cover</span></span><br><span class="line"></span><br><span class="line">  <span class="string">if</span> <span class="string">hexo-config(&#x27;footer_bg&#x27;)</span> <span class="type">!=</span> <span class="literal">false</span></span><br><span class="line">    <span class="string">&amp;:before</span></span><br><span class="line">      <span class="attr">position:</span> <span class="string">absolute</span></span><br><span class="line">      <span class="attr">width:</span> <span class="number">100</span><span class="string">%</span></span><br><span class="line">      <span class="attr">height:</span> <span class="number">100</span><span class="string">%</span></span><br><span class="line">      <span class="attr">background-color:</span> <span class="string">alpha($dark-black,</span> <span class="number">.1</span><span class="string">)</span></span><br><span class="line">      <span class="attr">content:</span> <span class="string">&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#footer-wrap</span></span><br><span class="line">  <span class="attr">position:</span> <span class="string">absolute</span></span><br><span class="line">  <span class="attr">padding:</span> <span class="number">1.</span><span class="string">2rem</span> <span class="string">1rem</span> <span class="number">1.</span><span class="string">4rem</span></span><br><span class="line">  <span class="attr">color:</span> <span class="string">$light-grey</span></span><br><span class="line">  <span class="attr">text-align:</span> <span class="string">center</span></span><br><span class="line">  <span class="attr">left:</span> <span class="number">0</span></span><br><span class="line">  <span class="attr">right:</span> <span class="number">0</span></span><br><span class="line">  <span class="string">top:0</span></span><br><span class="line">  <span class="attr">bottom:</span> <span class="number">0</span></span><br></pre></td></tr></table></figure><h2 id="6-2-valine评论"><a href="#6-2-valine评论" class="headerlink" title="6.2 valine评论"></a>6.2 valine评论</h2><p><a href="/posts/27831.html">博客美化之valine</a></p><h2 id="6-3-博客文章加密"><a href="#6-3-博客文章加密" class="headerlink" title="6.3 博客文章加密"></a>6.3 博客文章加密</h2><p><a href="/posts/43010.html">添加博客加密</a>: <code>文章添加密码</code>功能</p><h2 id="6-4-打字机效果"><a href="#6-4-打字机效果" class="headerlink" title="6.4 打字机效果"></a>6.4 打字机效果</h2><p><a href="/posts/7e7ef81c.html">花里胡哨的打字机js</a></p><p><a href="/posts/24067.html">Type.js打字机效果</a>: 添加<code>打字机效果</code></p><h2 id="6-5-图标，动态图标，网页运行时间，全站黑白，鼠标点击特效，网页动态标题，樱花，音效"><a href="#6-5-图标，动态图标，网页运行时间，全站黑白，鼠标点击特效，网页动态标题，樱花，音效" class="headerlink" title="6.5 图标，动态图标，网页运行时间，全站黑白，鼠标点击特效，网页动态标题，樱花，音效"></a>6.5 图标，动态图标，网页运行时间，全站黑白，鼠标点击特效，网页动态标题，樱花，音效</h2><p><a href="/posts/56163.html">博客中能用到的代码</a>: 关于<code>font awesome图标</code>字体库，使用<code>动态图标</code>，添加<code>网页运行时间</code>，<code>全站变黑白</code>，<code>鼠标点击特效</code>，<code>网页标题的动态效果</code>，<code>网页樱花特效</code>，<code>鼠标触动音乐特效</code></p><h2 id="6-6-旋转小人，每日诗句"><a href="#6-6-旋转小人，每日诗句" class="headerlink" title="6.6 旋转小人，每日诗句"></a>6.6 旋转小人，每日诗句</h2><p><a href="/posts/15391.html">博客中能用到的代码（二）</a>: 添加<code>旋转小人</code>和<code>每日诗句</code></p><h2 id="6-7-展示pdf"><a href="#6-7-展示pdf" class="headerlink" title="6.7 展示pdf"></a>6.7 展示pdf</h2><p><a href="/posts/7376.html">Hexo竟然可以展示PDF</a></p><h2 id="6-8-插件汇总"><a href="#6-8-插件汇总" class="headerlink" title="6.8 插件汇总"></a>6.8 插件汇总</h2><p><a href="/posts/86111745.html">Hexo插件总结推荐</a></p><h2 id="6-9-前端禁止右键，F12，F5"><a href="#6-9-前端禁止右键，F12，F5" class="headerlink" title="6.9 前端禁止右键，F12，F5"></a>6.9 前端禁止右键，F12，F5</h2><ul><li>在文件中添加以下代码</li></ul><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span>&gt;</span><span class="javascript"></span></span><br><span class="line"><span class="javascript"><span class="function"><span class="keyword">function</span> <span class="title">unmouse</span>(<span class="params"></span>)</span>&#123;</span></span><br><span class="line"><span class="javascript"><span class="built_in">document</span>.oncontextmenu = <span class="keyword">new</span> <span class="built_in">Function</span>(<span class="string">&quot;return false;&quot;</span>);</span></span><br><span class="line"><span class="javascript"><span class="built_in">document</span>.onkeydown = <span class="built_in">document</span>.onkeyup = <span class="built_in">document</span>.onkeypress = <span class="function"><span class="keyword">function</span>(<span class="params">event</span>) </span>&#123;</span></span><br><span class="line"><span class="javascript"><span class="keyword">var</span> e = event || <span class="built_in">window</span>.event || <span class="built_in">arguments</span>.callee.caller.arguments[<span class="number">0</span>];</span></span><br><span class="line"><span class="javascript"><span class="keyword">if</span> (e &amp;&amp; (e.keyCode == <span class="number">123</span> || (e.keyCode == <span class="number">116</span> &amp;&amp; e.type!=<span class="string">&#x27;keypress&#x27;</span>))) </span></span><br><span class="line"><span class="javascript">&#123;</span></span><br><span class="line"><span class="javascript">e.returnValue = <span class="literal">false</span>;</span></span><br><span class="line"><span class="javascript"><span class="keyword">return</span> (<span class="literal">false</span>);</span></span><br><span class="line"><span class="javascript">&#125;</span></span><br><span class="line"><span class="javascript">&#125;</span></span><br><span class="line"><span class="javascript">&#125;</span></span><br><span class="line"><span class="javascript">unmouse()</span></span><br><span class="line"><span class="javascript"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="6-10-星空背景"><a href="#6-10-星空背景" class="headerlink" title="6.10 星空背景"></a>6.10 星空背景</h2><ul><li><a href="/posts/6a260bf6.html">星空和流星特效</a><br><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/202108121834269.gif"></li></ul><h1 id="7-高级魔改"><a href="#7-高级魔改" class="headerlink" title="7. 高级魔改"></a>7. 高级魔改</h1><h2 id="7-1-Github-Calendar"><a href="#7-1-Github-Calendar" class="headerlink" title="7.1 Github Calendar"></a>7.1 Github Calendar</h2><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210208220529.png"></p><p><a href="https://akilar.top/posts/1f9c68c9/">Gitcalendar</a><br><a href="https://zfe.space/post/6948.html">教程：基于Butterfly主题（去jquery）的gitcalendar3.0</a></p><h2 id="7-2-首页磁贴"><a href="#7-2-首页磁贴" class="headerlink" title="7.2 首页磁贴"></a>7.2 首页磁贴</h2><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210208220855.png"><br><a href="https://akilar.top/posts/a9131002/">Akilar：Categories Magnet</a></p><h2 id="7-3-首页置顶轮播图"><a href="#7-3-首页置顶轮播图" class="headerlink" title="7.3 首页置顶轮播图"></a>7.3 首页置顶轮播图</h2><p><img src="https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210208220928.png"></p><p><a href="https://akilar.top/posts/8e1264d1/">Akilar：Slider Bar</a></p>]]></content>
    
    
    <summary type="html">Hexo博客搭建全过程，环境部署，Hexo基础命令，域名设置，CDN图床，博客魔改，页脚养鱼，前端禁止右键等等...</summary>
    
    
    
    <category term="博客相关" scheme="https://blog.justlovesmile.top/categories/%E5%8D%9A%E5%AE%A2%E7%9B%B8%E5%85%B3/"/>
    
    
    <category term="前端" scheme="https://blog.justlovesmile.top/tags/%E5%89%8D%E7%AB%AF/"/>
    
    <category term="Hexo" scheme="https://blog.justlovesmile.top/tags/Hexo/"/>
    
    <category term="博客" scheme="https://blog.justlovesmile.top/tags/%E5%8D%9A%E5%AE%A2/"/>
    
  </entry>
  
</feed>
