{"title":"旋转目标检测 | IENet，单阶段Anchor-Free旋转目标检测模型","slug":"人工智能-IENet，单阶段Anchor-Free旋转目标检测模型","date":"2022-06-30T02:35:40.000Z","updated":"2022-06-30T02:35:40.000Z","comments":true,"path":"api/articles/人工智能-IENet，单阶段Anchor-Free旋转目标检测模型.json","excerpt":null,"covers":["https://unpkg.com/justlovesmile-post@1.0.2/20220420214710.png","https://unpkg.com/justlovesmile-post@1.0.2/20220420221515.png","https://unpkg.com/justlovesmile-post@1.0.2/20220420221600.png","https://unpkg.com/justlovesmile-post@1.0.2/20220422192914.png","https://unpkg.com/justlovesmile-post@1.0.2/20220420221612.png","https://unpkg.com/justlovesmile-post@1.0.2/20220422200843.png","https://unpkg.com/justlovesmile-post@1.0.2/20220420222408.png","https://unpkg.com/justlovesmile-post@1.0.2/20220420222445.png","https://unpkg.com/justlovesmile-post@1.0.2/20220630104801.png","https://unpkg.com/justlovesmile-post@1.0.2/20220630104802.png","https://unpkg.com/justlovesmile-post@1.0.2/20220630104803.png"],"content":"<h1 id=\"1-论文信息\"><a href=\"#1-论文信息\" class=\"headerlink\" title=\"1. 论文信息\"></a>1. 论文信息</h1><p>论文标题：《IENet: Interactive Embranchment Network Based One-Stage Anchor Free Detector for Orientational Aerial Object Detection》</p>\n<blockquote>\n<p>论文发表：arxiv 2019<br>论文链接：<a href=\"https://arxiv.org/pdf/1912.00969\">https://arxiv.org/pdf/1912.00969</a></p>\n</blockquote>\n<p><img src=\"https://unpkg.com/justlovesmile-post@1.0.2/20220420214710.png\"></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@article&#123;lin2019ienet,</span><br><span class=\"line\">  title=&#123;IENet: Interacting embranchment one stage anchor free detector for orientation aerial object detection&#125;,</span><br><span class=\"line\">  author=&#123;Lin, Youtian and Feng, Pengming and Guan, Jian and Wang, Wenwu and Chambers, Jonathon&#125;,</span><br><span class=\"line\">  journal=&#123;arXiv preprint arXiv:1912.00969&#125;,</span><br><span class=\"line\">  year=&#123;2019&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n<h1 id=\"2-归纳总结\"><a href=\"#2-归纳总结\" class=\"headerlink\" title=\"2. 归纳总结\"></a>2. 归纳总结</h1><table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Value</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>标签</td>\n<td>#遥感 #目标检测</td>\n</tr>\n<tr>\n<td>数据集</td>\n<td>DOTA,HRSC2016</td>\n</tr>\n<tr>\n<td>目的</td>\n<td>两阶段方法计算量大，单阶段方法性能不足</td>\n</tr>\n<tr>\n<td>方法</td>\n<td>基于自注意力的互动分支</td>\n</tr>\n</tbody></table>\n<h1 id=\"3-问题背景\"><a href=\"#3-问题背景\" class=\"headerlink\" title=\"3. 问题背景\"></a>3. 问题背景</h1><p>作者提到，遥感图像的目标检测任务的难点在于：</p>\n<ul>\n<li>和自然图像相比，物体形状相似且可见特征稀少</li>\n<li>目标具有不同的旋转角度</li>\n<li>具有更多的小目标和密集目标</li>\n</ul>\n<p>而目前最好的性能都是两阶段算法实现的，但是两阶段算法通常在第一阶段定位，在第二阶段分类，因此计算量是非常大的，尤其是对旋转目标检测而言，因为Anchor匹配（涉及IoU计算）和RoI特征提取的计算量大。</p>\n<h1 id=\"4-主要工作\"><a href=\"#4-主要工作\" class=\"headerlink\" title=\"4. 主要工作\"></a>4. 主要工作</h1><p>针对上诉问题，作者提出了<strong>IENet</strong>（interactive embranchment network），其是一个<strong>单阶段</strong>的<strong>Anchor-Free旋转目标</strong>检测器，其包含如下贡献点：</p>\n<ul>\n<li>一个新的geometric transformation（几何变换），用于更好地表示旋转目标框</li>\n<li>一个基于自注意力机制的分支交互模块（a branch interactive module with a self-attention mechanism）</li>\n<li>一个针对旋转框检测改进的IoU Loss</li>\n</ul>\n<h2 id=\"4-1-模型结构\"><a href=\"#4-1-模型结构\" class=\"headerlink\" title=\"4.1 模型结构\"></a>4.1 模型结构</h2><h3 id=\"（1）-Baseline模型结构-FCOS-O\"><a href=\"#（1）-Baseline模型结构-FCOS-O\" class=\"headerlink\" title=\"（1） Baseline模型结构(FCOS-O)\"></a>（1） Baseline模型结构(FCOS-O)</h3><p>在FCOS的基础上增加了一个独立的角度回归分支（Orientation Regression）<br><img src=\"https://unpkg.com/justlovesmile-post@1.0.2/20220420221515.png\"></p>\n<h3 id=\"（2）-IENet\"><a href=\"#（2）-IENet\" class=\"headerlink\" title=\"（2） IENet\"></a>（2） IENet</h3><p>由于独立的角度分支不能很好的利用位置等信息，因此检测性能下降，基于此IENet提出了基于自注意力机制的分支交互模块即IE（Interactive Embranchment） Module，用于利用分类和位置回归信息。<br><img src=\"https://unpkg.com/justlovesmile-post@1.0.2/20220420221600.png\"></p>\n<h2 id=\"4-2-旋转框几何变换\"><a href=\"#4-2-旋转框几何变换\" class=\"headerlink\" title=\"4.2 旋转框几何变换\"></a>4.2 旋转框几何变换</h2><p>IENet使用HBB+几何变换来表征OBB，如下图所示：</p>\n<p><img src=\"https://unpkg.com/justlovesmile-post@1.0.2/20220422192914.png\"></p>\n<p>IENet使用HBB加h,w来表示一个OBB，其中GT OBB是一个8维的向量$[x_1,y_1,x_2,y_2,x_3,y_3,x_4,y_4]$，HBB可由$[x_{min},y_{min},x_{max},y_{max}]$表示，h和w计算如下：<br>$$w=x_{max}-x_2$$<br>$$h=y_{max}-y_1$$<br>基于上述几何变化，可将OBB回归问题转换成HBB回归和方向回归的问题，其中HBB回归和FCOS的一致，为Box的偏移量$R_b=[l,t,r,b]$，方向回归为$R_o=[w,h]$.</p>\n<h2 id=\"4-3-IE-Module\"><a href=\"#4-3-IE-Module\" class=\"headerlink\" title=\"4.3 IE Module\"></a>4.3 IE Module</h2><p>IE模块结构图：<br><img src=\"https://unpkg.com/justlovesmile-post@1.0.2/20220420221612.png\"></p>\n<p>在获得了分类和位置回归的特征图$F^m$后，使用1x1的卷积层和softmax层来构建自注意力模块：</p>\n<ul>\n<li>首先利用三个1x1的卷积层$f(F^m),g(F^m),h(F^m)$将特征映射到三个特征空间</li>\n<li>将$f(\\cdot)$和$g(\\cdot)$和并通过softmax层组成注意力图$\\gamma= softmax(f(F^m)^Tg(F^m))$，因此特征图之间的关系为$\\gamma_{q,p}= \\frac{exp(\\delta_{pq})}{\\sum_{p=1}^{N}exp(\\delta_{pq})}$，其中$q,p\\in{1,…,N}$为注意力图的行号和列号，$\\delta$代表$f(F^m)^Tg(F^m)$输出的NxN的矩阵</li>\n<li>然后，注意图可以用来表示输入特征之间的关系，并对$h(\\cdot)$的起作用，得到$\\theta=(\\theta_1,\\theta_2,…,\\theta_1,…,\\theta_N)$，且$\\theta_q=\\sum_{p=1}^N\\gamma_{q,p}h(f_p^m)$</li>\n<li>为了保留原始特征信息，最后输出的特征为$\\mathbb{Y}=\\gamma\\theta+F^m$</li>\n</ul>\n<h2 id=\"4-4-损失函数\"><a href=\"#4-4-损失函数\" class=\"headerlink\" title=\"4.4 损失函数\"></a>4.4 损失函数</h2><p>总损失为：<br>$$L = \\frac{1}{N_{pos}}L_{cls}+\\frac{\\lambda}{N_pos}L_{reg}+\\frac{\\omega}{N_{pos}}L_{ori}$$<br>其中分类损失为Focal Loss损失<br><img src=\"https://unpkg.com/justlovesmile-post@1.0.2/20220422200843.png\"><br>位置回归损失为centerness损失加smoothL1<br><img src=\"https://unpkg.com/justlovesmile-post@1.0.2/20220422200856.png\"><br>角度回归损失为smoothL1损失<br><img src=\"https://unpkg.com/justlovesmile-post@1.0.2/20220422200905.png\"></p>\n<h1 id=\"5-实验结果\"><a href=\"#5-实验结果\" class=\"headerlink\" title=\"5. 实验结果\"></a>5. 实验结果</h1><p>DOTA_v1<br><img src=\"https://unpkg.com/justlovesmile-post@1.0.2/20220420222408.png\"></p>\n<p>HRSC2016<br><img src=\"https://unpkg.com/justlovesmile-post@1.0.2/20220420222445.png\"></p>\n<p>同时IENet在推理和训练时的速度上也有优势：</p>\n<p><img src=\"https://unpkg.com/justlovesmile-post@1.0.2/20220630104801.png\"></p>\n<p>消融实验：</p>\n<p><img src=\"https://unpkg.com/justlovesmile-post@1.0.2/20220630104802.png\"></p>\n<p>特征可视化：</p>\n<p><img src=\"https://unpkg.com/justlovesmile-post@1.0.2/20220630104803.png\"></p>\n","more":"<h1 id=\"1-论文信息\"><a href=\"#1-论文信息\" class=\"headerlink\" title=\"1. 论文信息\"></a>1. 论文信息</h1><p>论文标题：《IENet: Interactive Embranchment Network Based One-Stage Anchor Free Detector for Orientational Aerial Object Detection》</p>\n<blockquote>\n<p>论文发表：arxiv 2019<br>论文链接：<a href=\"https://arxiv.org/pdf/1912.00969\">https://arxiv.org/pdf/1912.00969</a></p>\n</blockquote>\n<p><img src=\"https://unpkg.com/justlovesmile-post@1.0.2/20220420214710.png\"></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@article&#123;lin2019ienet,</span><br><span class=\"line\">  title=&#123;IENet: Interacting embranchment one stage anchor free detector for orientation aerial object detection&#125;,</span><br><span class=\"line\">  author=&#123;Lin, Youtian and Feng, Pengming and Guan, Jian and Wang, Wenwu and Chambers, Jonathon&#125;,</span><br><span class=\"line\">  journal=&#123;arXiv preprint arXiv:1912.00969&#125;,</span><br><span class=\"line\">  year=&#123;2019&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n<h1 id=\"2-归纳总结\"><a href=\"#2-归纳总结\" class=\"headerlink\" title=\"2. 归纳总结\"></a>2. 归纳总结</h1><table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Value</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>标签</td>\n<td>#遥感 #目标检测</td>\n</tr>\n<tr>\n<td>数据集</td>\n<td>DOTA,HRSC2016</td>\n</tr>\n<tr>\n<td>目的</td>\n<td>两阶段方法计算量大，单阶段方法性能不足</td>\n</tr>\n<tr>\n<td>方法</td>\n<td>基于自注意力的互动分支</td>\n</tr>\n</tbody></table>\n<h1 id=\"3-问题背景\"><a href=\"#3-问题背景\" class=\"headerlink\" title=\"3. 问题背景\"></a>3. 问题背景</h1><p>作者提到，遥感图像的目标检测任务的难点在于：</p>\n<ul>\n<li>和自然图像相比，物体形状相似且可见特征稀少</li>\n<li>目标具有不同的旋转角度</li>\n<li>具有更多的小目标和密集目标</li>\n</ul>\n<p>而目前最好的性能都是两阶段算法实现的，但是两阶段算法通常在第一阶段定位，在第二阶段分类，因此计算量是非常大的，尤其是对旋转目标检测而言，因为Anchor匹配（涉及IoU计算）和RoI特征提取的计算量大。</p>\n<h1 id=\"4-主要工作\"><a href=\"#4-主要工作\" class=\"headerlink\" title=\"4. 主要工作\"></a>4. 主要工作</h1><p>针对上诉问题，作者提出了<strong>IENet</strong>（interactive embranchment network），其是一个<strong>单阶段</strong>的<strong>Anchor-Free旋转目标</strong>检测器，其包含如下贡献点：</p>\n<ul>\n<li>一个新的geometric transformation（几何变换），用于更好地表示旋转目标框</li>\n<li>一个基于自注意力机制的分支交互模块（a branch interactive module with a self-attention mechanism）</li>\n<li>一个针对旋转框检测改进的IoU Loss</li>\n</ul>\n<h2 id=\"4-1-模型结构\"><a href=\"#4-1-模型结构\" class=\"headerlink\" title=\"4.1 模型结构\"></a>4.1 模型结构</h2><h3 id=\"（1）-Baseline模型结构-FCOS-O\"><a href=\"#（1）-Baseline模型结构-FCOS-O\" class=\"headerlink\" title=\"（1） Baseline模型结构(FCOS-O)\"></a>（1） Baseline模型结构(FCOS-O)</h3><p>在FCOS的基础上增加了一个独立的角度回归分支（Orientation Regression）<br><img src=\"https://unpkg.com/justlovesmile-post@1.0.2/20220420221515.png\"></p>\n<h3 id=\"（2）-IENet\"><a href=\"#（2）-IENet\" class=\"headerlink\" title=\"（2） IENet\"></a>（2） IENet</h3><p>由于独立的角度分支不能很好的利用位置等信息，因此检测性能下降，基于此IENet提出了基于自注意力机制的分支交互模块即IE（Interactive Embranchment） Module，用于利用分类和位置回归信息。<br><img src=\"https://unpkg.com/justlovesmile-post@1.0.2/20220420221600.png\"></p>\n<h2 id=\"4-2-旋转框几何变换\"><a href=\"#4-2-旋转框几何变换\" class=\"headerlink\" title=\"4.2 旋转框几何变换\"></a>4.2 旋转框几何变换</h2><p>IENet使用HBB+几何变换来表征OBB，如下图所示：</p>\n<p><img src=\"https://unpkg.com/justlovesmile-post@1.0.2/20220422192914.png\"></p>\n<p>IENet使用HBB加h,w来表示一个OBB，其中GT OBB是一个8维的向量$[x_1,y_1,x_2,y_2,x_3,y_3,x_4,y_4]$，HBB可由$[x_{min},y_{min},x_{max},y_{max}]$表示，h和w计算如下：<br>$$w=x_{max}-x_2$$<br>$$h=y_{max}-y_1$$<br>基于上述几何变化，可将OBB回归问题转换成HBB回归和方向回归的问题，其中HBB回归和FCOS的一致，为Box的偏移量$R_b=[l,t,r,b]$，方向回归为$R_o=[w,h]$.</p>\n<h2 id=\"4-3-IE-Module\"><a href=\"#4-3-IE-Module\" class=\"headerlink\" title=\"4.3 IE Module\"></a>4.3 IE Module</h2><p>IE模块结构图：<br><img src=\"https://unpkg.com/justlovesmile-post@1.0.2/20220420221612.png\"></p>\n<p>在获得了分类和位置回归的特征图$F^m$后，使用1x1的卷积层和softmax层来构建自注意力模块：</p>\n<ul>\n<li>首先利用三个1x1的卷积层$f(F^m),g(F^m),h(F^m)$将特征映射到三个特征空间</li>\n<li>将$f(\\cdot)$和$g(\\cdot)$和并通过softmax层组成注意力图$\\gamma= softmax(f(F^m)^Tg(F^m))$，因此特征图之间的关系为$\\gamma_{q,p}= \\frac{exp(\\delta_{pq})}{\\sum_{p=1}^{N}exp(\\delta_{pq})}$，其中$q,p\\in{1,…,N}$为注意力图的行号和列号，$\\delta$代表$f(F^m)^Tg(F^m)$输出的NxN的矩阵</li>\n<li>然后，注意图可以用来表示输入特征之间的关系，并对$h(\\cdot)$的起作用，得到$\\theta=(\\theta_1,\\theta_2,…,\\theta_1,…,\\theta_N)$，且$\\theta_q=\\sum_{p=1}^N\\gamma_{q,p}h(f_p^m)$</li>\n<li>为了保留原始特征信息，最后输出的特征为$\\mathbb{Y}=\\gamma\\theta+F^m$</li>\n</ul>\n<h2 id=\"4-4-损失函数\"><a href=\"#4-4-损失函数\" class=\"headerlink\" title=\"4.4 损失函数\"></a>4.4 损失函数</h2><p>总损失为：<br>$$L = \\frac{1}{N_{pos}}L_{cls}+\\frac{\\lambda}{N_pos}L_{reg}+\\frac{\\omega}{N_{pos}}L_{ori}$$<br>其中分类损失为Focal Loss损失<br><img src=\"https://unpkg.com/justlovesmile-post@1.0.2/20220422200843.png\"><br>位置回归损失为centerness损失加smoothL1<br><img src=\"https://unpkg.com/justlovesmile-post@1.0.2/20220422200856.png\"><br>角度回归损失为smoothL1损失<br><img src=\"https://unpkg.com/justlovesmile-post@1.0.2/20220422200905.png\"></p>\n<h1 id=\"5-实验结果\"><a href=\"#5-实验结果\" class=\"headerlink\" title=\"5. 实验结果\"></a>5. 实验结果</h1><p>DOTA_v1<br><img src=\"https://unpkg.com/justlovesmile-post@1.0.2/20220420222408.png\"></p>\n<p>HRSC2016<br><img src=\"https://unpkg.com/justlovesmile-post@1.0.2/20220420222445.png\"></p>\n<p>同时IENet在推理和训练时的速度上也有优势：</p>\n<p><img src=\"https://unpkg.com/justlovesmile-post@1.0.2/20220630104801.png\"></p>\n<p>消融实验：</p>\n<p><img src=\"https://unpkg.com/justlovesmile-post@1.0.2/20220630104802.png\"></p>\n<p>特征可视化：</p>\n<p><img src=\"https://unpkg.com/justlovesmile-post@1.0.2/20220630104803.png\"></p>\n","categories":[{"name":"人工智能","path":"api/categories/人工智能.json"}],"tags":[{"name":"深度学习","path":"api/tags/深度学习.json"},{"name":"论文笔记","path":"api/tags/论文笔记.json"},{"name":"目标检测","path":"api/tags/目标检测.json"}]}