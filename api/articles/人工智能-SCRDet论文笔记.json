{"title":"旋转目标检测 | SCRDet，适用于旋转、密集、小目标的检测器","slug":"人工智能-SCRDet论文笔记","date":"2022-07-04T07:21:48.000Z","updated":"2022-07-04T07:21:48.000Z","comments":true,"path":"api/articles/人工智能-SCRDet论文笔记.json","excerpt":null,"covers":["https://unpkg.com/justlovesmile-post@1.0.7/20220306124344.png","https://unpkg.com/justlovesmile-post@1.0.7/20220306125233.png","https://unpkg.com/justlovesmile-post@1.0.7/20220306130142.png","https://unpkg.com/justlovesmile-post@1.0.7/20220306130354.png","https://unpkg.com/justlovesmile-post@1.0.7/20220306130754.png","https://unpkg.com/justlovesmile-post@1.0.7/20220306131108.png","https://unpkg.com/justlovesmile-post@1.0.7/20220306131310.png","https://unpkg.com/justlovesmile-post@1.0.7/20220306131425.png","https://unpkg.com/justlovesmile-post@1.0.7/20220306131447.png"],"content":"<h1 id=\"1-论文信息\"><a href=\"#1-论文信息\" class=\"headerlink\" title=\"1. 论文信息\"></a>1. 论文信息</h1><p>论文标题：<code>《SCRDet: Towards More Robust Detection for Small, Cluttered and Rotated Objects》</code></p>\n<blockquote>\n<p>论文发表：ICCV2019<br>论文链接：<a href=\"https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_SCRDet_Towards_More_Robust_Detection_for_Small_Cluttered_and_Rotated_ICCV_2019_paper.pdf\">https://openaccess.thecvf.com</a><br>论文代码：<a href=\"https://github.com/DetectionTeamUCAS\">https://github.com/DetectionTeamUCAS</a></p>\n</blockquote>\n<p><img src=\"https://unpkg.com/justlovesmile-post@1.0.7/20220306124344.png\"></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@inproceedings&#123;yang2019scrdet,</span><br><span class=\"line\">  title=&#123;Scrdet: Towards more robust detection for small, cluttered and rotated objects&#125;,</span><br><span class=\"line\">  author=&#123;Yang, Xue and Yang, Jirui and Yan, Junchi and Zhang, Yue and Zhang, Tengfei and Guo, Zhi and Sun, Xian and Fu, Kun&#125;,</span><br><span class=\"line\">  booktitle=&#123;Proceedings of the IEEE/CVF International Conference on Computer Vision&#125;,</span><br><span class=\"line\">  pages=&#123;8232--8241&#125;,</span><br><span class=\"line\">  year=&#123;2019&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n<h1 id=\"2-归纳总结\"><a href=\"#2-归纳总结\" class=\"headerlink\" title=\"2. 归纳总结\"></a>2. 归纳总结</h1><table>\n<thead>\n<tr>\n<th>标签</th>\n<th>目的</th>\n<th>方法</th>\n<th>总结</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>#遥感 #注意力机制 #旋转目标检测</td>\n<td>解决了遥感目标角度边界问题</td>\n<td>IoU SmoothL1 Loss，多维注意力</td>\n<td>从遥感目标的难点出发</td>\n</tr>\n</tbody></table>\n<h1 id=\"3-问题背景\"><a href=\"#3-问题背景\" class=\"headerlink\" title=\"3. 问题背景\"></a>3. 问题背景</h1><p>遥感目标检测的难点：</p>\n<ul>\n<li>小目标（small size）</li>\n<li>密集（dense distribution）</li>\n<li>方向任意（arbitrary direction）</li>\n</ul>\n<h1 id=\"4-主要工作\"><a href=\"#4-主要工作\" class=\"headerlink\" title=\"4. 主要工作\"></a>4. 主要工作</h1><p>针对上述问题进行改进：</p>\n<ul>\n<li>  对于小目标：通过特征融合和anchor采样角度出发设计了一个特征融合结构。</li>\n<li>  对于密集排列问题：设计了一个有监督的多维注意力网络（supervised pixel attention network and the channel attention network）以减少背景噪声的不利影响。</li>\n<li>  对于任意方向问题：通过添加IoU常数因子设计了一种改进的平滑L1损失，该因子专门用于解决旋转边界框回归的边界问题。</li>\n</ul>\n<h1 id=\"5-相关工作\"><a href=\"#5-相关工作\" class=\"headerlink\" title=\"5. 相关工作\"></a>5. 相关工作</h1><p>目标检测经典模型：</p>\n<ul>\n<li>两阶段：Fast R-CNN，Faster R-CNN，R-FCN</li>\n<li>单阶段：YOLO，SSD</li>\n</ul>\n<p>针对小目标：RP-Faster R-CNN</p>\n<h1 id=\"6-模型方法\"><a href=\"#6-模型方法\" class=\"headerlink\" title=\"6. 模型方法\"></a>6. 模型方法</h1><p>整个框架基于Faster R-CNN based R2CNN实现，模型结构如下图：<br><img src=\"https://unpkg.com/justlovesmile-post@1.0.7/20220306125233.png\"></p>\n<p>主要包含三个部分：</p>\n<ul>\n<li>SF-Net</li>\n<li>MDA-Net</li>\n<li>Rotation-Branch</li>\n</ul>\n<h2 id=\"6-1-SF-Net\"><a href=\"#6-1-SF-Net\" class=\"headerlink\" title=\"6.1 SF-Net\"></a>6.1 SF-Net</h2><p><img src=\"https://unpkg.com/justlovesmile-post@1.0.7/20220306130142.png\"><br>针对小目标检测，作者认为<strong>特征融合</strong>和<strong>有效采样</strong>是关键。对于anchor-based来说，anchor的铺设方式直接影响正样本采样率。经典的anchor铺设方式和特征图的分辩率有关，也就是anchor铺设的步长（C2-C5上的anchor步长分别是4,8,16,32）。随着网络加加深，特征图分辨率下降，anchor的步长扩大，常常会导致小目标的采样丢失，如下图所示：<br><img src=\"https://unpkg.com/justlovesmile-post@1.0.7/20220306125943.png\"><br>文章通过resize的方式选取了一个合适的特征图分别率，尽可能保证小目标都被采样到，再加上简单的特征融合保证丰富的语义信息和位置信息。在这里之所以不使用C2，是因为遥感目标检测会设置较多的尺度和比例，那么在C2这个特征图上面的anchor就变得太多了，而且在遥感数据集中最小的目标一般也都在10像素以上（特指<a href=\"https://arxiv.org/abs/1807.02700\">DOTA1.0</a>，<a href=\"https://captain-whu.github.io/DOAI2019/dataset.html\">DOTA1.5</a>则给出了像素10以下的标注）。</p>\n<h2 id=\"6-2-MAD-Net\"><a href=\"#6-2-MAD-Net\" class=\"headerlink\" title=\"6.2 MAD-Net\"></a>6.2 MAD-Net</h2><p>由于遥感图像背景的复杂性，RPN产生的建议区域可能引入大量噪声信息，如下图所示:<br><img src=\"https://unpkg.com/justlovesmile-post@1.0.7/20220306130354.png\"><br>过多的噪音可能会混淆物体信息，物体之间的界限将变得模糊，导致漏检并增加虚警。因此，有必要增强物体特征并削弱非物体特征。为了更有效地捕捉复杂背景下小物体的特征，文章设计了一种有监督的多维注意力网络（MDA-Net），如下图所示。具体来说，在基于像素的注意网络中，特征图F3通过具有不同大小卷积核进行卷积运算，学习得到双通道的显著图（参见上图d）。这个显著图显示了前景和背景的分数。选择显著图中的一个通道与F3相乘，得到新的信息特征图A3（参见上图c）。需要注意的是，Softmax函数之后的显着图的值在[0,1]之间。换句话说，它可以降低噪声并相对的增强对象信息。由于显著图是连续的，因此不会完全消除背景信息，这有利于保留某些上下文信息并提高鲁棒性。<br><img src=\"https://unpkg.com/justlovesmile-post@1.0.7/20220306130420.png\"><br>其实这个模块现在也是被用的比较烂了，就是空间注意力加通道注意力的组合。但在实际的应用过程中，空间注意力在遥感检测真的是非常有用的；</p>\n<h2 id=\"6-3-IoU-Smooth-L1-Loss\"><a href=\"#6-3-IoU-Smooth-L1-Loss\" class=\"headerlink\" title=\"6.3 IoU-Smooth L1 Loss\"></a>6.3 IoU-Smooth L1 Loss</h2><p>首先我们要先了解一下两种旋转边界框的两种常见的方式，下图来自这篇文章的作者yangxue：<br><img src=\"https://unpkg.com/justlovesmile-post@1.0.7/20220306130754.png\"><br>SCRDet是采用的opencv 表示法。在当前常用的旋转检测框的角度定义下，由于存在旋转角度的边界问题，会产生不必要的损失，如下图所示：<br><img src=\"https://unpkg.com/justlovesmile-post@1.0.7/20220306130911.png\"></p>\n<p>最理想的角度回归路线是由蓝色框逆时针旋转到红色框，但由于角度的周期性，导致按照这个回归方式的损失非常大（参见上图右边的Example）。此时模型必须以更复杂的形式回归（例如蓝色框顺时针旋转，同时缩放w和h），增加了回归的难度。为了更好地解决这个问题，我们在传统的smooth L1 损失函数中引入了IoU常数因子。在边界情况下，新的损失函数近似等于0，消除了损失的突增。新的回归损失可分为两部分，smooth L1回归损失函数取单位向量确定梯度传播的方向，而IoU表示梯度的大小，这样loss函数就变得连续。此外，使用IoU优化回归任务与评估方法的度量标准保持一致，这比坐标回归更直接和有效。IoU-Smooth L1 loss公式如下： </p>\n<p><img src=\"https://unpkg.com/justlovesmile-post@1.0.7/20220306131108.png\"></p>\n<p>可以看一下两种loss在边界情况下的效果对比：<br><img src=\"https://unpkg.com/justlovesmile-post@1.0.7/20220306131310.png\"></p>\n<p>导致这种原因的根本原因是角度的预测超出了所定义范围。其实解决这种问题的方法并不唯一，<a href=\"https://arxiv.org/abs/1703.01086\">RRPN</a>和<a href=\"https://www.mdpi.com/2072-4292/10/1/132\">R-DFPN</a>在论文的loss公式中就判断了是不是在定义范围内，通过加减$k\\pi$来缓解这个问题，但这种做法明显不优美而且仍然存在问题，主要是较难判断超出预测范围几个角度周期。当然可以通过对角度部分的loss加一个周期性函数，比如tan、cos等三角函数来做，但是我在实际使用过程中常常出现不收敛的情况。对于边界问题，我其实还做了其他方法的研究，会在以后的文章中详细讨论。</p>\n<h1 id=\"7-实验结果\"><a href=\"#7-实验结果\" class=\"headerlink\" title=\"7. 实验结果\"></a>7. 实验结果</h1><p>消融实验：<br><img src=\"https://unpkg.com/justlovesmile-post@1.0.7/20220306131425.png\"></p>\n<p>对比实验：<br><img src=\"https://unpkg.com/justlovesmile-post@1.0.7/20220306131447.png\"></p>\n<h1 id=\"8-参考文献\"><a href=\"#8-参考文献\" class=\"headerlink\" title=\"8. 参考文献\"></a>8. 参考文献</h1><p><a href=\"https://zhuanlan.zhihu.com/p/107400817\">旋转目标检测方法解读 （SCRDet, ICCV2019） - 知乎 (zhihu.com)</a></p>\n","more":"<h1 id=\"1-论文信息\"><a href=\"#1-论文信息\" class=\"headerlink\" title=\"1. 论文信息\"></a>1. 论文信息</h1><p>论文标题：<code>《SCRDet: Towards More Robust Detection for Small, Cluttered and Rotated Objects》</code></p>\n<blockquote>\n<p>论文发表：ICCV2019<br>论文链接：<a href=\"https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_SCRDet_Towards_More_Robust_Detection_for_Small_Cluttered_and_Rotated_ICCV_2019_paper.pdf\">https://openaccess.thecvf.com</a><br>论文代码：<a href=\"https://github.com/DetectionTeamUCAS\">https://github.com/DetectionTeamUCAS</a></p>\n</blockquote>\n<p><img src=\"https://unpkg.com/justlovesmile-post@1.0.7/20220306124344.png\"></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@inproceedings&#123;yang2019scrdet,</span><br><span class=\"line\">  title=&#123;Scrdet: Towards more robust detection for small, cluttered and rotated objects&#125;,</span><br><span class=\"line\">  author=&#123;Yang, Xue and Yang, Jirui and Yan, Junchi and Zhang, Yue and Zhang, Tengfei and Guo, Zhi and Sun, Xian and Fu, Kun&#125;,</span><br><span class=\"line\">  booktitle=&#123;Proceedings of the IEEE/CVF International Conference on Computer Vision&#125;,</span><br><span class=\"line\">  pages=&#123;8232--8241&#125;,</span><br><span class=\"line\">  year=&#123;2019&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n<h1 id=\"2-归纳总结\"><a href=\"#2-归纳总结\" class=\"headerlink\" title=\"2. 归纳总结\"></a>2. 归纳总结</h1><table>\n<thead>\n<tr>\n<th>标签</th>\n<th>目的</th>\n<th>方法</th>\n<th>总结</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>#遥感 #注意力机制 #旋转目标检测</td>\n<td>解决了遥感目标角度边界问题</td>\n<td>IoU SmoothL1 Loss，多维注意力</td>\n<td>从遥感目标的难点出发</td>\n</tr>\n</tbody></table>\n<h1 id=\"3-问题背景\"><a href=\"#3-问题背景\" class=\"headerlink\" title=\"3. 问题背景\"></a>3. 问题背景</h1><p>遥感目标检测的难点：</p>\n<ul>\n<li>小目标（small size）</li>\n<li>密集（dense distribution）</li>\n<li>方向任意（arbitrary direction）</li>\n</ul>\n<h1 id=\"4-主要工作\"><a href=\"#4-主要工作\" class=\"headerlink\" title=\"4. 主要工作\"></a>4. 主要工作</h1><p>针对上述问题进行改进：</p>\n<ul>\n<li>  对于小目标：通过特征融合和anchor采样角度出发设计了一个特征融合结构。</li>\n<li>  对于密集排列问题：设计了一个有监督的多维注意力网络（supervised pixel attention network and the channel attention network）以减少背景噪声的不利影响。</li>\n<li>  对于任意方向问题：通过添加IoU常数因子设计了一种改进的平滑L1损失，该因子专门用于解决旋转边界框回归的边界问题。</li>\n</ul>\n<h1 id=\"5-相关工作\"><a href=\"#5-相关工作\" class=\"headerlink\" title=\"5. 相关工作\"></a>5. 相关工作</h1><p>目标检测经典模型：</p>\n<ul>\n<li>两阶段：Fast R-CNN，Faster R-CNN，R-FCN</li>\n<li>单阶段：YOLO，SSD</li>\n</ul>\n<p>针对小目标：RP-Faster R-CNN</p>\n<h1 id=\"6-模型方法\"><a href=\"#6-模型方法\" class=\"headerlink\" title=\"6. 模型方法\"></a>6. 模型方法</h1><p>整个框架基于Faster R-CNN based R2CNN实现，模型结构如下图：<br><img src=\"https://unpkg.com/justlovesmile-post@1.0.7/20220306125233.png\"></p>\n<p>主要包含三个部分：</p>\n<ul>\n<li>SF-Net</li>\n<li>MDA-Net</li>\n<li>Rotation-Branch</li>\n</ul>\n<h2 id=\"6-1-SF-Net\"><a href=\"#6-1-SF-Net\" class=\"headerlink\" title=\"6.1 SF-Net\"></a>6.1 SF-Net</h2><p><img src=\"https://unpkg.com/justlovesmile-post@1.0.7/20220306130142.png\"><br>针对小目标检测，作者认为<strong>特征融合</strong>和<strong>有效采样</strong>是关键。对于anchor-based来说，anchor的铺设方式直接影响正样本采样率。经典的anchor铺设方式和特征图的分辩率有关，也就是anchor铺设的步长（C2-C5上的anchor步长分别是4,8,16,32）。随着网络加加深，特征图分辨率下降，anchor的步长扩大，常常会导致小目标的采样丢失，如下图所示：<br><img src=\"https://unpkg.com/justlovesmile-post@1.0.7/20220306125943.png\"><br>文章通过resize的方式选取了一个合适的特征图分别率，尽可能保证小目标都被采样到，再加上简单的特征融合保证丰富的语义信息和位置信息。在这里之所以不使用C2，是因为遥感目标检测会设置较多的尺度和比例，那么在C2这个特征图上面的anchor就变得太多了，而且在遥感数据集中最小的目标一般也都在10像素以上（特指<a href=\"https://arxiv.org/abs/1807.02700\">DOTA1.0</a>，<a href=\"https://captain-whu.github.io/DOAI2019/dataset.html\">DOTA1.5</a>则给出了像素10以下的标注）。</p>\n<h2 id=\"6-2-MAD-Net\"><a href=\"#6-2-MAD-Net\" class=\"headerlink\" title=\"6.2 MAD-Net\"></a>6.2 MAD-Net</h2><p>由于遥感图像背景的复杂性，RPN产生的建议区域可能引入大量噪声信息，如下图所示:<br><img src=\"https://unpkg.com/justlovesmile-post@1.0.7/20220306130354.png\"><br>过多的噪音可能会混淆物体信息，物体之间的界限将变得模糊，导致漏检并增加虚警。因此，有必要增强物体特征并削弱非物体特征。为了更有效地捕捉复杂背景下小物体的特征，文章设计了一种有监督的多维注意力网络（MDA-Net），如下图所示。具体来说，在基于像素的注意网络中，特征图F3通过具有不同大小卷积核进行卷积运算，学习得到双通道的显著图（参见上图d）。这个显著图显示了前景和背景的分数。选择显著图中的一个通道与F3相乘，得到新的信息特征图A3（参见上图c）。需要注意的是，Softmax函数之后的显着图的值在[0,1]之间。换句话说，它可以降低噪声并相对的增强对象信息。由于显著图是连续的，因此不会完全消除背景信息，这有利于保留某些上下文信息并提高鲁棒性。<br><img src=\"https://unpkg.com/justlovesmile-post@1.0.7/20220306130420.png\"><br>其实这个模块现在也是被用的比较烂了，就是空间注意力加通道注意力的组合。但在实际的应用过程中，空间注意力在遥感检测真的是非常有用的；</p>\n<h2 id=\"6-3-IoU-Smooth-L1-Loss\"><a href=\"#6-3-IoU-Smooth-L1-Loss\" class=\"headerlink\" title=\"6.3 IoU-Smooth L1 Loss\"></a>6.3 IoU-Smooth L1 Loss</h2><p>首先我们要先了解一下两种旋转边界框的两种常见的方式，下图来自这篇文章的作者yangxue：<br><img src=\"https://unpkg.com/justlovesmile-post@1.0.7/20220306130754.png\"><br>SCRDet是采用的opencv 表示法。在当前常用的旋转检测框的角度定义下，由于存在旋转角度的边界问题，会产生不必要的损失，如下图所示：<br><img src=\"https://unpkg.com/justlovesmile-post@1.0.7/20220306130911.png\"></p>\n<p>最理想的角度回归路线是由蓝色框逆时针旋转到红色框，但由于角度的周期性，导致按照这个回归方式的损失非常大（参见上图右边的Example）。此时模型必须以更复杂的形式回归（例如蓝色框顺时针旋转，同时缩放w和h），增加了回归的难度。为了更好地解决这个问题，我们在传统的smooth L1 损失函数中引入了IoU常数因子。在边界情况下，新的损失函数近似等于0，消除了损失的突增。新的回归损失可分为两部分，smooth L1回归损失函数取单位向量确定梯度传播的方向，而IoU表示梯度的大小，这样loss函数就变得连续。此外，使用IoU优化回归任务与评估方法的度量标准保持一致，这比坐标回归更直接和有效。IoU-Smooth L1 loss公式如下： </p>\n<p><img src=\"https://unpkg.com/justlovesmile-post@1.0.7/20220306131108.png\"></p>\n<p>可以看一下两种loss在边界情况下的效果对比：<br><img src=\"https://unpkg.com/justlovesmile-post@1.0.7/20220306131310.png\"></p>\n<p>导致这种原因的根本原因是角度的预测超出了所定义范围。其实解决这种问题的方法并不唯一，<a href=\"https://arxiv.org/abs/1703.01086\">RRPN</a>和<a href=\"https://www.mdpi.com/2072-4292/10/1/132\">R-DFPN</a>在论文的loss公式中就判断了是不是在定义范围内，通过加减$k\\pi$来缓解这个问题，但这种做法明显不优美而且仍然存在问题，主要是较难判断超出预测范围几个角度周期。当然可以通过对角度部分的loss加一个周期性函数，比如tan、cos等三角函数来做，但是我在实际使用过程中常常出现不收敛的情况。对于边界问题，我其实还做了其他方法的研究，会在以后的文章中详细讨论。</p>\n<h1 id=\"7-实验结果\"><a href=\"#7-实验结果\" class=\"headerlink\" title=\"7. 实验结果\"></a>7. 实验结果</h1><p>消融实验：<br><img src=\"https://unpkg.com/justlovesmile-post@1.0.7/20220306131425.png\"></p>\n<p>对比实验：<br><img src=\"https://unpkg.com/justlovesmile-post@1.0.7/20220306131447.png\"></p>\n<h1 id=\"8-参考文献\"><a href=\"#8-参考文献\" class=\"headerlink\" title=\"8. 参考文献\"></a>8. 参考文献</h1><p><a href=\"https://zhuanlan.zhihu.com/p/107400817\">旋转目标检测方法解读 （SCRDet, ICCV2019） - 知乎 (zhihu.com)</a></p>\n","categories":[{"name":"人工智能","path":"api/categories/人工智能.json"}],"tags":[{"name":"深度学习","path":"api/tags/深度学习.json"},{"name":"论文笔记","path":"api/tags/论文笔记.json"},{"name":"目标检测","path":"api/tags/目标检测.json"}]}