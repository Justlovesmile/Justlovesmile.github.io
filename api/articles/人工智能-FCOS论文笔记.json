{"title":"目标检测 | FCOS，经典单阶段Anchor-Free目标检测模型","slug":"人工智能-FCOS论文笔记","date":"2022-03-26T05:48:39.000Z","updated":"2022-03-26T05:48:39.000Z","comments":true,"path":"api/articles/人工智能-FCOS论文笔记.json","excerpt":null,"covers":["https://unpkg.com/justlovesmile-img/20211119114214.png","https://unpkg.com/justlovesmile-img/image-20211119160053912.png","https://unpkg.com/justlovesmile-img/image-20211119155237711.png","https://unpkg.com/justlovesmile-img/image-20211119173209315.png","https://unpkg.com/justlovesmile-img/image-20211119175758242.png","https://unpkg.com/justlovesmile-img/image-20211119180027086.png","https://unpkg.com/justlovesmile-img/image-20211119180536280.png","https://unpkg.com/justlovesmile-img/20220326134621.png","https://unpkg.com/justlovesmile-img/20220326134721.png"],"content":"<h1 id=\"1-论文信息\"><a href=\"#1-论文信息\" class=\"headerlink\" title=\"1. 论文信息\"></a>1. 论文信息</h1><p>论文标题：<code> 《FCOS: Fully Convolutional One-Stage Object Detection》</code></p>\n<blockquote>\n<p>论文来源：ICCV2019<br>论文链接：<a href=\"https://arxiv.org/abs/1904.01355\">https://arxiv.org/abs/1904.01355</a><br>论文代码：<a href=\"https://github.com/tianzhi0549/FCOS/\">https://github.com/tianzhi0549/FCOS/</a></p>\n</blockquote>\n<p><img src=\"https://unpkg.com/justlovesmile-img/20211119114214.png\" alt=\"20211119114214\"></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@inproceedings&#123;tian2019fcos,</span><br><span class=\"line\">  title=&#123;Fcos: Fully convolutional one-stage object detection&#125;,</span><br><span class=\"line\">  author=&#123;Tian, Zhi and Shen, Chunhua and Chen, Hao and He, Tong&#125;,</span><br><span class=\"line\">  booktitle=&#123;Proceedings of the IEEE/CVF international conference on computer vision&#125;,</span><br><span class=\"line\">  pages=&#123;9627--9636&#125;,</span><br><span class=\"line\">  year=&#123;2019&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"2-归纳总结\"><a href=\"#2-归纳总结\" class=\"headerlink\" title=\"2. 归纳总结\"></a>2. 归纳总结</h1><table>\n<thead>\n<tr>\n<th>标签</th>\n<th>目的</th>\n<th>方法</th>\n<th>总结</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>#Anchor-Free #单阶段</td>\n<td>解决Anchor-Base算法超参数设置复杂，计算量大的问题</td>\n<td>FCN，Center-ness</td>\n<td>Anchor-Free经典算法</td>\n</tr>\n</tbody></table>\n<h1 id=\"3-主要工作\"><a href=\"#3-主要工作\" class=\"headerlink\" title=\"3. 主要工作\"></a>3. 主要工作</h1><p>FCOS是一种基于全卷积的单阶段目标检测算法，并且是一种Anchor box free的算法。其实现了无Anchor，无Proposal，并且提出了Center-ness的思想，极大的提升了Anchor-Free目标检测算法的性能。</p>\n<p>Anchor free的好处是：</p>\n<ul>\n<li>避免了Anchor Box带来的复杂计算，如计算重合度IoU；</li>\n<li>避免了Anchor Box相关的超参数设置，其对性能影响较大；</li>\n</ul>\n<p>因此，FCOS的优点是：</p>\n<ul>\n<li>其可以和其他使用FCN结构的任务相统一，方便其他任务方法之间的re-use</li>\n<li>proposal free和anchor free，减少了超参数数量，更简单</li>\n<li>减少了计算复杂度，如IoU计算</li>\n<li>FCOS在单阶段算法中性能不错，并且证明了FCOS替换两阶段算法里的RPNs也可以取得更好的性能</li>\n<li>适用于各种instance-wise的预测问题</li>\n</ul>\n<h2 id=\"3-1-模型结构\"><a href=\"#3-1-模型结构\" class=\"headerlink\" title=\"3.1 模型结构\"></a>3.1 模型结构</h2><p>模型结构如下图：</p>\n<p><img src=\"https://unpkg.com/justlovesmile-img/image-20211119160053912.png\" alt=\"image-20211119160053912\"></p>\n<p>FCOS包含三个大模块：</p>\n<ul>\n<li>Backbone：提取图像特征，如结构图左侧所示，其中特征图尺寸逐层减半，如左侧$H×W$所示，$s=\\frac{W^*}{W}$代表步长。对于坐标为$(x,y)$的位置，其映射回原图为$(\\lfloor\\frac{s}{2}\\rfloor + xs,\\lfloor\\frac{s}{2}\\rfloor+ys)$;</li>\n<li>FPN：多层级预测，提高检测器对不同尺寸目标的检测性能；与Anchor Based不同的是，FCOS通过限制不同层级边界框回归范围来分配层级</li>\n<li>Classification+Center-ness+Regression Head</li>\n</ul>\n<p><img src=\"https://unpkg.com/justlovesmile-img/image-20211119155237711.png\" alt=\"image-20211119155237711\"></p>\n<p>对于FCOS，其直接将每个位置$(x,y)$视为训练样本，其需要回归的值为一个4维向量$t=(l,t,r,b)$，如上图所示。<br>由于一张图片中的目标数量有限，所以导致基于Anchor的算法会产生更多的负样本，因此FCOS对于每个像素点只回归一组值（可以理解为Anchor数量为1），可以利用更多的前景（正）样本信息去训练。如果坐标落在任何ground-truth box中即为正样本，且该位置的类别为这个gt box的类别$c^{ * }$ ，否则为负样本（即背景，类别为0），如果落在多个gt box中，则认为其是一个歧义样本（ambiguous sample），针对这种情况，可通过FPN解决。计算 $(l^*, t^*, r^*, b^*)$，$m_i$为每个特征层最大距离（论文里作者设置$m_2$ ~ $m_7$分别为0，64，128，256，512，$\\infty$），如果$\\max(l^*, t^*, r^*, b^*) &gt; m _ i$ 或者 $\\max(l^*, t^*, r^*, b^*) &lt; m _ {i−1}$,则此位置为负样本，不进行计算；对于大小相近又存在重叠的gt box，FPN无法区别，则选择面积最小的gt box为回归目标。</p>\n<h2 id=\"3-2-正负样本定义\"><a href=\"#3-2-正负样本定义\" class=\"headerlink\" title=\"3.2 正负样本定义\"></a>3.2 正负样本定义</h2><p>一个目标检测算法性能的优异性，最大影响因素就是<strong>如何定义正负样本</strong>。而FCOS的定义方式非常通俗易懂。主要分为两步：<br><strong>(1) 设置regress_ranges=((-1, 64), (64, 128), (128, 256), (256, 512),(512, INF)，用于将不同大小的bbox分配到不同的FPN层进行预测即距离4条边的最大值在给定范围内</strong><br><strong>(2) 设置center_sampling_ratio=1.5,用于确定对于任意一个输出层距离bbox中心多远的区域属于正样本（基于gt bbox中心点进行扩展出正方形，扩展范围是center_sample_radius×stride，正方形区域就当做新的gt bbox），该值越大，扩张比例越大，选择正样本区域越大；（细节：如果扩展比例过大，导致中心采样区域超过了gt bbox本身范围了，此时需要截断操作）</strong></p>\n<h2 id=\"3-3-损失函数\"><a href=\"#3-3-损失函数\" class=\"headerlink\" title=\"3.3 损失函数\"></a>3.3 损失函数</h2><p>FCOS的损失函数为：</p>\n<p><img src=\"https://unpkg.com/justlovesmile-img/image-20211119173209315.png\" alt=\"image-20211119173209315\"></p>\n<p>其中$L_{cls}$是focal loss，$L_{reg}$是IoU loss，$N_{pos}$代表正样本数量，$\\lambda$用于平衡$L_{reg}$的权重；$\\mathbb{1} _ {c^{ * }<em>{i}}$当$c^{ * }</em>{i}&gt;0$时为1，否则为0；</p>\n<p>为了减少低质量检测框，减少误检，FCOS增加了一个一层的分支，用于预测Center-ness，其描绘了位置到目标中心的归一化距离，下图展示了使用Center-ness（左）和不使用Center-ness（右）的区别。</p>\n<p><img src=\"https://unpkg.com/justlovesmile-img/image-20211119175758242.png\" alt=\"image-20211119175758242\"></p>\n<p>Center-ness的计算公式如下，其范围为0-1，训练阶段使用BCE Loss并和之前的损失函数相加，测试阶段用于加权预测得分：</p>\n<p><img src=\"https://unpkg.com/justlovesmile-img/image-20211119180027086.png\" alt=\"image-20211119180027086\"></p>\n<h1 id=\"4-实验结果\"><a href=\"#4-实验结果\" class=\"headerlink\" title=\"4. 实验结果\"></a>4. 实验结果</h1><p>对比实验结果：</p>\n<p><img src=\"https://unpkg.com/justlovesmile-img/image-20211119180536280.png\" alt=\"image-20211119180536280\"></p>\n<p>有无Center-ness分支的消融实验：</p>\n<p><img src=\"https://unpkg.com/justlovesmile-img/20220326134621.png\"></p>\n<p>替换RPN的消融实验：</p>\n<p><img src=\"https://unpkg.com/justlovesmile-img/20220326134721.png\"></p>\n<h1 id=\"5-参考文献\"><a href=\"#5-参考文献\" class=\"headerlink\" title=\"5. 参考文献\"></a>5. 参考文献</h1><p><a href=\"https://zhuanlan.zhihu.com/p/267346645\">mmdetection最小复刻版(六)：FCOS深入可视化分析 - 知乎 (zhihu.com)</a></p>\n","more":"<h1 id=\"1-论文信息\"><a href=\"#1-论文信息\" class=\"headerlink\" title=\"1. 论文信息\"></a>1. 论文信息</h1><p>论文标题：<code> 《FCOS: Fully Convolutional One-Stage Object Detection》</code></p>\n<blockquote>\n<p>论文来源：ICCV2019<br>论文链接：<a href=\"https://arxiv.org/abs/1904.01355\">https://arxiv.org/abs/1904.01355</a><br>论文代码：<a href=\"https://github.com/tianzhi0549/FCOS/\">https://github.com/tianzhi0549/FCOS/</a></p>\n</blockquote>\n<p><img src=\"https://unpkg.com/justlovesmile-img/20211119114214.png\" alt=\"20211119114214\"></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@inproceedings&#123;tian2019fcos,</span><br><span class=\"line\">  title=&#123;Fcos: Fully convolutional one-stage object detection&#125;,</span><br><span class=\"line\">  author=&#123;Tian, Zhi and Shen, Chunhua and Chen, Hao and He, Tong&#125;,</span><br><span class=\"line\">  booktitle=&#123;Proceedings of the IEEE/CVF international conference on computer vision&#125;,</span><br><span class=\"line\">  pages=&#123;9627--9636&#125;,</span><br><span class=\"line\">  year=&#123;2019&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"2-归纳总结\"><a href=\"#2-归纳总结\" class=\"headerlink\" title=\"2. 归纳总结\"></a>2. 归纳总结</h1><table>\n<thead>\n<tr>\n<th>标签</th>\n<th>目的</th>\n<th>方法</th>\n<th>总结</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>#Anchor-Free #单阶段</td>\n<td>解决Anchor-Base算法超参数设置复杂，计算量大的问题</td>\n<td>FCN，Center-ness</td>\n<td>Anchor-Free经典算法</td>\n</tr>\n</tbody></table>\n<h1 id=\"3-主要工作\"><a href=\"#3-主要工作\" class=\"headerlink\" title=\"3. 主要工作\"></a>3. 主要工作</h1><p>FCOS是一种基于全卷积的单阶段目标检测算法，并且是一种Anchor box free的算法。其实现了无Anchor，无Proposal，并且提出了Center-ness的思想，极大的提升了Anchor-Free目标检测算法的性能。</p>\n<p>Anchor free的好处是：</p>\n<ul>\n<li>避免了Anchor Box带来的复杂计算，如计算重合度IoU；</li>\n<li>避免了Anchor Box相关的超参数设置，其对性能影响较大；</li>\n</ul>\n<p>因此，FCOS的优点是：</p>\n<ul>\n<li>其可以和其他使用FCN结构的任务相统一，方便其他任务方法之间的re-use</li>\n<li>proposal free和anchor free，减少了超参数数量，更简单</li>\n<li>减少了计算复杂度，如IoU计算</li>\n<li>FCOS在单阶段算法中性能不错，并且证明了FCOS替换两阶段算法里的RPNs也可以取得更好的性能</li>\n<li>适用于各种instance-wise的预测问题</li>\n</ul>\n<h2 id=\"3-1-模型结构\"><a href=\"#3-1-模型结构\" class=\"headerlink\" title=\"3.1 模型结构\"></a>3.1 模型结构</h2><p>模型结构如下图：</p>\n<p><img src=\"https://unpkg.com/justlovesmile-img/image-20211119160053912.png\" alt=\"image-20211119160053912\"></p>\n<p>FCOS包含三个大模块：</p>\n<ul>\n<li>Backbone：提取图像特征，如结构图左侧所示，其中特征图尺寸逐层减半，如左侧$H×W$所示，$s=\\frac{W^*}{W}$代表步长。对于坐标为$(x,y)$的位置，其映射回原图为$(\\lfloor\\frac{s}{2}\\rfloor + xs,\\lfloor\\frac{s}{2}\\rfloor+ys)$;</li>\n<li>FPN：多层级预测，提高检测器对不同尺寸目标的检测性能；与Anchor Based不同的是，FCOS通过限制不同层级边界框回归范围来分配层级</li>\n<li>Classification+Center-ness+Regression Head</li>\n</ul>\n<p><img src=\"https://unpkg.com/justlovesmile-img/image-20211119155237711.png\" alt=\"image-20211119155237711\"></p>\n<p>对于FCOS，其直接将每个位置$(x,y)$视为训练样本，其需要回归的值为一个4维向量$t=(l,t,r,b)$，如上图所示。<br>由于一张图片中的目标数量有限，所以导致基于Anchor的算法会产生更多的负样本，因此FCOS对于每个像素点只回归一组值（可以理解为Anchor数量为1），可以利用更多的前景（正）样本信息去训练。如果坐标落在任何ground-truth box中即为正样本，且该位置的类别为这个gt box的类别$c^{ * }$ ，否则为负样本（即背景，类别为0），如果落在多个gt box中，则认为其是一个歧义样本（ambiguous sample），针对这种情况，可通过FPN解决。计算 $(l^*, t^*, r^*, b^*)$，$m_i$为每个特征层最大距离（论文里作者设置$m_2$ ~ $m_7$分别为0，64，128，256，512，$\\infty$），如果$\\max(l^*, t^*, r^*, b^*) &gt; m _ i$ 或者 $\\max(l^*, t^*, r^*, b^*) &lt; m _ {i−1}$,则此位置为负样本，不进行计算；对于大小相近又存在重叠的gt box，FPN无法区别，则选择面积最小的gt box为回归目标。</p>\n<h2 id=\"3-2-正负样本定义\"><a href=\"#3-2-正负样本定义\" class=\"headerlink\" title=\"3.2 正负样本定义\"></a>3.2 正负样本定义</h2><p>一个目标检测算法性能的优异性，最大影响因素就是<strong>如何定义正负样本</strong>。而FCOS的定义方式非常通俗易懂。主要分为两步：<br><strong>(1) 设置regress_ranges=((-1, 64), (64, 128), (128, 256), (256, 512),(512, INF)，用于将不同大小的bbox分配到不同的FPN层进行预测即距离4条边的最大值在给定范围内</strong><br><strong>(2) 设置center_sampling_ratio=1.5,用于确定对于任意一个输出层距离bbox中心多远的区域属于正样本（基于gt bbox中心点进行扩展出正方形，扩展范围是center_sample_radius×stride，正方形区域就当做新的gt bbox），该值越大，扩张比例越大，选择正样本区域越大；（细节：如果扩展比例过大，导致中心采样区域超过了gt bbox本身范围了，此时需要截断操作）</strong></p>\n<h2 id=\"3-3-损失函数\"><a href=\"#3-3-损失函数\" class=\"headerlink\" title=\"3.3 损失函数\"></a>3.3 损失函数</h2><p>FCOS的损失函数为：</p>\n<p><img src=\"https://unpkg.com/justlovesmile-img/image-20211119173209315.png\" alt=\"image-20211119173209315\"></p>\n<p>其中$L_{cls}$是focal loss，$L_{reg}$是IoU loss，$N_{pos}$代表正样本数量，$\\lambda$用于平衡$L_{reg}$的权重；$\\mathbb{1} _ {c^{ * }<em>{i}}$当$c^{ * }</em>{i}&gt;0$时为1，否则为0；</p>\n<p>为了减少低质量检测框，减少误检，FCOS增加了一个一层的分支，用于预测Center-ness，其描绘了位置到目标中心的归一化距离，下图展示了使用Center-ness（左）和不使用Center-ness（右）的区别。</p>\n<p><img src=\"https://unpkg.com/justlovesmile-img/image-20211119175758242.png\" alt=\"image-20211119175758242\"></p>\n<p>Center-ness的计算公式如下，其范围为0-1，训练阶段使用BCE Loss并和之前的损失函数相加，测试阶段用于加权预测得分：</p>\n<p><img src=\"https://unpkg.com/justlovesmile-img/image-20211119180027086.png\" alt=\"image-20211119180027086\"></p>\n<h1 id=\"4-实验结果\"><a href=\"#4-实验结果\" class=\"headerlink\" title=\"4. 实验结果\"></a>4. 实验结果</h1><p>对比实验结果：</p>\n<p><img src=\"https://unpkg.com/justlovesmile-img/image-20211119180536280.png\" alt=\"image-20211119180536280\"></p>\n<p>有无Center-ness分支的消融实验：</p>\n<p><img src=\"https://unpkg.com/justlovesmile-img/20220326134621.png\"></p>\n<p>替换RPN的消融实验：</p>\n<p><img src=\"https://unpkg.com/justlovesmile-img/20220326134721.png\"></p>\n<h1 id=\"5-参考文献\"><a href=\"#5-参考文献\" class=\"headerlink\" title=\"5. 参考文献\"></a>5. 参考文献</h1><p><a href=\"https://zhuanlan.zhihu.com/p/267346645\">mmdetection最小复刻版(六)：FCOS深入可视化分析 - 知乎 (zhihu.com)</a></p>\n","categories":[{"name":"人工智能","path":"api/categories/人工智能.json"}],"tags":[{"name":"深度学习","path":"api/tags/深度学习.json"},{"name":"目标检测","path":"api/tags/目标检测.json"},{"name":"论文笔记","path":"api/tags/论文笔记.json"}]}