{"title":"深度学习 | GAN，什么是生成对抗网络","slug":"人工智能-GAN学习笔记","date":"2021-03-03T02:43:37.000Z","updated":"2021-03-03T02:43:37.000Z","comments":true,"path":"api/articles/人工智能-GAN学习笔记.json","excerpt":null,"covers":["https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210223180633.png","https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210224172221.png","https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210224183510.png","https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210224183753.png","https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210225180739.png","https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210226103604.jpg","https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210303105435.png"],"content":"<h1 id=\"GAN学习笔记\"><a href=\"#GAN学习笔记\" class=\"headerlink\" title=\"GAN学习笔记\"></a>GAN学习笔记</h1><h2 id=\"1-GAN原理\"><a href=\"#1-GAN原理\" class=\"headerlink\" title=\"1. GAN原理\"></a>1. GAN原理</h2><p>论文链接：<a href=\"https://arxiv.org/abs/1406.2661\">Generative Adversarial Networks</a></p>\n<blockquote>\n<p>生成式对抗网络(GAN, Generative Adversarial Networks)是一种深度学习模型，是近年来复杂分布上无监督学习最具前景的方法之一。模型通过框架中（至少）两个模块：生成模型（Generative Model）和判别模型（Discriminative Model）的互相博弈学习产生相当好的输出。原始 GAN 理论中，并不要求 G 和 D 都是神经网络，只需要是能拟合相应生成和判别的函数即可。但实用中一般均使用深度神经网络作为 G 和 D 。一个优秀的GAN应用需要有良好的训练方法，否则可能由于神经网络模型的自由性而导致输出不理想。<br>Ian J. Goodfellow等人于2014年10月在<a href=\"https://arxiv.org/abs/1406.2661\">Generative Adversarial Networks</a>中提出了一个通过对抗过程估计生成模型的新框架。框架中同时训练两个模型：捕获数据分布的生成模型G，和估计样本来自训练数据的概率的判别模型D。G的训练程序是将D错误的概率最大化。这个框架对应一个最大值集下限的双方对抗游戏。可以证明在任意函数G和D的空间中，存在唯一的解决方案，使得G重现训练数据分布，而D=0.5。在G和D由多层感知器定义的情况下，整个系统可以用反向传播进行训练。在训练或生成样本期间，不需要任何马尔科夫链或展开的近似推理网络。实验通过对生成的样品的定性和定量评估证明了本框架的潜力。<br>  —- 摘自<a href=\"https://baike.baidu.com/item/Gan/22181905?fr=aladdin\">百度百科</a></p>\n</blockquote>\n<p>GAN是由两部分组成的，第一部分是生成，第二部分是对抗。简单来说，就是有一个生成网络G和一个判别网络D，通过训练让两个网络相互竞争，生成网络G接受一个随机噪声z来生成假的数据G(z)，对抗网络D通过判别器去判别真伪概率，最后希望生成器G生成的数据能够以假乱真。在最理想的状态下，D(G(z)) = 0.5。</p>\n<p>以上原理的数学公式为：</p>\n<p>$$ min_{G}max_{D}V(D,G) = \\mathbb{E} _ {x \\sim p_{data}(x)} [\\log D(x)] + \\mathbb{E} _ {z \\sim p_{z}(z) [\\log (1-D(G(z)))]} $$</p>\n<p>式子中，x表示真实数据，z表示噪声，G(z)表示G网络根据z生成的数据，D(x)表示D网络判断真实数据是否为真的概率，因此D(x)接近1越好。而D(G(z))代表D网络判断G网络生成的虚假数据是真实的概率。<br>因此，对于D网络(辨别器)：</p>\n<ul>\n<li>如果x来自$P_{data}$，那么D(x)要越大越好，可以用$\\log(D(x)) \\uparrow$表示。</li>\n<li>如果x来自于$P_{generator}$，那么D(G(z))越小越好，进而表示为$\\log[1−D(G(z))] \\uparrow$。</li>\n<li>因此需要最大化$max_D$<br>对于G网络(生成器)：</li>\n<li>$D(G(z))$越大越好，进而表示为log[1−D(G(z))]↓</li>\n<li>因此需要最小化$min_{G}$。</li>\n</ul>\n<p><img src=\"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210223180633.png\"></p>\n<p>第一步我们训练D，D是希望V(D,G)越大越好，所以是加上梯度(ascending)。第二步训练G时，V(D,G)越小越好，所以是减去梯度(descending)。整个训练过程交替进行。</p>\n<h2 id=\"2-GAN实例\"><a href=\"#2-GAN实例\" class=\"headerlink\" title=\"2. GAN实例\"></a>2. GAN实例</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn,optim</span><br><span class=\"line\"><span class=\"keyword\">import</span> torchvision.transforms <span class=\"keyword\">as</span> tfs</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> PIL <span class=\"keyword\">import</span> Image</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> tqdm.auto <span class=\"keyword\">import</span> tqdm</span><br><span class=\"line\"></span><br><span class=\"line\">transforms = tfs.Compose([</span><br><span class=\"line\">    tfs.Resize((<span class=\"number\">32</span>,<span class=\"number\">32</span>)),</span><br><span class=\"line\">    tfs.ToTensor(),</span><br><span class=\"line\">    <span class=\"comment\">#tfs.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])</span></span><br><span class=\"line\">])</span><br><span class=\"line\"></span><br><span class=\"line\">flat_img = <span class=\"number\">32</span>*<span class=\"number\">32</span>*<span class=\"number\">3</span></span><br><span class=\"line\">noise_dim = <span class=\"number\">100</span></span><br><span class=\"line\"></span><br><span class=\"line\">img = Image.<span class=\"built_in\">open</span>(<span class=\"string\">&#x27;1.jpg&#x27;</span>)</span><br><span class=\"line\">real_img = transforms(img)</span><br><span class=\"line\"></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">2</span>)</span><br><span class=\"line\">fake_img = torch.rand(<span class=\"number\">1</span>,noise_dim)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.imshow(np.transpose(real_img.numpy(),(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">0</span>)))</span><br><span class=\"line\"><span class=\"comment\">#print(real_img)</span></span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210224172221.png\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Discriminator</span>(<span class=\"params\">nn.Module</span>):</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self</span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        self.linear = nn.Sequential(</span><br><span class=\"line\">            nn.Linear(flat_img, <span class=\"number\">1024</span>),</span><br><span class=\"line\">            nn.ReLU(),</span><br><span class=\"line\">            nn.Linear(<span class=\"number\">1024</span>, <span class=\"number\">2048</span>),</span><br><span class=\"line\">            nn.ReLU(),</span><br><span class=\"line\">            nn.Linear(<span class=\"number\">2048</span>, <span class=\"number\">1</span>),</span><br><span class=\"line\">            nn.Sigmoid() <span class=\"comment\">#sigmoid常用于二分类问题</span></span><br><span class=\"line\">        )</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, img</span>):</span></span><br><span class=\"line\">        img = img.view(<span class=\"number\">1</span>, -<span class=\"number\">1</span>)</span><br><span class=\"line\">        out = self.linear(img)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> out</span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Generator</span>(<span class=\"params\">nn.Module</span>):</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self</span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        self.linear = nn.Sequential(</span><br><span class=\"line\">            nn.Linear(noise_dim, <span class=\"number\">1024</span>),</span><br><span class=\"line\">            nn.LeakyReLU(),</span><br><span class=\"line\">            nn.Linear(<span class=\"number\">1024</span>, <span class=\"number\">2048</span>),</span><br><span class=\"line\">            nn.LeakyReLU(),</span><br><span class=\"line\">            nn.Linear(<span class=\"number\">2048</span>, flat_img)</span><br><span class=\"line\">        )</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, latent_space</span>):</span></span><br><span class=\"line\">        latent_space = latent_space.view(<span class=\"number\">1</span>, -<span class=\"number\">1</span>)</span><br><span class=\"line\">        out = self.linear(latent_space)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> out</span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">device = <span class=\"string\">&#x27;cuda:0&#x27;</span> <span class=\"keyword\">if</span> torch.cuda.is_available() <span class=\"keyword\">else</span> <span class=\"string\">&#x27;cpu&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\">discr = Discriminator().to(device)</span><br><span class=\"line\">gen = Generator().to(device)</span><br><span class=\"line\"></span><br><span class=\"line\">opt_d = optim.SGD(discr.parameters(), lr=<span class=\"number\">0.001</span>, momentum=<span class=\"number\">0.9</span>)</span><br><span class=\"line\">opt_g = optim.SGD(gen.parameters(), lr=<span class=\"number\">0.001</span>, momentum=<span class=\"number\">0.9</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">criterion = nn.BCELoss()</span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">epochs = <span class=\"number\">200</span></span><br><span class=\"line\">discr_e = <span class=\"number\">4</span></span><br><span class=\"line\">gen_e = <span class=\"number\">4</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#whole model training starts here</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(epochs):</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">#discriminator training</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(discr_e):</span><br><span class=\"line\">        out_d1 = discr(real_img.to(device))</span><br><span class=\"line\">        <span class=\"comment\">#loss for real image</span></span><br><span class=\"line\">        loss_d1 = criterion(out_d1, torch.ones((<span class=\"number\">1</span>, <span class=\"number\">1</span>)).to(device))</span><br><span class=\"line\"></span><br><span class=\"line\">        out_d2 = gen(fake_img.to(device)).detach()</span><br><span class=\"line\">        <span class=\"comment\">#loss for fake image</span></span><br><span class=\"line\">        loss_d2 = criterion(discr(out_d2.to(device)), torch.zeros((<span class=\"number\">1</span>, <span class=\"number\">1</span>)).to(device))</span><br><span class=\"line\"></span><br><span class=\"line\">        opt_d.zero_grad()</span><br><span class=\"line\">        loss_d = loss_d1+loss_d2</span><br><span class=\"line\">        loss_d.backward()</span><br><span class=\"line\">        opt_d.step()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">#generator training</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(gen_e):</span><br><span class=\"line\">        out_g = gen(fake_img.to(device))</span><br><span class=\"line\">        <span class=\"comment\">#Binary cross entropy loss</span></span><br><span class=\"line\">        loss_g = criterion(discr(out_g.to(device)), torch.ones(<span class=\"number\">1</span>, <span class=\"number\">1</span>).to(device))</span><br><span class=\"line\">        <span class=\"comment\">#Loss function in the GAN paper</span></span><br><span class=\"line\">        <span class=\"comment\">#[log(1 - D(G(z)))]</span></span><br><span class=\"line\">        <span class=\"comment\">#loss_g = torch.log(torch.ones(1, 1).to(device) - (discr(out_g.to(device))))</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        opt_g.zero_grad()</span><br><span class=\"line\">        loss_g.backward()</span><br><span class=\"line\">        opt_g.step()</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">if</span> (epoch+<span class=\"number\">1</span>)%<span class=\"number\">10</span>==<span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Epoch[&#123;&#125;/&#123;&#125;],d_loss:&#123;:.6f&#125;,g_loss:&#123;:.6f&#125;&#x27;</span>.<span class=\"built_in\">format</span>(epoch+<span class=\"number\">1</span>,epochs,loss_d.data.item(),loss_g.data.item()))</span><br><span class=\"line\"></span><br><span class=\"line\">out=gen(fake_img.to(device)).detach()</span><br><span class=\"line\">out_score=discr(out_g.to(device))</span><br><span class=\"line\">loss = criterion(out_score, torch.ones(<span class=\"number\">1</span>, <span class=\"number\">1</span>).to(device))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;score:&quot;</span>,out_score.item(),<span class=\"string\">&quot;loss:&quot;</span>,loss.item())</span><br><span class=\"line\"></span><br><span class=\"line\">out=out.reshape((<span class=\"number\">3</span>,<span class=\"number\">32</span>,<span class=\"number\">32</span>)).cpu()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#print(out)</span></span><br><span class=\"line\">plt.subplot(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>)</span><br><span class=\"line\">plt.title(<span class=\"string\">&#x27;fake&#x27;</span>)</span><br><span class=\"line\">plt.imshow(np.transpose(out.numpy(),(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">0</span>)))</span><br><span class=\"line\">plt.subplot(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">plt.title(<span class=\"string\">&#x27;real&#x27;</span>)</span><br><span class=\"line\">plt.imshow(np.transpose(real_img.numpy(),(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">0</span>)))</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210224183510.png\"></p>\n<h2 id=\"3-DCGAN原理\"><a href=\"#3-DCGAN原理\" class=\"headerlink\" title=\"3. DCGAN原理\"></a>3. DCGAN原理</h2><p><a href=\"https://arxiv.org/pdf/1511.06434.pdf\">https://arxiv.org/pdf/1511.06434.pdf</a></p>\n<p>DCGAN的原理和GAN是一样的。只不过DCGANs体系结构有所改变：</p>\n<ul>\n<li>使用指定步长的卷积层代替池化层</li>\n<li>在生成器和鉴别器中使用batch norm。</li>\n<li>移除全连接层，以实现更深层次的体系结构，减少参数。</li>\n<li>在生成器中使用ReLU激活，但输出使用Tanh。</li>\n<li>在鉴别器中使用LeakyReLU激活</li>\n</ul>\n<p><img src=\"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210224183753.png\"></p>\n<p>DCGAN中提到了网络的训练细节：</p>\n<ul>\n<li>使用Adam算法更新参数，betas=(0.5, 0.999)；</li>\n<li>batch size选为128；</li>\n<li>权重使用正太分布，均值为0，标准差为0.02；</li>\n<li>学习率0.0002。</li>\n</ul>\n<h2 id=\"4-DCGAN实例\"><a href=\"#4-DCGAN实例\" class=\"headerlink\" title=\"4. DCGAN实例\"></a>4. DCGAN实例</h2><p>生成动漫头像，数据集来自<a href=\"https://www.kaggle.com/soumikrakshit/anime-faces\">https://www.kaggle.com/soumikrakshit/anime-faces</a></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> imageio</span><br><span class=\"line\"><span class=\"keyword\">from</span> tqdm.auto <span class=\"keyword\">import</span> tqdm</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch,torchvision</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.data <span class=\"keyword\">import</span> Dataset, DataLoader</span><br><span class=\"line\"><span class=\"keyword\">from</span> torchvision <span class=\"keyword\">import</span> transforms, utils</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\">avatar_img_path = <span class=\"string\">&quot;E:/python/dataset/anime face/data&quot;</span></span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">trans = transforms.Compose([</span><br><span class=\"line\">    transforms.ToTensor(),</span><br><span class=\"line\">    transforms.Normalize((<span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>), (<span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>))</span><br><span class=\"line\">])</span><br><span class=\"line\"></span><br><span class=\"line\">noise_dim = <span class=\"number\">100</span></span><br><span class=\"line\">batch_size = <span class=\"number\">16</span></span><br><span class=\"line\">beta1=<span class=\"number\">0.5</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">#自定义数据集</span></span><br><span class=\"line\"><span class=\"string\">file_train=[]</span></span><br><span class=\"line\"><span class=\"string\">for image_name in tqdm(os.listdir(avatar_img_path)):</span></span><br><span class=\"line\"><span class=\"string\">    file_train.append(os.path.join(avatar_img_path,image_name))</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">def default_loader(path):</span></span><br><span class=\"line\"><span class=\"string\">    img = imageio.imread(path)</span></span><br><span class=\"line\"><span class=\"string\">    img = img/255</span></span><br><span class=\"line\"><span class=\"string\">    img = trans(img)</span></span><br><span class=\"line\"><span class=\"string\">    return img</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">class trainset(Dataset):</span></span><br><span class=\"line\"><span class=\"string\">    def __init__(self, loader=default_loader):</span></span><br><span class=\"line\"><span class=\"string\">    #定义好 image 的路径</span></span><br><span class=\"line\"><span class=\"string\">        self.images = file_train</span></span><br><span class=\"line\"><span class=\"string\">        self.target = 0</span></span><br><span class=\"line\"><span class=\"string\">        self.loader = loader</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    def __getitem__(self, index):</span></span><br><span class=\"line\"><span class=\"string\">        fn = self.images[index]</span></span><br><span class=\"line\"><span class=\"string\">        img = self.loader(fn)</span></span><br><span class=\"line\"><span class=\"string\">        target = self.target</span></span><br><span class=\"line\"><span class=\"string\">        return img,target</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    def __len__(self):</span></span><br><span class=\"line\"><span class=\"string\">        return len(self.images)</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\">img_dataset=torchvision.datasets.ImageFolder(<span class=\"string\">&quot;E:/python/dataset/anime face&quot;</span>, transform=trans)</span><br><span class=\"line\"><span class=\"comment\">#img_dataset=trainset()</span></span><br><span class=\"line\">img_dataloader=DataLoader(img_dataset,batch_size=batch_size,shuffle=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"comment\">#print(img_dataset)</span></span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Generator</span>(<span class=\"params\">nn.Module</span>):</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, z_dim</span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Generator,self).__init__()</span><br><span class=\"line\">        self.z_dim = z_dim</span><br><span class=\"line\">        self.generator = nn.Sequential(</span><br><span class=\"line\">            nn.ConvTranspose2d(self.z_dim,<span class=\"number\">512</span>,<span class=\"number\">4</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>,bias=<span class=\"literal\">False</span>),</span><br><span class=\"line\">            nn.BatchNorm2d(num_features=<span class=\"number\">512</span>),</span><br><span class=\"line\">            nn.ReLU(<span class=\"literal\">True</span>),</span><br><span class=\"line\">            nn.ConvTranspose2d(<span class=\"number\">512</span>,<span class=\"number\">256</span>,<span class=\"number\">4</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>,bias=<span class=\"literal\">False</span>),</span><br><span class=\"line\">            nn.BatchNorm2d(num_features=<span class=\"number\">256</span>),</span><br><span class=\"line\">            nn.ReLU(<span class=\"literal\">True</span>),</span><br><span class=\"line\">            nn.ConvTranspose2d(<span class=\"number\">256</span>,<span class=\"number\">128</span>,<span class=\"number\">4</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>,bias=<span class=\"literal\">False</span>),</span><br><span class=\"line\">            nn.BatchNorm2d(num_features=<span class=\"number\">128</span>),</span><br><span class=\"line\">            nn.ReLU(<span class=\"literal\">True</span>),</span><br><span class=\"line\">            nn.ConvTranspose2d(<span class=\"number\">128</span>,<span class=\"number\">64</span>,<span class=\"number\">4</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>,bias=<span class=\"literal\">False</span>),</span><br><span class=\"line\">            nn.BatchNorm2d(num_features=<span class=\"number\">64</span>),</span><br><span class=\"line\">            nn.ReLU(<span class=\"literal\">True</span>),</span><br><span class=\"line\">            nn.ConvTranspose2d(<span class=\"number\">64</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>,bias=<span class=\"literal\">False</span>),</span><br><span class=\"line\">            nn.Tanh()</span><br><span class=\"line\">        )</span><br><span class=\"line\">        self.weight_init()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">weight_init</span>(<span class=\"params\">self</span>):</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> m <span class=\"keyword\">in</span> self.generator.modules():</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(m, nn.ConvTranspose2d):</span><br><span class=\"line\">                nn.init.normal_(m.weight.data, <span class=\"number\">0</span>, <span class=\"number\">0.02</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">elif</span> <span class=\"built_in\">isinstance</span>(m, nn.BatchNorm2d):</span><br><span class=\"line\">                nn.init.normal_(m.weight.data, <span class=\"number\">0</span>, <span class=\"number\">0.02</span>)</span><br><span class=\"line\">                nn.init.constant_(m.bias.data, <span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, x</span>):</span></span><br><span class=\"line\">        out = self.generator(x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> out</span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Discriminator</span>(<span class=\"params\">nn.Module</span>):</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self</span>):</span></span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        initialize</span></span><br><span class=\"line\"><span class=\"string\">        </span></span><br><span class=\"line\"><span class=\"string\">        :param image_size: tuple (3, h, w)</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Discriminator,self).__init__()</span><br><span class=\"line\">        self.discriminator = nn.Sequential(</span><br><span class=\"line\">            nn.Conv2d(<span class=\"number\">3</span>,<span class=\"number\">64</span>,<span class=\"number\">4</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>,bias=<span class=\"literal\">False</span>),</span><br><span class=\"line\">            nn.BatchNorm2d(num_features=<span class=\"number\">64</span>),</span><br><span class=\"line\">            nn.LeakyReLU(<span class=\"number\">0.2</span>),</span><br><span class=\"line\">            nn.Conv2d(<span class=\"number\">64</span>,<span class=\"number\">128</span>,<span class=\"number\">4</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>,bias=<span class=\"literal\">False</span>),</span><br><span class=\"line\">            nn.BatchNorm2d(num_features=<span class=\"number\">128</span>),</span><br><span class=\"line\">            nn.LeakyReLU(<span class=\"number\">0.2</span>),</span><br><span class=\"line\">            nn.Conv2d(<span class=\"number\">128</span>,<span class=\"number\">256</span>,<span class=\"number\">4</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>,bias=<span class=\"literal\">False</span>),</span><br><span class=\"line\">            nn.BatchNorm2d(num_features=<span class=\"number\">256</span>),</span><br><span class=\"line\">            nn.LeakyReLU(<span class=\"number\">0.2</span>),</span><br><span class=\"line\">            nn.Conv2d(<span class=\"number\">256</span>,<span class=\"number\">512</span>,<span class=\"number\">4</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>,bias=<span class=\"literal\">False</span>),</span><br><span class=\"line\">            nn.BatchNorm2d(num_features=<span class=\"number\">512</span>),</span><br><span class=\"line\">            nn.LeakyReLU(<span class=\"number\">0.2</span>),</span><br><span class=\"line\">            nn.Conv2d(<span class=\"number\">512</span>,<span class=\"number\">1</span>,<span class=\"number\">4</span>,<span class=\"number\">2</span>,<span class=\"number\">0</span>,bias=<span class=\"literal\">False</span>),</span><br><span class=\"line\">            nn.Sigmoid()</span><br><span class=\"line\">        )</span><br><span class=\"line\">        self.weight_init()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">weight_init</span>(<span class=\"params\">self</span>):</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> m <span class=\"keyword\">in</span> self.discriminator.modules():</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(m, nn.ConvTranspose2d):</span><br><span class=\"line\">                nn.init.normal_(m.weight.data, <span class=\"number\">0</span>, <span class=\"number\">0.02</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">elif</span> <span class=\"built_in\">isinstance</span>(m, nn.BatchNorm2d):</span><br><span class=\"line\">                nn.init.normal_(m.weight.data, <span class=\"number\">0</span>, <span class=\"number\">0.02</span>)</span><br><span class=\"line\">                nn.init.constant_(m.bias.data, <span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, x</span>):</span></span><br><span class=\"line\">        out = self.discriminator(x)</span><br><span class=\"line\">        out = out.view(x.size(<span class=\"number\">0</span>), -<span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> out</span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">device = torch.device(<span class=\"string\">&quot;cuda:0&quot;</span> <span class=\"keyword\">if</span> torch.cuda.is_available <span class=\"keyword\">else</span> <span class=\"string\">&quot;cpu&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">generator = Generator(noise_dim).to(device)</span><br><span class=\"line\">discriminator = Discriminator().to(device)</span><br><span class=\"line\"></span><br><span class=\"line\">bce_loss = nn.BCELoss()</span><br><span class=\"line\"><span class=\"comment\">#optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(beta1, 0.999))</span></span><br><span class=\"line\">optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=<span class=\"number\">0.00005</span>, betas=(beta1, <span class=\"number\">0.999</span>))</span><br><span class=\"line\">optimizer_G = torch.optim.Adam(generator.parameters(), lr=<span class=\"number\">0.0002</span>, betas=(beta1, <span class=\"number\">0.999</span>))</span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">epochs=<span class=\"number\">20</span></span><br><span class=\"line\"></span><br><span class=\"line\">fixed_z=torch.randn(batch_size,noise_dim,<span class=\"number\">1</span>,<span class=\"number\">1</span>,device=device)</span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(epochs):</span><br><span class=\"line\">    <span class=\"keyword\">for</span> step,(image,_) <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(img_dataloader):</span><br><span class=\"line\">        batch_size=image.size(<span class=\"number\">0</span>)</span><br><span class=\"line\">        <span class=\"comment\">#=====训练辨别器====</span></span><br><span class=\"line\">        optimizer_D.zero_grad()</span><br><span class=\"line\">        <span class=\"comment\"># 计算判别器对真实样本给出为真的概率</span></span><br><span class=\"line\">        d_out_real = discriminator(image.<span class=\"built_in\">type</span>(torch.FloatTensor).to(device))</span><br><span class=\"line\">        real_loss = bce_loss(d_out_real, torch.ones(size=(batch_size, <span class=\"number\">1</span>)).to(device))</span><br><span class=\"line\">        real_scores = d_out_real</span><br><span class=\"line\">        <span class=\"comment\">#real_loss.backward()</span></span><br><span class=\"line\">        <span class=\"comment\"># 计算判别器对假样本给出为真的概率</span></span><br><span class=\"line\">        noise = torch.randn(batch_size,noise_dim,<span class=\"number\">1</span>,<span class=\"number\">1</span>,device=device)</span><br><span class=\"line\">        fake_img = generator(noise)</span><br><span class=\"line\">        d_out_fake = discriminator(fake_img.detach())</span><br><span class=\"line\">        fake_loss = bce_loss(d_out_fake, torch.zeros(size=(batch_size, <span class=\"number\">1</span>)).to(device))</span><br><span class=\"line\">        fake_scores = d_out_fake</span><br><span class=\"line\">        <span class=\"comment\">#fake_loss.backward()</span></span><br><span class=\"line\">        <span class=\"comment\"># 更新判别器参数</span></span><br><span class=\"line\">        d_loss = (real_loss + fake_loss)/<span class=\"number\">2</span></span><br><span class=\"line\">        d_loss.backward()</span><br><span class=\"line\">        optimizer_D.step()</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\">#=====训练生成器====</span></span><br><span class=\"line\">        optimizer_G.zero_grad()</span><br><span class=\"line\">        <span class=\"comment\"># 计算判别器对伪造样本的输出的为真样本的概率值</span></span><br><span class=\"line\">        d_out_fake = discriminator(fake_img)</span><br><span class=\"line\">        <span class=\"comment\"># 计算生成器伪造样本不被认为是真的损失</span></span><br><span class=\"line\">        g_loss = bce_loss(d_out_fake, torch.ones(size=(batch_size, <span class=\"number\">1</span>)).to(device))</span><br><span class=\"line\">        <span class=\"comment\"># 更新生成器</span></span><br><span class=\"line\">        g_loss.backward()</span><br><span class=\"line\">        optimizer_G.step()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># #################################################</span></span><br><span class=\"line\">        <span class=\"comment\"># 4：打印损失，保存图片</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> step % <span class=\"number\">200</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            generator.<span class=\"built_in\">eval</span>()</span><br><span class=\"line\">            fixed_image = generator(fixed_z)</span><br><span class=\"line\">            generator.train()</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(<span class=\"string\">&quot;[epoch: &#123;&#125;/&#123;&#125;], [iter: &#123;&#125;], [G loss: &#123;:.3f&#125;], [D loss: &#123;:.3f&#125;], [R Score: &#123;:.3f&#125;], [F Score: &#123;:.3f&#125;]&quot;</span>.<span class=\"built_in\">format</span>(epoch+<span class=\"number\">1</span>,epochs,step, g_loss.item(), d_loss.item(),real_scores.data.mean(), fake_scores.data.mean()))</span><br><span class=\"line\">            utils.save_image(fixed_image.detach(), <span class=\"built_in\">str</span>(epoch+<span class=\"number\">1</span>)+<span class=\"string\">&quot;fake.jpg&quot;</span>,normalize=<span class=\"literal\">True</span>)</span><br><span class=\"line\">            utils.save_image(image,<span class=\"built_in\">str</span>(epoch+<span class=\"number\">1</span>)+<span class=\"string\">&quot;real.jpg&quot;</span>,normalize=<span class=\"literal\">True</span>)</span><br></pre></td></tr></table></figure>\n\n<p>结果如下：<br>    [epoch: 1/20], [iter: 0], [G loss: 0.699], [D loss: 0.694], [R Score: 0.499], [F Score: 0.500]<br>    [epoch: 1/20], [iter: 200], [G loss: 0.803], [D loss: 0.715], [R Score: 0.512], [F Score: 0.529]<br>    [epoch: 1/20], [iter: 400], [G loss: 0.734], [D loss: 0.692], [R Score: 0.492], [F Score: 0.491]<br>    [epoch: 1/20], [iter: 600], [G loss: 0.730], [D loss: 0.693], [R Score: 0.496], [F Score: 0.496]<br>    [epoch: 1/20], [iter: 800], [G loss: 0.748], [D loss: 0.686], [R Score: 0.500], [F Score: 0.492]<br>    [epoch: 1/20], [iter: 1000], [G loss: 0.745], [D loss: 0.680], [R Score: 0.514], [F Score: 0.499]<br>    [epoch: 1/20], [iter: 1200], [G loss: 0.715], [D loss: 0.701], [R Score: 0.527], [F Score: 0.532]<br>    [epoch: 2/20], [iter: 0], [G loss: 0.762], [D loss: 0.679], [R Score: 0.524], [F Score: 0.508]<br>    [epoch: 2/20], [iter: 200], [G loss: 0.815], [D loss: 0.686], [R Score: 0.507], [F Score: 0.498]<br>    [epoch: 2/20], [iter: 400], [G loss: 0.836], [D loss: 0.665], [R Score: 0.509], [F Score: 0.479]<br>    [epoch: 2/20], [iter: 600], [G loss: 0.759], [D loss: 0.694], [R Score: 0.523], [F Score: 0.520]<br>    [epoch: 2/20], [iter: 800], [G loss: 0.973], [D loss: 0.646], [R Score: 0.551], [F Score: 0.499]<br>    [epoch: 2/20], [iter: 1000], [G loss: 0.926], [D loss: 0.671], [R Score: 0.531], [F Score: 0.495]<br>    [epoch: 2/20], [iter: 1200], [G loss: 1.100], [D loss: 0.582], [R Score: 0.497], [F Score: 0.362]</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210225180739.png\"></p>\n<p>第7个epoch：<br><img src=\"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210226103604.jpg\"></p>\n<p>batch_size以及其他参数可自行调整。</p>\n<h2 id=\"5-WGAN原理\"><a href=\"#5-WGAN原理\" class=\"headerlink\" title=\"5. WGAN原理\"></a>5. WGAN原理</h2><p>论文：<a href=\"https://arxiv.org/pdf/1701.07875.pdf\">Wasserstein GAN</a><br><a href=\"https://arxiv.org/abs/1701.04862\">Towards Principled Methods for Training Generative Adversarial Networks</a></p>\n<p>总所周知，GAN的训练存在很多问题和挑战：</p>\n<ul>\n<li>训练困难，需要精心设计模型结构，协调G和D的训练程度</li>\n<li>G和D的损失函数无法指示训练过程，缺乏一个有意义的指标和生成图片的质量相关联</li>\n<li>模式崩坏（mode collapse），生成的图片虽然看起来像是真的，但是缺乏多样性</li>\n</ul>\n<p>WGAN相比较于传统的GAN，做了如下修改：</p>\n<ul>\n<li>D最后一层去掉sigmoid</li>\n<li>G和D的loss不取log</li>\n<li>每次更新D的参数后，将其绝对值截断到不超过一个固定常数c</li>\n<li>不要用基于动量的优化算法（包括momentum和Adam），推荐RMSProp，SGD也行</li>\n</ul>\n<p><img src=\"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210303105435.png\"></p>\n<p>G的损失函数原本为$\\mathbb{E} _ {z \\sim p _ z}[\\log(1-D(G(z)))]$ ，其导致的结果是，如果D训练得太好，G将学习不到有效的梯度。但是，如果D训练得不够好，G也学习不到有效的梯度。<br>因此以上损失函数导致GAN训练特别不稳定，需要小心协调G和D的训练程度。</p>\n<blockquote>\n<p>WGAN参考资料：<br><a href=\"https://zhuanlan.zhihu.com/p/44169714\">https://zhuanlan.zhihu.com/p/44169714</a><br><a href=\"https://www.cnblogs.com/Allen-rg/p/10305125.html\">https://www.cnblogs.com/Allen-rg/p/10305125.html</a></p>\n</blockquote>\n","more":"<h1 id=\"GAN学习笔记\"><a href=\"#GAN学习笔记\" class=\"headerlink\" title=\"GAN学习笔记\"></a>GAN学习笔记</h1><h2 id=\"1-GAN原理\"><a href=\"#1-GAN原理\" class=\"headerlink\" title=\"1. GAN原理\"></a>1. GAN原理</h2><p>论文链接：<a href=\"https://arxiv.org/abs/1406.2661\">Generative Adversarial Networks</a></p>\n<blockquote>\n<p>生成式对抗网络(GAN, Generative Adversarial Networks)是一种深度学习模型，是近年来复杂分布上无监督学习最具前景的方法之一。模型通过框架中（至少）两个模块：生成模型（Generative Model）和判别模型（Discriminative Model）的互相博弈学习产生相当好的输出。原始 GAN 理论中，并不要求 G 和 D 都是神经网络，只需要是能拟合相应生成和判别的函数即可。但实用中一般均使用深度神经网络作为 G 和 D 。一个优秀的GAN应用需要有良好的训练方法，否则可能由于神经网络模型的自由性而导致输出不理想。<br>Ian J. Goodfellow等人于2014年10月在<a href=\"https://arxiv.org/abs/1406.2661\">Generative Adversarial Networks</a>中提出了一个通过对抗过程估计生成模型的新框架。框架中同时训练两个模型：捕获数据分布的生成模型G，和估计样本来自训练数据的概率的判别模型D。G的训练程序是将D错误的概率最大化。这个框架对应一个最大值集下限的双方对抗游戏。可以证明在任意函数G和D的空间中，存在唯一的解决方案，使得G重现训练数据分布，而D=0.5。在G和D由多层感知器定义的情况下，整个系统可以用反向传播进行训练。在训练或生成样本期间，不需要任何马尔科夫链或展开的近似推理网络。实验通过对生成的样品的定性和定量评估证明了本框架的潜力。<br>  —- 摘自<a href=\"https://baike.baidu.com/item/Gan/22181905?fr=aladdin\">百度百科</a></p>\n</blockquote>\n<p>GAN是由两部分组成的，第一部分是生成，第二部分是对抗。简单来说，就是有一个生成网络G和一个判别网络D，通过训练让两个网络相互竞争，生成网络G接受一个随机噪声z来生成假的数据G(z)，对抗网络D通过判别器去判别真伪概率，最后希望生成器G生成的数据能够以假乱真。在最理想的状态下，D(G(z)) = 0.5。</p>\n<p>以上原理的数学公式为：</p>\n<p>$$ min_{G}max_{D}V(D,G) = \\mathbb{E} _ {x \\sim p_{data}(x)} [\\log D(x)] + \\mathbb{E} _ {z \\sim p_{z}(z) [\\log (1-D(G(z)))]} $$</p>\n<p>式子中，x表示真实数据，z表示噪声，G(z)表示G网络根据z生成的数据，D(x)表示D网络判断真实数据是否为真的概率，因此D(x)接近1越好。而D(G(z))代表D网络判断G网络生成的虚假数据是真实的概率。<br>因此，对于D网络(辨别器)：</p>\n<ul>\n<li>如果x来自$P_{data}$，那么D(x)要越大越好，可以用$\\log(D(x)) \\uparrow$表示。</li>\n<li>如果x来自于$P_{generator}$，那么D(G(z))越小越好，进而表示为$\\log[1−D(G(z))] \\uparrow$。</li>\n<li>因此需要最大化$max_D$<br>对于G网络(生成器)：</li>\n<li>$D(G(z))$越大越好，进而表示为log[1−D(G(z))]↓</li>\n<li>因此需要最小化$min_{G}$。</li>\n</ul>\n<p><img src=\"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210223180633.png\"></p>\n<p>第一步我们训练D，D是希望V(D,G)越大越好，所以是加上梯度(ascending)。第二步训练G时，V(D,G)越小越好，所以是减去梯度(descending)。整个训练过程交替进行。</p>\n<h2 id=\"2-GAN实例\"><a href=\"#2-GAN实例\" class=\"headerlink\" title=\"2. GAN实例\"></a>2. GAN实例</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn,optim</span><br><span class=\"line\"><span class=\"keyword\">import</span> torchvision.transforms <span class=\"keyword\">as</span> tfs</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">from</span> PIL <span class=\"keyword\">import</span> Image</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> tqdm.auto <span class=\"keyword\">import</span> tqdm</span><br><span class=\"line\"></span><br><span class=\"line\">transforms = tfs.Compose([</span><br><span class=\"line\">    tfs.Resize((<span class=\"number\">32</span>,<span class=\"number\">32</span>)),</span><br><span class=\"line\">    tfs.ToTensor(),</span><br><span class=\"line\">    <span class=\"comment\">#tfs.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])</span></span><br><span class=\"line\">])</span><br><span class=\"line\"></span><br><span class=\"line\">flat_img = <span class=\"number\">32</span>*<span class=\"number\">32</span>*<span class=\"number\">3</span></span><br><span class=\"line\">noise_dim = <span class=\"number\">100</span></span><br><span class=\"line\"></span><br><span class=\"line\">img = Image.<span class=\"built_in\">open</span>(<span class=\"string\">&#x27;1.jpg&#x27;</span>)</span><br><span class=\"line\">real_img = transforms(img)</span><br><span class=\"line\"></span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">2</span>)</span><br><span class=\"line\">fake_img = torch.rand(<span class=\"number\">1</span>,noise_dim)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.imshow(np.transpose(real_img.numpy(),(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">0</span>)))</span><br><span class=\"line\"><span class=\"comment\">#print(real_img)</span></span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210224172221.png\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Discriminator</span>(<span class=\"params\">nn.Module</span>):</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self</span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        self.linear = nn.Sequential(</span><br><span class=\"line\">            nn.Linear(flat_img, <span class=\"number\">1024</span>),</span><br><span class=\"line\">            nn.ReLU(),</span><br><span class=\"line\">            nn.Linear(<span class=\"number\">1024</span>, <span class=\"number\">2048</span>),</span><br><span class=\"line\">            nn.ReLU(),</span><br><span class=\"line\">            nn.Linear(<span class=\"number\">2048</span>, <span class=\"number\">1</span>),</span><br><span class=\"line\">            nn.Sigmoid() <span class=\"comment\">#sigmoid常用于二分类问题</span></span><br><span class=\"line\">        )</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, img</span>):</span></span><br><span class=\"line\">        img = img.view(<span class=\"number\">1</span>, -<span class=\"number\">1</span>)</span><br><span class=\"line\">        out = self.linear(img)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> out</span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Generator</span>(<span class=\"params\">nn.Module</span>):</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self</span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>().__init__()</span><br><span class=\"line\">        self.linear = nn.Sequential(</span><br><span class=\"line\">            nn.Linear(noise_dim, <span class=\"number\">1024</span>),</span><br><span class=\"line\">            nn.LeakyReLU(),</span><br><span class=\"line\">            nn.Linear(<span class=\"number\">1024</span>, <span class=\"number\">2048</span>),</span><br><span class=\"line\">            nn.LeakyReLU(),</span><br><span class=\"line\">            nn.Linear(<span class=\"number\">2048</span>, flat_img)</span><br><span class=\"line\">        )</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, latent_space</span>):</span></span><br><span class=\"line\">        latent_space = latent_space.view(<span class=\"number\">1</span>, -<span class=\"number\">1</span>)</span><br><span class=\"line\">        out = self.linear(latent_space)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> out</span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">device = <span class=\"string\">&#x27;cuda:0&#x27;</span> <span class=\"keyword\">if</span> torch.cuda.is_available() <span class=\"keyword\">else</span> <span class=\"string\">&#x27;cpu&#x27;</span></span><br><span class=\"line\"></span><br><span class=\"line\">discr = Discriminator().to(device)</span><br><span class=\"line\">gen = Generator().to(device)</span><br><span class=\"line\"></span><br><span class=\"line\">opt_d = optim.SGD(discr.parameters(), lr=<span class=\"number\">0.001</span>, momentum=<span class=\"number\">0.9</span>)</span><br><span class=\"line\">opt_g = optim.SGD(gen.parameters(), lr=<span class=\"number\">0.001</span>, momentum=<span class=\"number\">0.9</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">criterion = nn.BCELoss()</span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">epochs = <span class=\"number\">200</span></span><br><span class=\"line\">discr_e = <span class=\"number\">4</span></span><br><span class=\"line\">gen_e = <span class=\"number\">4</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#whole model training starts here</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(epochs):</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">#discriminator training</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> k <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(discr_e):</span><br><span class=\"line\">        out_d1 = discr(real_img.to(device))</span><br><span class=\"line\">        <span class=\"comment\">#loss for real image</span></span><br><span class=\"line\">        loss_d1 = criterion(out_d1, torch.ones((<span class=\"number\">1</span>, <span class=\"number\">1</span>)).to(device))</span><br><span class=\"line\"></span><br><span class=\"line\">        out_d2 = gen(fake_img.to(device)).detach()</span><br><span class=\"line\">        <span class=\"comment\">#loss for fake image</span></span><br><span class=\"line\">        loss_d2 = criterion(discr(out_d2.to(device)), torch.zeros((<span class=\"number\">1</span>, <span class=\"number\">1</span>)).to(device))</span><br><span class=\"line\"></span><br><span class=\"line\">        opt_d.zero_grad()</span><br><span class=\"line\">        loss_d = loss_d1+loss_d2</span><br><span class=\"line\">        loss_d.backward()</span><br><span class=\"line\">        opt_d.step()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">#generator training</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(gen_e):</span><br><span class=\"line\">        out_g = gen(fake_img.to(device))</span><br><span class=\"line\">        <span class=\"comment\">#Binary cross entropy loss</span></span><br><span class=\"line\">        loss_g = criterion(discr(out_g.to(device)), torch.ones(<span class=\"number\">1</span>, <span class=\"number\">1</span>).to(device))</span><br><span class=\"line\">        <span class=\"comment\">#Loss function in the GAN paper</span></span><br><span class=\"line\">        <span class=\"comment\">#[log(1 - D(G(z)))]</span></span><br><span class=\"line\">        <span class=\"comment\">#loss_g = torch.log(torch.ones(1, 1).to(device) - (discr(out_g.to(device))))</span></span><br><span class=\"line\">        </span><br><span class=\"line\">        opt_g.zero_grad()</span><br><span class=\"line\">        loss_g.backward()</span><br><span class=\"line\">        opt_g.step()</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">if</span> (epoch+<span class=\"number\">1</span>)%<span class=\"number\">10</span>==<span class=\"number\">0</span>:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Epoch[&#123;&#125;/&#123;&#125;],d_loss:&#123;:.6f&#125;,g_loss:&#123;:.6f&#125;&#x27;</span>.<span class=\"built_in\">format</span>(epoch+<span class=\"number\">1</span>,epochs,loss_d.data.item(),loss_g.data.item()))</span><br><span class=\"line\"></span><br><span class=\"line\">out=gen(fake_img.to(device)).detach()</span><br><span class=\"line\">out_score=discr(out_g.to(device))</span><br><span class=\"line\">loss = criterion(out_score, torch.ones(<span class=\"number\">1</span>, <span class=\"number\">1</span>).to(device))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;score:&quot;</span>,out_score.item(),<span class=\"string\">&quot;loss:&quot;</span>,loss.item())</span><br><span class=\"line\"></span><br><span class=\"line\">out=out.reshape((<span class=\"number\">3</span>,<span class=\"number\">32</span>,<span class=\"number\">32</span>)).cpu()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#print(out)</span></span><br><span class=\"line\">plt.subplot(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>)</span><br><span class=\"line\">plt.title(<span class=\"string\">&#x27;fake&#x27;</span>)</span><br><span class=\"line\">plt.imshow(np.transpose(out.numpy(),(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">0</span>)))</span><br><span class=\"line\">plt.subplot(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">plt.title(<span class=\"string\">&#x27;real&#x27;</span>)</span><br><span class=\"line\">plt.imshow(np.transpose(real_img.numpy(),(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">0</span>)))</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210224183510.png\"></p>\n<h2 id=\"3-DCGAN原理\"><a href=\"#3-DCGAN原理\" class=\"headerlink\" title=\"3. DCGAN原理\"></a>3. DCGAN原理</h2><p><a href=\"https://arxiv.org/pdf/1511.06434.pdf\">https://arxiv.org/pdf/1511.06434.pdf</a></p>\n<p>DCGAN的原理和GAN是一样的。只不过DCGANs体系结构有所改变：</p>\n<ul>\n<li>使用指定步长的卷积层代替池化层</li>\n<li>在生成器和鉴别器中使用batch norm。</li>\n<li>移除全连接层，以实现更深层次的体系结构，减少参数。</li>\n<li>在生成器中使用ReLU激活，但输出使用Tanh。</li>\n<li>在鉴别器中使用LeakyReLU激活</li>\n</ul>\n<p><img src=\"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210224183753.png\"></p>\n<p>DCGAN中提到了网络的训练细节：</p>\n<ul>\n<li>使用Adam算法更新参数，betas=(0.5, 0.999)；</li>\n<li>batch size选为128；</li>\n<li>权重使用正太分布，均值为0，标准差为0.02；</li>\n<li>学习率0.0002。</li>\n</ul>\n<h2 id=\"4-DCGAN实例\"><a href=\"#4-DCGAN实例\" class=\"headerlink\" title=\"4. DCGAN实例\"></a>4. DCGAN实例</h2><p>生成动漫头像，数据集来自<a href=\"https://www.kaggle.com/soumikrakshit/anime-faces\">https://www.kaggle.com/soumikrakshit/anime-faces</a></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> os</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> imageio</span><br><span class=\"line\"><span class=\"keyword\">from</span> tqdm.auto <span class=\"keyword\">import</span> tqdm</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch,torchvision</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.data <span class=\"keyword\">import</span> Dataset, DataLoader</span><br><span class=\"line\"><span class=\"keyword\">from</span> torchvision <span class=\"keyword\">import</span> transforms, utils</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\">avatar_img_path = <span class=\"string\">&quot;E:/python/dataset/anime face/data&quot;</span></span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">trans = transforms.Compose([</span><br><span class=\"line\">    transforms.ToTensor(),</span><br><span class=\"line\">    transforms.Normalize((<span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>), (<span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>, <span class=\"number\">0.5</span>))</span><br><span class=\"line\">])</span><br><span class=\"line\"></span><br><span class=\"line\">noise_dim = <span class=\"number\">100</span></span><br><span class=\"line\">batch_size = <span class=\"number\">16</span></span><br><span class=\"line\">beta1=<span class=\"number\">0.5</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">#自定义数据集</span></span><br><span class=\"line\"><span class=\"string\">file_train=[]</span></span><br><span class=\"line\"><span class=\"string\">for image_name in tqdm(os.listdir(avatar_img_path)):</span></span><br><span class=\"line\"><span class=\"string\">    file_train.append(os.path.join(avatar_img_path,image_name))</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">def default_loader(path):</span></span><br><span class=\"line\"><span class=\"string\">    img = imageio.imread(path)</span></span><br><span class=\"line\"><span class=\"string\">    img = img/255</span></span><br><span class=\"line\"><span class=\"string\">    img = trans(img)</span></span><br><span class=\"line\"><span class=\"string\">    return img</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">class trainset(Dataset):</span></span><br><span class=\"line\"><span class=\"string\">    def __init__(self, loader=default_loader):</span></span><br><span class=\"line\"><span class=\"string\">    #定义好 image 的路径</span></span><br><span class=\"line\"><span class=\"string\">        self.images = file_train</span></span><br><span class=\"line\"><span class=\"string\">        self.target = 0</span></span><br><span class=\"line\"><span class=\"string\">        self.loader = loader</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    def __getitem__(self, index):</span></span><br><span class=\"line\"><span class=\"string\">        fn = self.images[index]</span></span><br><span class=\"line\"><span class=\"string\">        img = self.loader(fn)</span></span><br><span class=\"line\"><span class=\"string\">        target = self.target</span></span><br><span class=\"line\"><span class=\"string\">        return img,target</span></span><br><span class=\"line\"><span class=\"string\">    </span></span><br><span class=\"line\"><span class=\"string\">    def __len__(self):</span></span><br><span class=\"line\"><span class=\"string\">        return len(self.images)</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\">img_dataset=torchvision.datasets.ImageFolder(<span class=\"string\">&quot;E:/python/dataset/anime face&quot;</span>, transform=trans)</span><br><span class=\"line\"><span class=\"comment\">#img_dataset=trainset()</span></span><br><span class=\"line\">img_dataloader=DataLoader(img_dataset,batch_size=batch_size,shuffle=<span class=\"literal\">True</span>)</span><br><span class=\"line\"><span class=\"comment\">#print(img_dataset)</span></span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Generator</span>(<span class=\"params\">nn.Module</span>):</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self, z_dim</span>):</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Generator,self).__init__()</span><br><span class=\"line\">        self.z_dim = z_dim</span><br><span class=\"line\">        self.generator = nn.Sequential(</span><br><span class=\"line\">            nn.ConvTranspose2d(self.z_dim,<span class=\"number\">512</span>,<span class=\"number\">4</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>,bias=<span class=\"literal\">False</span>),</span><br><span class=\"line\">            nn.BatchNorm2d(num_features=<span class=\"number\">512</span>),</span><br><span class=\"line\">            nn.ReLU(<span class=\"literal\">True</span>),</span><br><span class=\"line\">            nn.ConvTranspose2d(<span class=\"number\">512</span>,<span class=\"number\">256</span>,<span class=\"number\">4</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>,bias=<span class=\"literal\">False</span>),</span><br><span class=\"line\">            nn.BatchNorm2d(num_features=<span class=\"number\">256</span>),</span><br><span class=\"line\">            nn.ReLU(<span class=\"literal\">True</span>),</span><br><span class=\"line\">            nn.ConvTranspose2d(<span class=\"number\">256</span>,<span class=\"number\">128</span>,<span class=\"number\">4</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>,bias=<span class=\"literal\">False</span>),</span><br><span class=\"line\">            nn.BatchNorm2d(num_features=<span class=\"number\">128</span>),</span><br><span class=\"line\">            nn.ReLU(<span class=\"literal\">True</span>),</span><br><span class=\"line\">            nn.ConvTranspose2d(<span class=\"number\">128</span>,<span class=\"number\">64</span>,<span class=\"number\">4</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>,bias=<span class=\"literal\">False</span>),</span><br><span class=\"line\">            nn.BatchNorm2d(num_features=<span class=\"number\">64</span>),</span><br><span class=\"line\">            nn.ReLU(<span class=\"literal\">True</span>),</span><br><span class=\"line\">            nn.ConvTranspose2d(<span class=\"number\">64</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>,bias=<span class=\"literal\">False</span>),</span><br><span class=\"line\">            nn.Tanh()</span><br><span class=\"line\">        )</span><br><span class=\"line\">        self.weight_init()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">weight_init</span>(<span class=\"params\">self</span>):</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> m <span class=\"keyword\">in</span> self.generator.modules():</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(m, nn.ConvTranspose2d):</span><br><span class=\"line\">                nn.init.normal_(m.weight.data, <span class=\"number\">0</span>, <span class=\"number\">0.02</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">elif</span> <span class=\"built_in\">isinstance</span>(m, nn.BatchNorm2d):</span><br><span class=\"line\">                nn.init.normal_(m.weight.data, <span class=\"number\">0</span>, <span class=\"number\">0.02</span>)</span><br><span class=\"line\">                nn.init.constant_(m.bias.data, <span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, x</span>):</span></span><br><span class=\"line\">        out = self.generator(x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> out</span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">Discriminator</span>(<span class=\"params\">nn.Module</span>):</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span>(<span class=\"params\">self</span>):</span></span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        initialize</span></span><br><span class=\"line\"><span class=\"string\">        </span></span><br><span class=\"line\"><span class=\"string\">        :param image_size: tuple (3, h, w)</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Discriminator,self).__init__()</span><br><span class=\"line\">        self.discriminator = nn.Sequential(</span><br><span class=\"line\">            nn.Conv2d(<span class=\"number\">3</span>,<span class=\"number\">64</span>,<span class=\"number\">4</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>,bias=<span class=\"literal\">False</span>),</span><br><span class=\"line\">            nn.BatchNorm2d(num_features=<span class=\"number\">64</span>),</span><br><span class=\"line\">            nn.LeakyReLU(<span class=\"number\">0.2</span>),</span><br><span class=\"line\">            nn.Conv2d(<span class=\"number\">64</span>,<span class=\"number\">128</span>,<span class=\"number\">4</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>,bias=<span class=\"literal\">False</span>),</span><br><span class=\"line\">            nn.BatchNorm2d(num_features=<span class=\"number\">128</span>),</span><br><span class=\"line\">            nn.LeakyReLU(<span class=\"number\">0.2</span>),</span><br><span class=\"line\">            nn.Conv2d(<span class=\"number\">128</span>,<span class=\"number\">256</span>,<span class=\"number\">4</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>,bias=<span class=\"literal\">False</span>),</span><br><span class=\"line\">            nn.BatchNorm2d(num_features=<span class=\"number\">256</span>),</span><br><span class=\"line\">            nn.LeakyReLU(<span class=\"number\">0.2</span>),</span><br><span class=\"line\">            nn.Conv2d(<span class=\"number\">256</span>,<span class=\"number\">512</span>,<span class=\"number\">4</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>,bias=<span class=\"literal\">False</span>),</span><br><span class=\"line\">            nn.BatchNorm2d(num_features=<span class=\"number\">512</span>),</span><br><span class=\"line\">            nn.LeakyReLU(<span class=\"number\">0.2</span>),</span><br><span class=\"line\">            nn.Conv2d(<span class=\"number\">512</span>,<span class=\"number\">1</span>,<span class=\"number\">4</span>,<span class=\"number\">2</span>,<span class=\"number\">0</span>,bias=<span class=\"literal\">False</span>),</span><br><span class=\"line\">            nn.Sigmoid()</span><br><span class=\"line\">        )</span><br><span class=\"line\">        self.weight_init()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">weight_init</span>(<span class=\"params\">self</span>):</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> m <span class=\"keyword\">in</span> self.discriminator.modules():</span><br><span class=\"line\">            <span class=\"keyword\">if</span> <span class=\"built_in\">isinstance</span>(m, nn.ConvTranspose2d):</span><br><span class=\"line\">                nn.init.normal_(m.weight.data, <span class=\"number\">0</span>, <span class=\"number\">0.02</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"keyword\">elif</span> <span class=\"built_in\">isinstance</span>(m, nn.BatchNorm2d):</span><br><span class=\"line\">                nn.init.normal_(m.weight.data, <span class=\"number\">0</span>, <span class=\"number\">0.02</span>)</span><br><span class=\"line\">                nn.init.constant_(m.bias.data, <span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">forward</span>(<span class=\"params\">self, x</span>):</span></span><br><span class=\"line\">        out = self.discriminator(x)</span><br><span class=\"line\">        out = out.view(x.size(<span class=\"number\">0</span>), -<span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> out</span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">device = torch.device(<span class=\"string\">&quot;cuda:0&quot;</span> <span class=\"keyword\">if</span> torch.cuda.is_available <span class=\"keyword\">else</span> <span class=\"string\">&quot;cpu&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">generator = Generator(noise_dim).to(device)</span><br><span class=\"line\">discriminator = Discriminator().to(device)</span><br><span class=\"line\"></span><br><span class=\"line\">bce_loss = nn.BCELoss()</span><br><span class=\"line\"><span class=\"comment\">#optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(beta1, 0.999))</span></span><br><span class=\"line\">optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=<span class=\"number\">0.00005</span>, betas=(beta1, <span class=\"number\">0.999</span>))</span><br><span class=\"line\">optimizer_G = torch.optim.Adam(generator.parameters(), lr=<span class=\"number\">0.0002</span>, betas=(beta1, <span class=\"number\">0.999</span>))</span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">epochs=<span class=\"number\">20</span></span><br><span class=\"line\"></span><br><span class=\"line\">fixed_z=torch.randn(batch_size,noise_dim,<span class=\"number\">1</span>,<span class=\"number\">1</span>,device=device)</span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(epochs):</span><br><span class=\"line\">    <span class=\"keyword\">for</span> step,(image,_) <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(img_dataloader):</span><br><span class=\"line\">        batch_size=image.size(<span class=\"number\">0</span>)</span><br><span class=\"line\">        <span class=\"comment\">#=====训练辨别器====</span></span><br><span class=\"line\">        optimizer_D.zero_grad()</span><br><span class=\"line\">        <span class=\"comment\"># 计算判别器对真实样本给出为真的概率</span></span><br><span class=\"line\">        d_out_real = discriminator(image.<span class=\"built_in\">type</span>(torch.FloatTensor).to(device))</span><br><span class=\"line\">        real_loss = bce_loss(d_out_real, torch.ones(size=(batch_size, <span class=\"number\">1</span>)).to(device))</span><br><span class=\"line\">        real_scores = d_out_real</span><br><span class=\"line\">        <span class=\"comment\">#real_loss.backward()</span></span><br><span class=\"line\">        <span class=\"comment\"># 计算判别器对假样本给出为真的概率</span></span><br><span class=\"line\">        noise = torch.randn(batch_size,noise_dim,<span class=\"number\">1</span>,<span class=\"number\">1</span>,device=device)</span><br><span class=\"line\">        fake_img = generator(noise)</span><br><span class=\"line\">        d_out_fake = discriminator(fake_img.detach())</span><br><span class=\"line\">        fake_loss = bce_loss(d_out_fake, torch.zeros(size=(batch_size, <span class=\"number\">1</span>)).to(device))</span><br><span class=\"line\">        fake_scores = d_out_fake</span><br><span class=\"line\">        <span class=\"comment\">#fake_loss.backward()</span></span><br><span class=\"line\">        <span class=\"comment\"># 更新判别器参数</span></span><br><span class=\"line\">        d_loss = (real_loss + fake_loss)/<span class=\"number\">2</span></span><br><span class=\"line\">        d_loss.backward()</span><br><span class=\"line\">        optimizer_D.step()</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\">#=====训练生成器====</span></span><br><span class=\"line\">        optimizer_G.zero_grad()</span><br><span class=\"line\">        <span class=\"comment\"># 计算判别器对伪造样本的输出的为真样本的概率值</span></span><br><span class=\"line\">        d_out_fake = discriminator(fake_img)</span><br><span class=\"line\">        <span class=\"comment\"># 计算生成器伪造样本不被认为是真的损失</span></span><br><span class=\"line\">        g_loss = bce_loss(d_out_fake, torch.ones(size=(batch_size, <span class=\"number\">1</span>)).to(device))</span><br><span class=\"line\">        <span class=\"comment\"># 更新生成器</span></span><br><span class=\"line\">        g_loss.backward()</span><br><span class=\"line\">        optimizer_G.step()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># #################################################</span></span><br><span class=\"line\">        <span class=\"comment\"># 4：打印损失，保存图片</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> step % <span class=\"number\">200</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            generator.<span class=\"built_in\">eval</span>()</span><br><span class=\"line\">            fixed_image = generator(fixed_z)</span><br><span class=\"line\">            generator.train()</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(<span class=\"string\">&quot;[epoch: &#123;&#125;/&#123;&#125;], [iter: &#123;&#125;], [G loss: &#123;:.3f&#125;], [D loss: &#123;:.3f&#125;], [R Score: &#123;:.3f&#125;], [F Score: &#123;:.3f&#125;]&quot;</span>.<span class=\"built_in\">format</span>(epoch+<span class=\"number\">1</span>,epochs,step, g_loss.item(), d_loss.item(),real_scores.data.mean(), fake_scores.data.mean()))</span><br><span class=\"line\">            utils.save_image(fixed_image.detach(), <span class=\"built_in\">str</span>(epoch+<span class=\"number\">1</span>)+<span class=\"string\">&quot;fake.jpg&quot;</span>,normalize=<span class=\"literal\">True</span>)</span><br><span class=\"line\">            utils.save_image(image,<span class=\"built_in\">str</span>(epoch+<span class=\"number\">1</span>)+<span class=\"string\">&quot;real.jpg&quot;</span>,normalize=<span class=\"literal\">True</span>)</span><br></pre></td></tr></table></figure>\n\n<p>结果如下：<br>    [epoch: 1/20], [iter: 0], [G loss: 0.699], [D loss: 0.694], [R Score: 0.499], [F Score: 0.500]<br>    [epoch: 1/20], [iter: 200], [G loss: 0.803], [D loss: 0.715], [R Score: 0.512], [F Score: 0.529]<br>    [epoch: 1/20], [iter: 400], [G loss: 0.734], [D loss: 0.692], [R Score: 0.492], [F Score: 0.491]<br>    [epoch: 1/20], [iter: 600], [G loss: 0.730], [D loss: 0.693], [R Score: 0.496], [F Score: 0.496]<br>    [epoch: 1/20], [iter: 800], [G loss: 0.748], [D loss: 0.686], [R Score: 0.500], [F Score: 0.492]<br>    [epoch: 1/20], [iter: 1000], [G loss: 0.745], [D loss: 0.680], [R Score: 0.514], [F Score: 0.499]<br>    [epoch: 1/20], [iter: 1200], [G loss: 0.715], [D loss: 0.701], [R Score: 0.527], [F Score: 0.532]<br>    [epoch: 2/20], [iter: 0], [G loss: 0.762], [D loss: 0.679], [R Score: 0.524], [F Score: 0.508]<br>    [epoch: 2/20], [iter: 200], [G loss: 0.815], [D loss: 0.686], [R Score: 0.507], [F Score: 0.498]<br>    [epoch: 2/20], [iter: 400], [G loss: 0.836], [D loss: 0.665], [R Score: 0.509], [F Score: 0.479]<br>    [epoch: 2/20], [iter: 600], [G loss: 0.759], [D loss: 0.694], [R Score: 0.523], [F Score: 0.520]<br>    [epoch: 2/20], [iter: 800], [G loss: 0.973], [D loss: 0.646], [R Score: 0.551], [F Score: 0.499]<br>    [epoch: 2/20], [iter: 1000], [G loss: 0.926], [D loss: 0.671], [R Score: 0.531], [F Score: 0.495]<br>    [epoch: 2/20], [iter: 1200], [G loss: 1.100], [D loss: 0.582], [R Score: 0.497], [F Score: 0.362]</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210225180739.png\"></p>\n<p>第7个epoch：<br><img src=\"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210226103604.jpg\"></p>\n<p>batch_size以及其他参数可自行调整。</p>\n<h2 id=\"5-WGAN原理\"><a href=\"#5-WGAN原理\" class=\"headerlink\" title=\"5. WGAN原理\"></a>5. WGAN原理</h2><p>论文：<a href=\"https://arxiv.org/pdf/1701.07875.pdf\">Wasserstein GAN</a><br><a href=\"https://arxiv.org/abs/1701.04862\">Towards Principled Methods for Training Generative Adversarial Networks</a></p>\n<p>总所周知，GAN的训练存在很多问题和挑战：</p>\n<ul>\n<li>训练困难，需要精心设计模型结构，协调G和D的训练程度</li>\n<li>G和D的损失函数无法指示训练过程，缺乏一个有意义的指标和生成图片的质量相关联</li>\n<li>模式崩坏（mode collapse），生成的图片虽然看起来像是真的，但是缺乏多样性</li>\n</ul>\n<p>WGAN相比较于传统的GAN，做了如下修改：</p>\n<ul>\n<li>D最后一层去掉sigmoid</li>\n<li>G和D的loss不取log</li>\n<li>每次更新D的参数后，将其绝对值截断到不超过一个固定常数c</li>\n<li>不要用基于动量的优化算法（包括momentum和Adam），推荐RMSProp，SGD也行</li>\n</ul>\n<p><img src=\"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20210303105435.png\"></p>\n<p>G的损失函数原本为$\\mathbb{E} _ {z \\sim p _ z}[\\log(1-D(G(z)))]$ ，其导致的结果是，如果D训练得太好，G将学习不到有效的梯度。但是，如果D训练得不够好，G也学习不到有效的梯度。<br>因此以上损失函数导致GAN训练特别不稳定，需要小心协调G和D的训练程度。</p>\n<blockquote>\n<p>WGAN参考资料：<br><a href=\"https://zhuanlan.zhihu.com/p/44169714\">https://zhuanlan.zhihu.com/p/44169714</a><br><a href=\"https://www.cnblogs.com/Allen-rg/p/10305125.html\">https://www.cnblogs.com/Allen-rg/p/10305125.html</a></p>\n</blockquote>\n","categories":[{"name":"人工智能","path":"api/categories/人工智能.json"}],"tags":[{"name":"深度学习","path":"api/tags/深度学习.json"},{"name":"交叉熵","path":"api/tags/交叉熵.json"}]}