{"title":"目标检测 | YOLOv1，经典单阶段Anchor-Free目标检测模型","slug":"人工智能-YOLO阅读笔记","date":"2022-03-28T03:57:51.000Z","updated":"2022-03-28T03:57:51.000Z","comments":true,"path":"api/articles/人工智能-YOLO阅读笔记.json","excerpt":null,"covers":["https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/1648433292(1).png","https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20220328103035.png","https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20220328103753.png","https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20220328103101.png","https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20220328110113.png","https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20220328105647.png","https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20220328105758.png","https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20220328105711.png"],"content":"<blockquote>\n<p>PS:参考YOLO官网的配色和logo做的封面图，感觉还挺好看的，hhhh</p>\n</blockquote>\n<h1 id=\"You-Only-Look-Once-Unified-Real-Time-Object-Detection\"><a href=\"#You-Only-Look-Once-Unified-Real-Time-Object-Detection\" class=\"headerlink\" title=\"You Only Look Once: Unified, Real-Time Object Detection\"></a>You Only Look Once: Unified, Real-Time Object Detection</h1><blockquote>\n<p>论文发表：CVPR 2016<br>论文链接：<a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf\">You Only Look Once: Unified, Real-Time Object Detection (cv-foundation.org)</a><br>论文官网：<a href=\"https://pjreddie.com/darknet/yolo/\">YOLO: Real-Time Object Detection (pjreddie.com)</a></p>\n</blockquote>\n<p><img src=\"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/1648433292(1).png\"></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@inproceedings&#123;redmon2016you,</span><br><span class=\"line\">  title=&#123;You only look once: Unified, real-time object detection&#125;,</span><br><span class=\"line\">  author=&#123;Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali&#125;,</span><br><span class=\"line\">  booktitle=&#123;Proceedings of the IEEE conference on computer vision and pattern recognition&#125;,</span><br><span class=\"line\">  pages=&#123;779--788&#125;,</span><br><span class=\"line\">  year=&#123;2016&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"1-归纳总结\"><a href=\"#1-归纳总结\" class=\"headerlink\" title=\"1. 归纳总结\"></a>1. 归纳总结</h2><table>\n<thead>\n<tr>\n<th>标签</th>\n<th>目的</th>\n<th>方法</th>\n<th>总结</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>#Anchor-Free</td>\n<td>解决两阶段算法检测慢的问题</td>\n<td>将目标检测（cls和reg）都视为回归问题</td>\n<td>经典单阶段算法</td>\n</tr>\n</tbody></table>\n<h2 id=\"2-问题背景\"><a href=\"#2-问题背景\" class=\"headerlink\" title=\"2. 问题背景\"></a>2. 问题背景</h2><p>随着深度学习的大火，在YOLO提出那一年，主流的目标检测算法框架主要分为两类：</p>\n<ul>\n<li>两阶段算法：基于Region Proposal的RCNN系列算法，先生成Proposal，再分类回归</li>\n<li>单阶段算法：直接预测不同目标的类别和位置<br>这两种算法各有优点，一般而言，两阶段算法准确度高，但速度慢；单阶段算法速度快，但准确度相对低。</li>\n</ul>\n<p>作者认为人可以一眼看到目标在哪，并且能立即知道是什么，并且对于很多实际场景而言，如自动驾驶，实时性和准确性都是非常重要的。</p>\n<h2 id=\"3-主要工作\"><a href=\"#3-主要工作\" class=\"headerlink\" title=\"3. 主要工作\"></a>3. 主要工作</h2><p>针对上述问题，作者提出了经典的YOLO算法，它是一个统一的，端到端的单阶段目标检测算法。YOLO具体做法是，首先将输入图片缩放到448x448，然后送入CNN网络，最后使用NMS过滤网络预测结果得到检测的目标。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20220328103035.png\"></p>\n<p>而在CNN网络里，它首先将图片划分为S×S大小的网格，然后每个单元格负责检测中心点落在该格子的目标，如下图，每个单元格会输出B个边界框（每个边界框输出5个预测值：x, y, w, h, confidence）以及边界框类别概率C，例如：作者在PASCAL VOC的检测实验里使用S=7，B=2，C=类别数量20，一共预测7×7×(2×5+20)个向量。同时这里的confidence代表边界框置信度，它的定义为: </p>\n<p>$$Pr(object)\\times IoU_{pred}^{truth}$$</p>\n<p>其中边界框包含目标时，$Pr(object)=1$，否则为0。而C代表每个类别的置信度，即：</p>\n<p>$$Pr(Class_i|Object)\\times Pr(objec) \\times IoU_{pred}^{truth}=Pr(class_i)\\times IoU_{pred}^{truth}$$</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20220328103753.png\"></p>\n<h3 id=\"3-1-模型结构\"><a href=\"#3-1-模型结构\" class=\"headerlink\" title=\"3.1 模型结构\"></a>3.1 模型结构</h3><p>YOLO采用卷积网络来提取特征，然后使用全连接层来得到预测值。网络结构参考GooLeNet模型，包含24个卷积层和2个全连接层。对于卷积层，主要使用1x1卷积来做channle reduction，然后紧跟3x3卷积。对于卷积层和全连接层，采用Leaky ReLU激活函数$max(x,0.1x)$，但是最后一层却采用线性激活函数。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20220328103101.png\"></p>\n<h3 id=\"3-2-模型训练\"><a href=\"#3-2-模型训练\" class=\"headerlink\" title=\"3.2 模型训练\"></a>3.2 模型训练</h3><p>在训练之前，先在ImageNet上进行了预训练，其预训练的分类模型采用图8中前20个卷积层，然后添加一个average-pool层和全连接层。预训练之后，在预训练得到的20层卷积层之上加上随机初始化的4个卷积层和2个全连接层。由于检测任务一般需要更高清的图片，所以将网络的输入从224x224增加到了448x448。</p>\n<h3 id=\"3-3-模型损失\"><a href=\"#3-3-模型损失\" class=\"headerlink\" title=\"3.3 模型损失\"></a>3.3 模型损失</h3><p><img src=\"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20220328110113.png\"></p>\n<p>Yolo算法将目标检测看成回归问题，所以采用的是均方差损失函数。但是对不同的部分采用了不同的权重值。首先区分定位误差和分类误差。对于定位误差，即边界框坐标预测误差，采用较大的权重 $\\lambda_{coord}=5$ 。然后其区分不包含目标的边界框与含有目标的边界框的置信度，对于前者，采用较小的权重值 $\\lambda_{noobj}=0.5$ 。其它权重值均设为1。然后采用均方误差，其同等对待大小不同的边界框，但是实际上较小的边界框的坐标误差应该要比较大的边界框要更敏感。为了保证这一点，将网络的边界框的宽与高预测改为对其平方根的预测，即预测值变为$(x,y,\\sqrt{w},\\sqrt{h})$。</p>\n<p>损失函数中，第一项为边界框中心坐标的误差项，$\\mathbb{1} _ {ij}^{obj}$ 是指第 i 个单元格存在目标，且该单元格中的第 j 个边界框负责预测该目标，第二项是边界框的高与宽的误差项。第三项是包含目标的边界框的置信度误差项。第四项是不包含目标的边界框的置信度误差项。最后一项是包含目标的单元格的分类误差项，$\\mathbb{1} _ {i}^{obj}$值是指第 i 个单元格存在目标。</p>\n<h2 id=\"4-实验结果\"><a href=\"#4-实验结果\" class=\"headerlink\" title=\"4. 实验结果\"></a>4. 实验结果</h2><p><img src=\"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20220328105647.png\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20220328105758.png\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20220328105711.png\"></p>\n<h2 id=\"5-参考文献\"><a href=\"#5-参考文献\" class=\"headerlink\" title=\"5. 参考文献\"></a>5. 参考文献</h2><p><a href=\"https://zhuanlan.zhihu.com/p/32525231\">目标检测|YOLO原理与实现 - 知乎 (zhihu.com)</a></p>\n","more":"<blockquote>\n<p>PS:参考YOLO官网的配色和logo做的封面图，感觉还挺好看的，hhhh</p>\n</blockquote>\n<h1 id=\"You-Only-Look-Once-Unified-Real-Time-Object-Detection\"><a href=\"#You-Only-Look-Once-Unified-Real-Time-Object-Detection\" class=\"headerlink\" title=\"You Only Look Once: Unified, Real-Time Object Detection\"></a>You Only Look Once: Unified, Real-Time Object Detection</h1><blockquote>\n<p>论文发表：CVPR 2016<br>论文链接：<a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf\">You Only Look Once: Unified, Real-Time Object Detection (cv-foundation.org)</a><br>论文官网：<a href=\"https://pjreddie.com/darknet/yolo/\">YOLO: Real-Time Object Detection (pjreddie.com)</a></p>\n</blockquote>\n<p><img src=\"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/1648433292(1).png\"></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@inproceedings&#123;redmon2016you,</span><br><span class=\"line\">  title=&#123;You only look once: Unified, real-time object detection&#125;,</span><br><span class=\"line\">  author=&#123;Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali&#125;,</span><br><span class=\"line\">  booktitle=&#123;Proceedings of the IEEE conference on computer vision and pattern recognition&#125;,</span><br><span class=\"line\">  pages=&#123;779--788&#125;,</span><br><span class=\"line\">  year=&#123;2016&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"1-归纳总结\"><a href=\"#1-归纳总结\" class=\"headerlink\" title=\"1. 归纳总结\"></a>1. 归纳总结</h2><table>\n<thead>\n<tr>\n<th>标签</th>\n<th>目的</th>\n<th>方法</th>\n<th>总结</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>#Anchor-Free</td>\n<td>解决两阶段算法检测慢的问题</td>\n<td>将目标检测（cls和reg）都视为回归问题</td>\n<td>经典单阶段算法</td>\n</tr>\n</tbody></table>\n<h2 id=\"2-问题背景\"><a href=\"#2-问题背景\" class=\"headerlink\" title=\"2. 问题背景\"></a>2. 问题背景</h2><p>随着深度学习的大火，在YOLO提出那一年，主流的目标检测算法框架主要分为两类：</p>\n<ul>\n<li>两阶段算法：基于Region Proposal的RCNN系列算法，先生成Proposal，再分类回归</li>\n<li>单阶段算法：直接预测不同目标的类别和位置<br>这两种算法各有优点，一般而言，两阶段算法准确度高，但速度慢；单阶段算法速度快，但准确度相对低。</li>\n</ul>\n<p>作者认为人可以一眼看到目标在哪，并且能立即知道是什么，并且对于很多实际场景而言，如自动驾驶，实时性和准确性都是非常重要的。</p>\n<h2 id=\"3-主要工作\"><a href=\"#3-主要工作\" class=\"headerlink\" title=\"3. 主要工作\"></a>3. 主要工作</h2><p>针对上述问题，作者提出了经典的YOLO算法，它是一个统一的，端到端的单阶段目标检测算法。YOLO具体做法是，首先将输入图片缩放到448x448，然后送入CNN网络，最后使用NMS过滤网络预测结果得到检测的目标。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20220328103035.png\"></p>\n<p>而在CNN网络里，它首先将图片划分为S×S大小的网格，然后每个单元格负责检测中心点落在该格子的目标，如下图，每个单元格会输出B个边界框（每个边界框输出5个预测值：x, y, w, h, confidence）以及边界框类别概率C，例如：作者在PASCAL VOC的检测实验里使用S=7，B=2，C=类别数量20，一共预测7×7×(2×5+20)个向量。同时这里的confidence代表边界框置信度，它的定义为: </p>\n<p>$$Pr(object)\\times IoU_{pred}^{truth}$$</p>\n<p>其中边界框包含目标时，$Pr(object)=1$，否则为0。而C代表每个类别的置信度，即：</p>\n<p>$$Pr(Class_i|Object)\\times Pr(objec) \\times IoU_{pred}^{truth}=Pr(class_i)\\times IoU_{pred}^{truth}$$</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20220328103753.png\"></p>\n<h3 id=\"3-1-模型结构\"><a href=\"#3-1-模型结构\" class=\"headerlink\" title=\"3.1 模型结构\"></a>3.1 模型结构</h3><p>YOLO采用卷积网络来提取特征，然后使用全连接层来得到预测值。网络结构参考GooLeNet模型，包含24个卷积层和2个全连接层。对于卷积层，主要使用1x1卷积来做channle reduction，然后紧跟3x3卷积。对于卷积层和全连接层，采用Leaky ReLU激活函数$max(x,0.1x)$，但是最后一层却采用线性激活函数。</p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20220328103101.png\"></p>\n<h3 id=\"3-2-模型训练\"><a href=\"#3-2-模型训练\" class=\"headerlink\" title=\"3.2 模型训练\"></a>3.2 模型训练</h3><p>在训练之前，先在ImageNet上进行了预训练，其预训练的分类模型采用图8中前20个卷积层，然后添加一个average-pool层和全连接层。预训练之后，在预训练得到的20层卷积层之上加上随机初始化的4个卷积层和2个全连接层。由于检测任务一般需要更高清的图片，所以将网络的输入从224x224增加到了448x448。</p>\n<h3 id=\"3-3-模型损失\"><a href=\"#3-3-模型损失\" class=\"headerlink\" title=\"3.3 模型损失\"></a>3.3 模型损失</h3><p><img src=\"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20220328110113.png\"></p>\n<p>Yolo算法将目标检测看成回归问题，所以采用的是均方差损失函数。但是对不同的部分采用了不同的权重值。首先区分定位误差和分类误差。对于定位误差，即边界框坐标预测误差，采用较大的权重 $\\lambda_{coord}=5$ 。然后其区分不包含目标的边界框与含有目标的边界框的置信度，对于前者，采用较小的权重值 $\\lambda_{noobj}=0.5$ 。其它权重值均设为1。然后采用均方误差，其同等对待大小不同的边界框，但是实际上较小的边界框的坐标误差应该要比较大的边界框要更敏感。为了保证这一点，将网络的边界框的宽与高预测改为对其平方根的预测，即预测值变为$(x,y,\\sqrt{w},\\sqrt{h})$。</p>\n<p>损失函数中，第一项为边界框中心坐标的误差项，$\\mathbb{1} _ {ij}^{obj}$ 是指第 i 个单元格存在目标，且该单元格中的第 j 个边界框负责预测该目标，第二项是边界框的高与宽的误差项。第三项是包含目标的边界框的置信度误差项。第四项是不包含目标的边界框的置信度误差项。最后一项是包含目标的单元格的分类误差项，$\\mathbb{1} _ {i}^{obj}$值是指第 i 个单元格存在目标。</p>\n<h2 id=\"4-实验结果\"><a href=\"#4-实验结果\" class=\"headerlink\" title=\"4. 实验结果\"></a>4. 实验结果</h2><p><img src=\"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20220328105647.png\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20220328105758.png\"></p>\n<p><img src=\"https://cdn.jsdelivr.net/gh/Justlovesmile/CDN2/post/20220328105711.png\"></p>\n<h2 id=\"5-参考文献\"><a href=\"#5-参考文献\" class=\"headerlink\" title=\"5. 参考文献\"></a>5. 参考文献</h2><p><a href=\"https://zhuanlan.zhihu.com/p/32525231\">目标检测|YOLO原理与实现 - 知乎 (zhihu.com)</a></p>\n","categories":[{"name":"人工智能","path":"api/categories/人工智能.json"}],"tags":[{"name":"深度学习","path":"api/tags/深度学习.json"},{"name":"论文笔记","path":"api/tags/论文笔记.json"},{"name":"目标检测","path":"api/tags/目标检测.json"}]}