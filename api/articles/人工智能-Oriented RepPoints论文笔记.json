{"title":"旋转目标检测 | Oriented RepPoints，基于点集表示的旋转目标检测模型","slug":"人工智能-Oriented RepPoints论文笔记","date":"2023-02-13T07:24:42.000Z","updated":"2023-02-13T07:24:42.000Z","comments":true,"path":"api/articles/人工智能-Oriented RepPoints论文笔记.json","excerpt":null,"covers":["https://npm.elemecdn.com/justlovesmile-post@1.0.11/20230131112137.png","https://npm.elemecdn.com/justlovesmile-post@1.0.11/20230131115920.png","https://npm.elemecdn.com/justlovesmile-post@1.0.11/20230131134539.png","https://npm.elemecdn.com/justlovesmile-post@1.0.11/20230206141139.png"],"content":"<h1 id=\"1-论文信息\"><a href=\"#1-论文信息\" class=\"headerlink\" title=\"1. 论文信息\"></a>1. 论文信息</h1><p>论文标题：<code>《Oriented RepPoints for Aerial Object Detection Wentong》</code></p>\n<blockquote>\n<p>论文发表：CVPR2022<br>论文链接：<a href=\"http://openaccess.thecvf.com/content/CVPR2022/html/Li_Oriented_RepPoints_for_Aerial_Object_Detection_CVPR_2022_paper.html\">http://openaccess.thecvf.com</a></p>\n</blockquote>\n<p><img src= \"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\" data-lazy-src=\"https://npm.elemecdn.com/justlovesmile-post@1.0.11/20230131112137.png\"></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@inproceedings&#123;li2022oriented,</span><br><span class=\"line\">  title=&#123;Oriented reppoints for aerial object detection&#125;,</span><br><span class=\"line\">  author=&#123;Li, Wentong and Chen, Yijie and Hu, Kaixuan and Zhu, Jianke&#125;,</span><br><span class=\"line\">  booktitle=&#123;Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition&#125;,</span><br><span class=\"line\">  pages=&#123;1829--1838&#125;,</span><br><span class=\"line\">  year=&#123;2022&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Value</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>标签</td>\n<td>#旋转目标检测 #标签分配</td>\n</tr>\n<tr>\n<td>数据集</td>\n<td>#DOTA #HRSC2016 #UCAS-AOD #DIOR-R</td>\n</tr>\n<tr>\n<td>目的</td>\n<td>设计面向航拍图像的旋转目标检测器</td>\n</tr>\n<tr>\n<td>方法</td>\n<td>基于RepPoint实现</td>\n</tr>\n</tbody></table>\n<h1 id=\"2-问题背景\"><a href=\"#2-问题背景\" class=\"headerlink\" title=\"2. 问题背景\"></a>2. 问题背景</h1><p>作者提到航拍图像中目标具有非水平，任意方向，密集分布，背景复杂等困难，主流的方法大多将其视为简单的旋转目标检测问题。其中基于角度回归的方法最受欢迎，然而这种增加了角度预测的方法会面临损失的不连续性以及回归的不一致性问题。这是因为角度的有界周期性和旋转框的方向定义造成的。因此为了避免这种问题，一些方法重新定义了目标旋转框的表示方法。例如，基于点集表示的方法RepPoints可以捕获关键的语义特征。但是这种简单的转换函数只产生垂直-水平边界框，无法精确估计航拍图像中旋转物体的方位。同时RepPoint在忽略学到的点集的质量的同时只根据语义特征回归关键点集，会导致旋转的、密集分布的和复杂背景下的目标精度较差。</p>\n<h1 id=\"3-主要工作\"><a href=\"#3-主要工作\" class=\"headerlink\" title=\"3. 主要工作\"></a>3. 主要工作</h1><p>针对上述问题，作者提出Oriented RepPoints方法，其引入自适应点表示不同的方向，形状和姿势。同时该方法不仅可以精确定位任意方向目标，还可以捕获目标的底层几何结构。<br><img src= \"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\" data-lazy-src=\"https://npm.elemecdn.com/justlovesmile-post@1.0.11/20230131115920.png\"></p>\n<p>文章贡献点如下：</p>\n<ul>\n<li>提出了一个高效的航拍目标检测器Oriented RepPoint</li>\n<li>提出了一个质量评估和样本选择机制用于自适应学习点集</li>\n<li>在四个具有挑战的数据集上实验并展现出不错的性能</li>\n</ul>\n<h2 id=\"3-1-模型结构\"><a href=\"#3-1-模型结构\" class=\"headerlink\" title=\"3.1 模型结构\"></a>3.1 模型结构</h2><p><img src= \"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\" data-lazy-src=\"https://npm.elemecdn.com/justlovesmile-post@1.0.11/20230131134539.png\"></p>\n<p>与传统直接回归方向的方法不同，Oriented RepPoint利用自适应点集来细致地表征目标，甚至能够表征目标的几何结构。为了这个目的，Oriented RepPoint引入了可微分转换函数，其可以使点集自适应地移动到合适的位置。为了在没有直接点对点监督的情况下有效地学习高质量的自适应点，提出了一种在训练阶段选择高质量的方向点的质量度量策略。</p>\n<h2 id=\"3-2-自适应方位点集学习\"><a href=\"#3-2-自适应方位点集学习\" class=\"headerlink\" title=\"3.2 自适应方位点集学习\"></a>3.2 自适应方位点集学习</h2><p>为了将点集表示转换成旋转框表示，Oriented RepPoint引入了转换函数。在文章中，作者测试了三种转换函数，分别是：</p>\n<ul>\n<li>MinAreaRect：点集的最小外接矩形构成边界框</li>\n<li>NearestGTCorner：距离真值顶点最近的四个点构成边界框</li>\n<li>ConvexHull：通过Jarvis March算法，从点集中取能包含所有点的最大凸四边形作为边界框</li>\n</ul>\n<p>其中MinAreaRect不可微分，其他两个可微分，因此作者在推理时使用MinAreaRect，在训练时从NearestGTCorner和ConvexHull中任选一个。</p>\n<p>Oriented RepPoint包含两个阶段，第一个阶段根据特征点生成自适应点集，第二个阶段为精炼阶段，对点集进行优化。损失函数如下：<br>$$L=L_{cls}+\\lambda_1L_{s1}+\\lambda_2L_{s2}$$<br>其中$\\lambda_1,\\lambda_2$是平衡权重，$L_{cls}$是分类损失：<br>$$L_{cls}=\\frac{1}{N_{cls}}\\sum\\limits_iF_{cls}(R_i^{cls}(\\theta),b_j^{cls})$$<br>其中$R_{i}^{cls}(\\theta)$代表预测类别置信度，$b_j^{cls}$是真实类别，$F_{cls}$是focal loss。<br>$L_{s1},L_{s2}$分别代表第一阶段和第二阶段的空间定位损失，对于每一阶段定位损失计算为：<br>$$L_s=L_{loc}+L_{s.c.}$$<br>其中$L_{loc},L_{s.c.}$分别代表基于转换后边界框的定位损失（localization loss based on converted oriented boxes）和空间限制损失(spatial constraint loss)。其中<br>$$L_{loc}=\\frac{1}{N_{loc}}\\sum\\limits_i[b_j^{cls}\\geq1]F_{loc}(OB_i^{loc}(\\theta),b_j^{cls})$$<br>其中$F_{loc}$代表GIoU损失，$N_{loc}$代表全部正样本点数。<br>$$L_{s.c.}=\\frac{1}{N_a}\\frac{1}{N_o}\\sum\\limits_{i=1}\\sum\\limits_{j=1}\\rho_{ij}$$<br>其中$N_a,N_o$分别代表对每个目标分配的正样本点数以及在真值框外的点数。<br>$$\\rho=\\begin{equation}<br>    \\begin{cases}<br>      ||p_o-p_c|| , &amp; \\text{$p_{o}$ is outside GT} \\<br>      0 , &amp; \\text{otherwise}<br>    \\end{cases}<br>\\end{equation}<br>$$<br>其中$p_o$代表GT外的点，$p_c$代表GT的中心点</p>\n<h2 id=\"3-3-APAA\"><a href=\"#3-3-APAA\" class=\"headerlink\" title=\"3.3 APAA\"></a>3.3 APAA</h2><p>首先，APAA定义了一个质量评估值Q，该值从四个方法来度量学到的自适应点集的质量。<br>$$Q=Q_{cls}+\\mu_1Q_{loc}+\\mu_2Q_{ori}+\\mu_3Q_{poc}$$<br>其中$Q_{cls},Q_{loc},Q_{ori},Q_{poc}$分别代表分类置信度（classification confidence），空间位置距离（spatial location distance），Chamfer距离（Chamfer distance）以及特征多样性（point-wise feature diversity）。Chamfe距离计算如下：<br>$$CD(R^v,R^g)=\\frac{1}{2n}\\sum_{i=1}^nmin_{j}||(x_i^v,y_j^v)-(x_i^g,y_j^g)||<em>2+\\frac{1}{2n}\\sum</em>{j=1}^nmin_{i}||(x_i^v,y_j^v)-(x_i^g,y_j^g)||<em>2$$<br>特征多样性计算如下：<br>$$Q</em>{poc}=1-\\frac{1}{N_p}\\sum_{k}cos&lt;e^\\star_{i,k},e^\\star_i&gt;=1-\\frac{1}{N_p}\\sum_k\\frac{e^\\star_{i,k}\\cdot{e^\\star_i}}{||e^\\star_{i,k}||\\times||e^\\star_i||}$$<br>之后针对每个目标，利用Q值，选择前k个样本作为正样本。</p>\n<h1 id=\"4-实验结果\"><a href=\"#4-实验结果\" class=\"headerlink\" title=\"4. 实验结果\"></a>4. 实验结果</h1><p><img src= \"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\" data-lazy-src=\"https://npm.elemecdn.com/justlovesmile-post@1.0.11/20230206141139.png\"></p>\n","more":"<h1 id=\"1-论文信息\"><a href=\"#1-论文信息\" class=\"headerlink\" title=\"1. 论文信息\"></a>1. 论文信息</h1><p>论文标题：<code>《Oriented RepPoints for Aerial Object Detection Wentong》</code></p>\n<blockquote>\n<p>论文发表：CVPR2022<br>论文链接：<a href=\"http://openaccess.thecvf.com/content/CVPR2022/html/Li_Oriented_RepPoints_for_Aerial_Object_Detection_CVPR_2022_paper.html\">http://openaccess.thecvf.com</a></p>\n</blockquote>\n<p><img src=\"https://npm.elemecdn.com/justlovesmile-post@1.0.11/20230131112137.png\"></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@inproceedings&#123;li2022oriented,</span><br><span class=\"line\">  title=&#123;Oriented reppoints for aerial object detection&#125;,</span><br><span class=\"line\">  author=&#123;Li, Wentong and Chen, Yijie and Hu, Kaixuan and Zhu, Jianke&#125;,</span><br><span class=\"line\">  booktitle=&#123;Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition&#125;,</span><br><span class=\"line\">  pages=&#123;1829--1838&#125;,</span><br><span class=\"line\">  year=&#123;2022&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<table>\n<thead>\n<tr>\n<th>Name</th>\n<th>Value</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>标签</td>\n<td>#旋转目标检测 #标签分配</td>\n</tr>\n<tr>\n<td>数据集</td>\n<td>#DOTA #HRSC2016 #UCAS-AOD #DIOR-R</td>\n</tr>\n<tr>\n<td>目的</td>\n<td>设计面向航拍图像的旋转目标检测器</td>\n</tr>\n<tr>\n<td>方法</td>\n<td>基于RepPoint实现</td>\n</tr>\n</tbody></table>\n<h1 id=\"2-问题背景\"><a href=\"#2-问题背景\" class=\"headerlink\" title=\"2. 问题背景\"></a>2. 问题背景</h1><p>作者提到航拍图像中目标具有非水平，任意方向，密集分布，背景复杂等困难，主流的方法大多将其视为简单的旋转目标检测问题。其中基于角度回归的方法最受欢迎，然而这种增加了角度预测的方法会面临损失的不连续性以及回归的不一致性问题。这是因为角度的有界周期性和旋转框的方向定义造成的。因此为了避免这种问题，一些方法重新定义了目标旋转框的表示方法。例如，基于点集表示的方法RepPoints可以捕获关键的语义特征。但是这种简单的转换函数只产生垂直-水平边界框，无法精确估计航拍图像中旋转物体的方位。同时RepPoint在忽略学到的点集的质量的同时只根据语义特征回归关键点集，会导致旋转的、密集分布的和复杂背景下的目标精度较差。</p>\n<h1 id=\"3-主要工作\"><a href=\"#3-主要工作\" class=\"headerlink\" title=\"3. 主要工作\"></a>3. 主要工作</h1><p>针对上述问题，作者提出Oriented RepPoints方法，其引入自适应点表示不同的方向，形状和姿势。同时该方法不仅可以精确定位任意方向目标，还可以捕获目标的底层几何结构。<br><img src=\"https://npm.elemecdn.com/justlovesmile-post@1.0.11/20230131115920.png\"></p>\n<p>文章贡献点如下：</p>\n<ul>\n<li>提出了一个高效的航拍目标检测器Oriented RepPoint</li>\n<li>提出了一个质量评估和样本选择机制用于自适应学习点集</li>\n<li>在四个具有挑战的数据集上实验并展现出不错的性能</li>\n</ul>\n<h2 id=\"3-1-模型结构\"><a href=\"#3-1-模型结构\" class=\"headerlink\" title=\"3.1 模型结构\"></a>3.1 模型结构</h2><p><img src=\"https://npm.elemecdn.com/justlovesmile-post@1.0.11/20230131134539.png\"></p>\n<p>与传统直接回归方向的方法不同，Oriented RepPoint利用自适应点集来细致地表征目标，甚至能够表征目标的几何结构。为了这个目的，Oriented RepPoint引入了可微分转换函数，其可以使点集自适应地移动到合适的位置。为了在没有直接点对点监督的情况下有效地学习高质量的自适应点，提出了一种在训练阶段选择高质量的方向点的质量度量策略。</p>\n<h2 id=\"3-2-自适应方位点集学习\"><a href=\"#3-2-自适应方位点集学习\" class=\"headerlink\" title=\"3.2 自适应方位点集学习\"></a>3.2 自适应方位点集学习</h2><p>为了将点集表示转换成旋转框表示，Oriented RepPoint引入了转换函数。在文章中，作者测试了三种转换函数，分别是：</p>\n<ul>\n<li>MinAreaRect：点集的最小外接矩形构成边界框</li>\n<li>NearestGTCorner：距离真值顶点最近的四个点构成边界框</li>\n<li>ConvexHull：通过Jarvis March算法，从点集中取能包含所有点的最大凸四边形作为边界框</li>\n</ul>\n<p>其中MinAreaRect不可微分，其他两个可微分，因此作者在推理时使用MinAreaRect，在训练时从NearestGTCorner和ConvexHull中任选一个。</p>\n<p>Oriented RepPoint包含两个阶段，第一个阶段根据特征点生成自适应点集，第二个阶段为精炼阶段，对点集进行优化。损失函数如下：<br>$$L=L_{cls}+\\lambda_1L_{s1}+\\lambda_2L_{s2}$$<br>其中$\\lambda_1,\\lambda_2$是平衡权重，$L_{cls}$是分类损失：<br>$$L_{cls}=\\frac{1}{N_{cls}}\\sum\\limits_iF_{cls}(R_i^{cls}(\\theta),b_j^{cls})$$<br>其中$R_{i}^{cls}(\\theta)$代表预测类别置信度，$b_j^{cls}$是真实类别，$F_{cls}$是focal loss。<br>$L_{s1},L_{s2}$分别代表第一阶段和第二阶段的空间定位损失，对于每一阶段定位损失计算为：<br>$$L_s=L_{loc}+L_{s.c.}$$<br>其中$L_{loc},L_{s.c.}$分别代表基于转换后边界框的定位损失（localization loss based on converted oriented boxes）和空间限制损失(spatial constraint loss)。其中<br>$$L_{loc}=\\frac{1}{N_{loc}}\\sum\\limits_i[b_j^{cls}\\geq1]F_{loc}(OB_i^{loc}(\\theta),b_j^{cls})$$<br>其中$F_{loc}$代表GIoU损失，$N_{loc}$代表全部正样本点数。<br>$$L_{s.c.}=\\frac{1}{N_a}\\frac{1}{N_o}\\sum\\limits_{i=1}\\sum\\limits_{j=1}\\rho_{ij}$$<br>其中$N_a,N_o$分别代表对每个目标分配的正样本点数以及在真值框外的点数。<br>$$\\rho=\\begin{equation}<br>    \\begin{cases}<br>      ||p_o-p_c|| , &amp; \\text{$p_{o}$ is outside GT} \\<br>      0 , &amp; \\text{otherwise}<br>    \\end{cases}<br>\\end{equation}<br>$$<br>其中$p_o$代表GT外的点，$p_c$代表GT的中心点</p>\n<h2 id=\"3-3-APAA\"><a href=\"#3-3-APAA\" class=\"headerlink\" title=\"3.3 APAA\"></a>3.3 APAA</h2><p>首先，APAA定义了一个质量评估值Q，该值从四个方法来度量学到的自适应点集的质量。<br>$$Q=Q_{cls}+\\mu_1Q_{loc}+\\mu_2Q_{ori}+\\mu_3Q_{poc}$$<br>其中$Q_{cls},Q_{loc},Q_{ori},Q_{poc}$分别代表分类置信度（classification confidence），空间位置距离（spatial location distance），Chamfer距离（Chamfer distance）以及特征多样性（point-wise feature diversity）。Chamfe距离计算如下：<br>$$CD(R^v,R^g)=\\frac{1}{2n}\\sum_{i=1}^nmin_{j}||(x_i^v,y_j^v)-(x_i^g,y_j^g)||<em>2+\\frac{1}{2n}\\sum</em>{j=1}^nmin_{i}||(x_i^v,y_j^v)-(x_i^g,y_j^g)||<em>2$$<br>特征多样性计算如下：<br>$$Q</em>{poc}=1-\\frac{1}{N_p}\\sum_{k}cos&lt;e^\\star_{i,k},e^\\star_i&gt;=1-\\frac{1}{N_p}\\sum_k\\frac{e^\\star_{i,k}\\cdot{e^\\star_i}}{||e^\\star_{i,k}||\\times||e^\\star_i||}$$<br>之后针对每个目标，利用Q值，选择前k个样本作为正样本。</p>\n<h1 id=\"4-实验结果\"><a href=\"#4-实验结果\" class=\"headerlink\" title=\"4. 实验结果\"></a>4. 实验结果</h1><p><img src=\"https://npm.elemecdn.com/justlovesmile-post@1.0.11/20230206141139.png\"></p>\n","categories":[{"name":"人工智能","path":"api/categories/人工智能.json"}],"tags":[{"name":"深度学习","path":"api/tags/深度学习.json"},{"name":"目标检测","path":"api/tags/目标检测.json"},{"name":"论文笔记","path":"api/tags/论文笔记.json"}]}