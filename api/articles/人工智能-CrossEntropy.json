{"title":"深度学习 | 交叉熵损失函数","slug":"人工智能-CrossEntropy","date":"2020-12-11T14:48:21.000Z","updated":"2020-12-11T14:48:21.000Z","comments":true,"path":"api/articles/人工智能-CrossEntropy.json","excerpt":null,"covers":null,"content":"<h1 id=\"Cross-Entropy-Error-Function\"><a href=\"#Cross-Entropy-Error-Function\" class=\"headerlink\" title=\"Cross Entropy Error Function\"></a>Cross Entropy Error Function</h1><p>交叉熵损失函数</p>\n<h2 id=\"一，信息量\"><a href=\"#一，信息量\" class=\"headerlink\" title=\"一，信息量\"></a>一，信息量</h2><p><strong>信息量：</strong></p>\n<p>任何事件都会承载着一定的信息量，包括已经发生的事件和未发生的事件，只是它们承载的信息量会有所不同。如昨天下雨这个已知事件，因为已经发生，是既定事实，那么它的信息量就为0。如明天会下雨这个事件，因为未有发生，那么这个事件的信息量就大。</p>\n<p>从上面例子可以看出信息量是一个与事件发生概率相关的概念，而且可以得出，事件发生的概率越小，其信息量越大。</p>\n<p>假设$x$是一个离散型随机变量，其取值集合为$X$，概率分布函数为$p(x)$，则定义事件$x=x_0$的信息量为：$I(x_0)=-\\log(p(x_0))$</p>\n<h2 id=\"二，熵\"><a href=\"#二，熵\" class=\"headerlink\" title=\"二，熵\"></a>二，熵</h2><p><strong>熵是表示随机变量不确定的度量，是对所有可能发生的事件产生的信息量的期望。</strong>熵值越大，表明这个系统的不确定性就越大。公式如下：</p>\n<p>$$H(X)=-\\sum_{i=1}^n p(x_i)\\log(p(x_i))$$</p>\n<p>对于0-1分布问题，熵的计算方法可以简化为：</p>\n<p>$$H(x)=-\\sum_{i=1}^np(x_i)log(p(x_i))\\ =-p(x)\\log(p(x))-(1-p(x))\\log(1-p(x))$$</p>\n<h2 id=\"三，相对熵（KL散度）\"><a href=\"#三，相对熵（KL散度）\" class=\"headerlink\" title=\"三，相对熵（KL散度）\"></a>三，相对熵（KL散度）</h2><p>相对熵又称KL散度，用于衡量对于同一个随机变量x的两个分布p(x)和q(x)之间的差异。在机器学习中，p(x)常用于描述样本的真实分布，例如[1,0,0,0]表示样本属于第一类，而q(x)则常常用于表示预测的分布，例如[0.7,0.1,0.1,0.1]。显然使用q(x)来描述样本不如p(x)准确，q(x)需要不断地学习来拟合准确的分布p(x)。</p>\n<p>KL散度的公式如下：</p>\n<p>$$D_{KL}(p||q)=\\sum_{i=1}^np(x_i)\\log(\\frac{p(x_i)}{q(x_i)})$$</p>\n<p>KL散度的值越小，表示两个分布越接近。在机器学习中，p往往用来表示样本的真实分布，q用来表示模型所预测的分布，那么KL散度就可以计算两个分布的差异，也就是Loss损失值。</p>\n<h2 id=\"四，交叉熵\"><a href=\"#四，交叉熵\" class=\"headerlink\" title=\"四，交叉熵\"></a>四，交叉熵</h2><p>将KL散度的公式进行变形，得到：</p>\n<p>$$D_{KL}(p||q)=\\sum_{i=1}^np(x_i)\\log(\\frac{p(x_i)}{q(x_i)})\\ =\\sum_{i=1}^np(x_i)\\log(p(x_i))-\\sum_{i=1}^np(x_i)\\log(q(x_i))$$</p>\n<p>根据熵的定义，前半部分是$p(x)$的熵$H(x)=-\\sum_{i=1}^np(x_i)\\log(p(x_i))$，而后半部分则是交叉熵，定义为：</p>\n<p>$$H(p,q)=-\\sum_{i=1}^np(x_i)\\log(q(x_i))$$</p>\n<p>因此$D_{KL}(p||q)=H(p,q)-H(p)$ ，在机器学习中，我们需要评估label和predicts之间的差距，使用KL散度刚刚好，即 $D_{KL}(p|| \\widetilde {q})$ ，由于KL散度中的前一部分$−H(p)$不变，故在优化过程中，只需要关注交叉熵就可以了。</p>\n<h2 id=\"五，交叉熵损失函数\"><a href=\"#五，交叉熵损失函数\" class=\"headerlink\" title=\"五，交叉熵损失函数\"></a>五，交叉熵损失函数</h2><p>在线性回归问题中，常常使用MSE(Mean Squared Error)作为loss函数，而在分类问题中常常使用交叉熵作为loss函数，特别是在神经网络作分类问题时，并且由于交叉熵涉及到计算每个类别的概率，所以交叉熵几乎每次都和sigmoid或者softmax函数一起出现。</p>\n<p><strong>(1)二分类</strong></p>\n<p>在二分的情况下，对于每个类别我们的预测的到的概率为p和1-p。此时表达式为：</p>\n<p>$$L=\\frac{1}{N}\\sum_iL_i=\\frac{1}{N}\\sum_i(-[y_i\\log(p_i)+(1-y_i)\\log(1-p_i)])$$</p>\n<p>其中：</p>\n<ul>\n<li>$y_i$表示样本i的label，正类为1，负类为0</li>\n<li>$p_i$表示样本i预测为正的概率</li>\n</ul>\n<p><strong>(2)多分类</strong></p>\n<p>多分类问题实际上就是对二分类问题的扩展：</p>\n<p>$$L=\\frac{1}{N}\\sum_iL_i=\\frac{1}{N}\\sum_i(-\\sum_{j=1}^My_{ij}\\log(p_{ij}))$$</p>\n<p>其中：</p>\n<ul>\n<li>M 表示类别的数量</li>\n<li>$y_{ij}$表示该类别和样本i类别是否相同，相同为1，不同为0</li>\n<li>$p_{ij}$表示对于观测样本i属于类别j的预测概率</li>\n</ul>\n<p>例如：</p>\n<table>\n<thead>\n<tr>\n<th>id</th>\n<th>predict</th>\n<th>label</th>\n<th>isCorrect</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1</td>\n<td>0.3 0.3 0.4</td>\n<td>0 0 1</td>\n<td>1</td>\n</tr>\n<tr>\n<td>2</td>\n<td>0.3 0.4 0.3</td>\n<td>0 1 0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>3</td>\n<td>0.1 0.2 0.7</td>\n<td>1 0 0</td>\n<td>0</td>\n</tr>\n</tbody></table>\n<p>那么求其Loss：<br>$$L_1=-(0\\times \\log 0.3+0\\times \\log 0.3+1\\times \\log 0.4)$$<br>$$L_2=-(0\\times \\log 0.3+1\\times \\log 0.4+0\\times \\log 0.3)$$<br>$$L_3=-(1\\times \\log 0.1+0\\times \\log 0.2+0\\times \\log 0.7)$$<br>对所有样本的Loss求平均<br>$$Loss=\\frac{L_1+L_2+L_3}{3}$$</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p><a href=\"https://zhuanlan.zhihu.com/p/74075915\">https://zhuanlan.zhihu.com/p/74075915</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/61944055\">https://zhuanlan.zhihu.com/p/61944055</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/35709485\">https://zhuanlan.zhihu.com/p/35709485</a></p>\n<p><a href=\"https://blog.csdn.net/b1055077005/article/details/100152102\">https://blog.csdn.net/b1055077005/article/details/100152102</a></p>\n","more":"<h1 id=\"Cross-Entropy-Error-Function\"><a href=\"#Cross-Entropy-Error-Function\" class=\"headerlink\" title=\"Cross Entropy Error Function\"></a>Cross Entropy Error Function</h1><p>交叉熵损失函数</p>\n<h2 id=\"一，信息量\"><a href=\"#一，信息量\" class=\"headerlink\" title=\"一，信息量\"></a>一，信息量</h2><p><strong>信息量：</strong></p>\n<p>任何事件都会承载着一定的信息量，包括已经发生的事件和未发生的事件，只是它们承载的信息量会有所不同。如昨天下雨这个已知事件，因为已经发生，是既定事实，那么它的信息量就为0。如明天会下雨这个事件，因为未有发生，那么这个事件的信息量就大。</p>\n<p>从上面例子可以看出信息量是一个与事件发生概率相关的概念，而且可以得出，事件发生的概率越小，其信息量越大。</p>\n<p>假设$x$是一个离散型随机变量，其取值集合为$X$，概率分布函数为$p(x)$，则定义事件$x=x_0$的信息量为：$I(x_0)=-\\log(p(x_0))$</p>\n<h2 id=\"二，熵\"><a href=\"#二，熵\" class=\"headerlink\" title=\"二，熵\"></a>二，熵</h2><p><strong>熵是表示随机变量不确定的度量，是对所有可能发生的事件产生的信息量的期望。</strong>熵值越大，表明这个系统的不确定性就越大。公式如下：</p>\n<p>$$H(X)=-\\sum_{i=1}^n p(x_i)\\log(p(x_i))$$</p>\n<p>对于0-1分布问题，熵的计算方法可以简化为：</p>\n<p>$$H(x)=-\\sum_{i=1}^np(x_i)log(p(x_i))\\ =-p(x)\\log(p(x))-(1-p(x))\\log(1-p(x))$$</p>\n<h2 id=\"三，相对熵（KL散度）\"><a href=\"#三，相对熵（KL散度）\" class=\"headerlink\" title=\"三，相对熵（KL散度）\"></a>三，相对熵（KL散度）</h2><p>相对熵又称KL散度，用于衡量对于同一个随机变量x的两个分布p(x)和q(x)之间的差异。在机器学习中，p(x)常用于描述样本的真实分布，例如[1,0,0,0]表示样本属于第一类，而q(x)则常常用于表示预测的分布，例如[0.7,0.1,0.1,0.1]。显然使用q(x)来描述样本不如p(x)准确，q(x)需要不断地学习来拟合准确的分布p(x)。</p>\n<p>KL散度的公式如下：</p>\n<p>$$D_{KL}(p||q)=\\sum_{i=1}^np(x_i)\\log(\\frac{p(x_i)}{q(x_i)})$$</p>\n<p>KL散度的值越小，表示两个分布越接近。在机器学习中，p往往用来表示样本的真实分布，q用来表示模型所预测的分布，那么KL散度就可以计算两个分布的差异，也就是Loss损失值。</p>\n<h2 id=\"四，交叉熵\"><a href=\"#四，交叉熵\" class=\"headerlink\" title=\"四，交叉熵\"></a>四，交叉熵</h2><p>将KL散度的公式进行变形，得到：</p>\n<p>$$D_{KL}(p||q)=\\sum_{i=1}^np(x_i)\\log(\\frac{p(x_i)}{q(x_i)})\\ =\\sum_{i=1}^np(x_i)\\log(p(x_i))-\\sum_{i=1}^np(x_i)\\log(q(x_i))$$</p>\n<p>根据熵的定义，前半部分是$p(x)$的熵$H(x)=-\\sum_{i=1}^np(x_i)\\log(p(x_i))$，而后半部分则是交叉熵，定义为：</p>\n<p>$$H(p,q)=-\\sum_{i=1}^np(x_i)\\log(q(x_i))$$</p>\n<p>因此$D_{KL}(p||q)=H(p,q)-H(p)$ ，在机器学习中，我们需要评估label和predicts之间的差距，使用KL散度刚刚好，即 $D_{KL}(p|| \\widetilde {q})$ ，由于KL散度中的前一部分$−H(p)$不变，故在优化过程中，只需要关注交叉熵就可以了。</p>\n<h2 id=\"五，交叉熵损失函数\"><a href=\"#五，交叉熵损失函数\" class=\"headerlink\" title=\"五，交叉熵损失函数\"></a>五，交叉熵损失函数</h2><p>在线性回归问题中，常常使用MSE(Mean Squared Error)作为loss函数，而在分类问题中常常使用交叉熵作为loss函数，特别是在神经网络作分类问题时，并且由于交叉熵涉及到计算每个类别的概率，所以交叉熵几乎每次都和sigmoid或者softmax函数一起出现。</p>\n<p><strong>(1)二分类</strong></p>\n<p>在二分的情况下，对于每个类别我们的预测的到的概率为p和1-p。此时表达式为：</p>\n<p>$$L=\\frac{1}{N}\\sum_iL_i=\\frac{1}{N}\\sum_i(-[y_i\\log(p_i)+(1-y_i)\\log(1-p_i)])$$</p>\n<p>其中：</p>\n<ul>\n<li>$y_i$表示样本i的label，正类为1，负类为0</li>\n<li>$p_i$表示样本i预测为正的概率</li>\n</ul>\n<p><strong>(2)多分类</strong></p>\n<p>多分类问题实际上就是对二分类问题的扩展：</p>\n<p>$$L=\\frac{1}{N}\\sum_iL_i=\\frac{1}{N}\\sum_i(-\\sum_{j=1}^My_{ij}\\log(p_{ij}))$$</p>\n<p>其中：</p>\n<ul>\n<li>M 表示类别的数量</li>\n<li>$y_{ij}$表示该类别和样本i类别是否相同，相同为1，不同为0</li>\n<li>$p_{ij}$表示对于观测样本i属于类别j的预测概率</li>\n</ul>\n<p>例如：</p>\n<table>\n<thead>\n<tr>\n<th>id</th>\n<th>predict</th>\n<th>label</th>\n<th>isCorrect</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1</td>\n<td>0.3 0.3 0.4</td>\n<td>0 0 1</td>\n<td>1</td>\n</tr>\n<tr>\n<td>2</td>\n<td>0.3 0.4 0.3</td>\n<td>0 1 0</td>\n<td>1</td>\n</tr>\n<tr>\n<td>3</td>\n<td>0.1 0.2 0.7</td>\n<td>1 0 0</td>\n<td>0</td>\n</tr>\n</tbody></table>\n<p>那么求其Loss：<br>$$L_1=-(0\\times \\log 0.3+0\\times \\log 0.3+1\\times \\log 0.4)$$<br>$$L_2=-(0\\times \\log 0.3+1\\times \\log 0.4+0\\times \\log 0.3)$$<br>$$L_3=-(1\\times \\log 0.1+0\\times \\log 0.2+0\\times \\log 0.7)$$<br>对所有样本的Loss求平均<br>$$Loss=\\frac{L_1+L_2+L_3}{3}$$</p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><p><a href=\"https://zhuanlan.zhihu.com/p/74075915\">https://zhuanlan.zhihu.com/p/74075915</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/61944055\">https://zhuanlan.zhihu.com/p/61944055</a></p>\n<p><a href=\"https://zhuanlan.zhihu.com/p/35709485\">https://zhuanlan.zhihu.com/p/35709485</a></p>\n<p><a href=\"https://blog.csdn.net/b1055077005/article/details/100152102\">https://blog.csdn.net/b1055077005/article/details/100152102</a></p>\n","categories":[{"name":"人工智能","path":"api/categories/人工智能.json"}],"tags":[{"name":"深度学习","path":"api/tags/深度学习.json"},{"name":"交叉熵","path":"api/tags/交叉熵.json"}]}