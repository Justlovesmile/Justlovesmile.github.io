{"title":"目标检测 | RetinaNet，经典单阶段Anchor-Based目标检测模型","slug":"人工智能-RetinaNet论文解读","date":"2022-03-14T03:26:21.000Z","updated":"2022-03-14T03:26:21.000Z","comments":true,"path":"api/articles/人工智能-RetinaNet论文解读.json","excerpt":null,"covers":["https://npm.elemecdn.com/justlovesmile-img/image-20211119213555220.png","https://npm.elemecdn.com/justlovesmile-img/image-20211120135540595.png","https://npm.elemecdn.com/justlovesmile-img/image-20211120135222791.png","https://npm.elemecdn.com/justlovesmile-img/image-20211120135146831.png","https://npm.elemecdn.com/justlovesmile-img/image-20211120132951497.png","https://npm.elemecdn.com/justlovesmile-img/image-20211120133459396.png","https://npm.elemecdn.com/justlovesmile-img/image-20211120133655071.png","https://npm.elemecdn.com/justlovesmile-img/image-20211120133916235.png","https://npm.elemecdn.com/justlovesmile-img/image-20211120134227038.png","https://npm.elemecdn.com/justlovesmile-img/image-20211120141212266.png"],"content":"<h1 id=\"1-论文信息\"><a href=\"#1-论文信息\" class=\"headerlink\" title=\"1. 论文信息\"></a>1. 论文信息</h1><p>论文标题：Focal Loss for Dense Object Detection</p>\n<blockquote>\n<p>论文来源：IEEE Transactions on Pattern Analysis and Machine Intelligence 2020<br>论文链接：<a href=\"https://ieeexplore.ieee.org/document/8417976\">Focal Loss for Dense Object Detection | IEEE Xplore</a><br>论文代码：<a href=\"https://github.com/facebookresearch/Detectron\">https://github.com/facebookresearch/Detectron</a></p>\n</blockquote>\n<p><img src= \"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\" data-lazy-src=\"https://npm.elemecdn.com/justlovesmile-img/image-20211119213555220.png\" alt=\"image-20211119213555220\"></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@inproceedings&#123;lin2017focal,</span><br><span class=\"line\">  title=&#123;Focal loss for dense object detection&#125;,</span><br><span class=\"line\">  author=&#123;Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll&#123;\\&#x27;a&#125;r, Piotr&#125;,</span><br><span class=\"line\">  booktitle=&#123;Proceedings of the IEEE international conference on computer vision&#125;,</span><br><span class=\"line\">  pages=&#123;2980--2988&#125;,</span><br><span class=\"line\">  year=&#123;2017&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"2-归纳总结\"><a href=\"#2-归纳总结\" class=\"headerlink\" title=\"2. 归纳总结\"></a>2. 归纳总结</h1><table>\n<thead>\n<tr>\n<th>标签</th>\n<th>目的</th>\n<th>方法</th>\n<th>总结</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>#Anchor #单阶段</td>\n<td>解决正负样本严重不均衡的问题</td>\n<td>retinanet和focal loss</td>\n<td>针对训练过程中的实际问题，修改损失函数以达到优化的目的</td>\n</tr>\n</tbody></table>\n<h1 id=\"3-主要工作\"><a href=\"#3-主要工作\" class=\"headerlink\" title=\"3. 主要工作\"></a>3. 主要工作</h1><p>作者认为之前的单阶段检测算法精度不高的原因可能是前后景类别（正负样本）严重不均衡导致的。因此作者重新设计了一个损失：Focal Loss，其能降低可以较好分类的样本的损失权重，防止训练过程中大量的easy negatives给检测器带来的压制影响，并基于Focal Loss设计提出并训练了RetinaNet。</p>\n<h2 id=\"3-1-网络结构\"><a href=\"#3-1-网络结构\" class=\"headerlink\" title=\"3.1 网络结构\"></a>3.1 网络结构</h2><p>RetinaNet的结构如下图：</p>\n<p><img src= \"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\" data-lazy-src=\"https://npm.elemecdn.com/justlovesmile-img/image-20211120135540595.png\" alt=\"image-20211120135540595\"></p>\n<p>主要包括三个部分：</p>\n<ul>\n<li>Backbone：使用了ResNet+FPN，用于生成多尺度{p3~p7}卷积特征图</li>\n<li>Anchor：p3-p7特征图的base_size设置为$[32^2,64^2,128^2,256^2,512^2]$，在每一层特征图针对denser scale coverage，设置{${2^0,2^{1/3},2^{2/3}}$}三种不同的anchor size，比例为{1:2,1:1,2:1}，即每个位置9种Anchor。</li>\n<li>subnets：用于分类和回归，结构相同但参数不共享的小型FCN结构</li>\n</ul>\n<p><img src= \"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\" data-lazy-src=\"https://npm.elemecdn.com/justlovesmile-img/image-20211120135222791.png\" alt=\"image-20211120135222791\"></p>\n<h2 id=\"3-2-Focal-Loss\"><a href=\"#3-2-Focal-Loss\" class=\"headerlink\" title=\"3.2 Focal Loss\"></a>3.2 Focal Loss</h2><p>作者提到基于R-CNN模式的两阶段算法在解决训练过程中的正负样本不均衡的方法是：</p>\n<ul>\n<li>两阶段级联：在proposal阶段过滤掉大量负样本</li>\n<li>启发式采样：例如固定正负样本比例（例如1:3）或者在线难样本挖掘（Online Hard Example Mining，OHEM）</li>\n</ul>\n<p>Focal loss的做法是设置一个sacling factor，如下图的$(1-p_t)^{\\gamma}$，其可以自动的对easy example进行降权，从而使模型更关注hard example。</p>\n<p><img src= \"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\" data-lazy-src=\"https://npm.elemecdn.com/justlovesmile-img/image-20211120135146831.png\" alt=\"image-20211120135146831\"></p>\n<p>首先，对于二分类任务，普通的交叉熵如下：</p>\n<p><img src= \"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\" data-lazy-src=\"https://npm.elemecdn.com/justlovesmile-img/image-20211120132951497.png\" alt=\"image-20211120132951497\"></p>\n<p>如果定义$p_t$：</p>\n<p><img src= \"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\" data-lazy-src=\"https://npm.elemecdn.com/justlovesmile-img/image-20211120133459396.png\" alt=\"image-20211120133459396\"></p>\n<p>那么交叉熵可以写成$CE(p,y)=CE(p_t)=-log(p_t)$</p>\n<p>有一种常见的用于解决类别不均衡的方法是添加一个权重变量$\\alpha \\in [0,1]$：</p>\n<p><img src= \"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\" data-lazy-src=\"https://npm.elemecdn.com/justlovesmile-img/image-20211120133655071.png\" alt=\"image-20211120133655071\"></p>\n<p>Focal Loss的做法是添加了一个权重变量$(1-p_t)^{\\gamma}$：</p>\n<p><img src= \"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\" data-lazy-src=\"https://npm.elemecdn.com/justlovesmile-img/image-20211120133916235.png\" alt=\"image-20211120133916235\"></p>\n<p>因此当$p_t$趋近于1时，可以较好分类的样本被降权；而$\\gamma$可以用来调节权重比率。除此之外，还可以将$\\alpha$和FL损失相结合：</p>\n<p><img src= \"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\" data-lazy-src=\"https://npm.elemecdn.com/justlovesmile-img/image-20211120134227038.png\" alt=\"image-20211120134227038\"></p>\n<p>除此之外还有其他的Focal Loss变种形式。</p>\n<h1 id=\"4-实验结果\"><a href=\"#4-实验结果\" class=\"headerlink\" title=\"4. 实验结果\"></a>4. 实验结果</h1><p><img src= \"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7\" data-lazy-src=\"https://npm.elemecdn.com/justlovesmile-img/image-20211120141212266.png\" alt=\"image-20211120141212266\"></p>\n","more":"<h1 id=\"1-论文信息\"><a href=\"#1-论文信息\" class=\"headerlink\" title=\"1. 论文信息\"></a>1. 论文信息</h1><p>论文标题：Focal Loss for Dense Object Detection</p>\n<blockquote>\n<p>论文来源：IEEE Transactions on Pattern Analysis and Machine Intelligence 2020<br>论文链接：<a href=\"https://ieeexplore.ieee.org/document/8417976\">Focal Loss for Dense Object Detection | IEEE Xplore</a><br>论文代码：<a href=\"https://github.com/facebookresearch/Detectron\">https://github.com/facebookresearch/Detectron</a></p>\n</blockquote>\n<p><img src=\"https://npm.elemecdn.com/justlovesmile-img/image-20211119213555220.png\" alt=\"image-20211119213555220\"></p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">@inproceedings&#123;lin2017focal,</span><br><span class=\"line\">  title=&#123;Focal loss for dense object detection&#125;,</span><br><span class=\"line\">  author=&#123;Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll&#123;\\&#x27;a&#125;r, Piotr&#125;,</span><br><span class=\"line\">  booktitle=&#123;Proceedings of the IEEE international conference on computer vision&#125;,</span><br><span class=\"line\">  pages=&#123;2980--2988&#125;,</span><br><span class=\"line\">  year=&#123;2017&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"2-归纳总结\"><a href=\"#2-归纳总结\" class=\"headerlink\" title=\"2. 归纳总结\"></a>2. 归纳总结</h1><table>\n<thead>\n<tr>\n<th>标签</th>\n<th>目的</th>\n<th>方法</th>\n<th>总结</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>#Anchor #单阶段</td>\n<td>解决正负样本严重不均衡的问题</td>\n<td>retinanet和focal loss</td>\n<td>针对训练过程中的实际问题，修改损失函数以达到优化的目的</td>\n</tr>\n</tbody></table>\n<h1 id=\"3-主要工作\"><a href=\"#3-主要工作\" class=\"headerlink\" title=\"3. 主要工作\"></a>3. 主要工作</h1><p>作者认为之前的单阶段检测算法精度不高的原因可能是前后景类别（正负样本）严重不均衡导致的。因此作者重新设计了一个损失：Focal Loss，其能降低可以较好分类的样本的损失权重，防止训练过程中大量的easy negatives给检测器带来的压制影响，并基于Focal Loss设计提出并训练了RetinaNet。</p>\n<h2 id=\"3-1-网络结构\"><a href=\"#3-1-网络结构\" class=\"headerlink\" title=\"3.1 网络结构\"></a>3.1 网络结构</h2><p>RetinaNet的结构如下图：</p>\n<p><img src=\"https://npm.elemecdn.com/justlovesmile-img/image-20211120135540595.png\" alt=\"image-20211120135540595\"></p>\n<p>主要包括三个部分：</p>\n<ul>\n<li>Backbone：使用了ResNet+FPN，用于生成多尺度{p3~p7}卷积特征图</li>\n<li>Anchor：p3-p7特征图的base_size设置为$[32^2,64^2,128^2,256^2,512^2]$，在每一层特征图针对denser scale coverage，设置{${2^0,2^{1/3},2^{2/3}}$}三种不同的anchor size，比例为{1:2,1:1,2:1}，即每个位置9种Anchor。</li>\n<li>subnets：用于分类和回归，结构相同但参数不共享的小型FCN结构</li>\n</ul>\n<p><img src=\"https://npm.elemecdn.com/justlovesmile-img/image-20211120135222791.png\" alt=\"image-20211120135222791\"></p>\n<h2 id=\"3-2-Focal-Loss\"><a href=\"#3-2-Focal-Loss\" class=\"headerlink\" title=\"3.2 Focal Loss\"></a>3.2 Focal Loss</h2><p>作者提到基于R-CNN模式的两阶段算法在解决训练过程中的正负样本不均衡的方法是：</p>\n<ul>\n<li>两阶段级联：在proposal阶段过滤掉大量负样本</li>\n<li>启发式采样：例如固定正负样本比例（例如1:3）或者在线难样本挖掘（Online Hard Example Mining，OHEM）</li>\n</ul>\n<p>Focal loss的做法是设置一个sacling factor，如下图的$(1-p_t)^{\\gamma}$，其可以自动的对easy example进行降权，从而使模型更关注hard example。</p>\n<p><img src=\"https://npm.elemecdn.com/justlovesmile-img/image-20211120135146831.png\" alt=\"image-20211120135146831\"></p>\n<p>首先，对于二分类任务，普通的交叉熵如下：</p>\n<p><img src=\"https://npm.elemecdn.com/justlovesmile-img/image-20211120132951497.png\" alt=\"image-20211120132951497\"></p>\n<p>如果定义$p_t$：</p>\n<p><img src=\"https://npm.elemecdn.com/justlovesmile-img/image-20211120133459396.png\" alt=\"image-20211120133459396\"></p>\n<p>那么交叉熵可以写成$CE(p,y)=CE(p_t)=-log(p_t)$</p>\n<p>有一种常见的用于解决类别不均衡的方法是添加一个权重变量$\\alpha \\in [0,1]$：</p>\n<p><img src=\"https://npm.elemecdn.com/justlovesmile-img/image-20211120133655071.png\" alt=\"image-20211120133655071\"></p>\n<p>Focal Loss的做法是添加了一个权重变量$(1-p_t)^{\\gamma}$：</p>\n<p><img src=\"https://npm.elemecdn.com/justlovesmile-img/image-20211120133916235.png\" alt=\"image-20211120133916235\"></p>\n<p>因此当$p_t$趋近于1时，可以较好分类的样本被降权；而$\\gamma$可以用来调节权重比率。除此之外，还可以将$\\alpha$和FL损失相结合：</p>\n<p><img src=\"https://npm.elemecdn.com/justlovesmile-img/image-20211120134227038.png\" alt=\"image-20211120134227038\"></p>\n<p>除此之外还有其他的Focal Loss变种形式。</p>\n<h1 id=\"4-实验结果\"><a href=\"#4-实验结果\" class=\"headerlink\" title=\"4. 实验结果\"></a>4. 实验结果</h1><p><img src=\"https://npm.elemecdn.com/justlovesmile-img/image-20211120141212266.png\" alt=\"image-20211120141212266\"></p>\n","categories":[{"name":"人工智能","path":"api/categories/人工智能.json"}],"tags":[{"name":"深度学习","path":"api/tags/深度学习.json"},{"name":"论文笔记","path":"api/tags/论文笔记.json"},{"name":"目标检测","path":"api/tags/目标检测.json"}]}